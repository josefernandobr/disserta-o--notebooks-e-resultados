{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Etapa 1: não-padronizado (LAG e valores ausentes).ipynb","provenance":[],"authorship_tag":"ABX9TyOmLYhkiUedz/wjRNSVGWJX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yEWrgwv6L9OL"},"source":["#**Pré-processamento** "]},{"cell_type":"code","metadata":{"id":"u0CFgb1wMDCE","executionInfo":{"status":"ok","timestamp":1604628639822,"user_tz":180,"elapsed":28336,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"13c29ebc-4097-446b-c4e1-f129b9f2c0dc","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive \n","drive.mount('/content/gdrive')\n","\n","import pandas as pd\n","import seaborn as sns\n","import missingno as msno\n","import warnings\n","from matplotlib import pyplot as plt\n","import datetime\n","import numpy as np\n","from statsmodels.graphics.tsaplots import plot_acf\n","from statsmodels.graphics.tsaplots import plot_pacf\n","from matplotlib import pyplot\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns',100)\n","\n","df = pd.read_csv(\"gdrive/My Drive/dissertação/Qualidade_do_ar_-_Dados_horarios.csv\",sep=\",\")\n","df['Data']=df['Data'].astype('str')\n","df['Data']=df['Data'].apply(lambda x: x[:-3])\n","\n","#conversão para datetime\n","def convert_to_datetime(x):\n","    return(datetime.datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\"))\n","df['Data']=df['Data'].apply(convert_to_datetime) \n","df['Hora-minuto']=df['Data'].apply(lambda x: str(x)[-8:-3])\n","# #extrair ano, mes, hora \n","df['Ano']=df['Data'].apply(lambda x: x.year)\n","df['Mês']=df['Data'].apply(lambda x: x.month)\n","df['Dia']=df['Data'].apply(lambda x: x.day)\n","df['Hora']=df['Data'].apply(lambda x: x.hour)\n","df['Hora-minuto']=df['Data'].apply(lambda x: str(x)[-11:-6])\n","\n","df_bg=df[df['Estação']=='BG'] #Bangu\n","df_gc=df[df['Estação']=='CG'] #Campo Grande\n","df_ca=df[df['Estação']=='CA'] #Centro\n","df_av=df[df['Estação']=='AV'] #Copacabana\n","df_ir=df[df['Estação']=='IR'] #Irajá\n","df_pg=df[df['Estação']=='PG'] #Pedra de Guaratiba\n","df_sc=df[df['Estação']=='SC'] #São Cristóvão\n","df_sp=df[df['Estação']=='SP'] #Tijuca\n","\n","df_sc.set_index('Data',inplace=True)\n","df_sc.drop(columns=['OBJECTID','CodNum','Estação','Dir_Vento','NO2','HCNM','HCT','CH4','NO','NOx',\\\n","                   'PM2_5', 'Lat', 'Lon', 'X_UTM_Sirgas2000','Y_UTM_Sirgas2000'], inplace=True)\n","df_sc_2=df_sc.copy() #para fazer distribuição sem eliminar ausentes\n","\n","df_sc=df_sc['2011-01-01':'2018-01-01']\n","#explicar que usou interpolação linear com spline e justificar\n","df_sc['Chuva'].interpolate(method='slinear', inplace=True)\n","df_sc['Pres'].interpolate(method='slinear', inplace=True)\n","df_sc['RS'].interpolate(method='slinear', inplace=True)\n","df_sc['Temp'].interpolate(method='slinear', inplace=True)\n","df_sc['UR'].interpolate(method='slinear', inplace=True)\n","df_sc['Vel_Vento'].interpolate(method='slinear', inplace=True)\n","df_sc['SO2'].interpolate(method='slinear', inplace=True)\n","df_sc['CO'].interpolate(method='slinear', inplace=True)\n","df_sc['O3'].interpolate(method='slinear', inplace=True)\n","df_sc['PM10'].interpolate(method='slinear', inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W4GDhk0vRk55"},"source":["#**(spline) janela=1**"]},{"cell_type":"code","metadata":{"id":"Wh9sIrq54IuX","executionInfo":{"status":"ok","timestamp":1604626999818,"user_tz":180,"elapsed":51668,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"65b83cfc-3dda-4709-fbb0-dd62b814963a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 1 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 1)\n","reframed = series_to_supervised(scaled, n_hours, 1) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(300, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 18.5955 - val_loss: 17.7760\n","Epoch 2/150\n","90/90 - 0s - loss: 16.5612 - val_loss: 16.1010\n","Epoch 3/150\n","90/90 - 0s - loss: 15.0051 - val_loss: 14.6024\n","Epoch 4/150\n","90/90 - 0s - loss: 13.6808 - val_loss: 13.4055\n","Epoch 5/150\n","90/90 - 0s - loss: 12.6145 - val_loss: 12.4107\n","Epoch 6/150\n","90/90 - 0s - loss: 11.7774 - val_loss: 11.6263\n","Epoch 7/150\n","90/90 - 0s - loss: 11.0767 - val_loss: 11.0414\n","Epoch 8/150\n","90/90 - 0s - loss: 10.5449 - val_loss: 10.5041\n","Epoch 9/150\n","90/90 - 0s - loss: 9.9712 - val_loss: 10.0051\n","Epoch 10/150\n","90/90 - 0s - loss: 9.5619 - val_loss: 9.5837\n","Epoch 11/150\n","90/90 - 0s - loss: 9.2285 - val_loss: 9.2092\n","Epoch 12/150\n","90/90 - 0s - loss: 8.8958 - val_loss: 8.9227\n","Epoch 13/150\n","90/90 - 0s - loss: 8.6175 - val_loss: 8.6416\n","Epoch 14/150\n","90/90 - 0s - loss: 8.3549 - val_loss: 8.4827\n","Epoch 15/150\n","90/90 - 0s - loss: 8.1797 - val_loss: 8.1316\n","Epoch 16/150\n","90/90 - 0s - loss: 7.9769 - val_loss: 8.0349\n","Epoch 17/150\n","90/90 - 0s - loss: 7.8754 - val_loss: 7.8596\n","Epoch 18/150\n","90/90 - 0s - loss: 7.7046 - val_loss: 7.7450\n","Epoch 19/150\n","90/90 - 0s - loss: 7.5655 - val_loss: 7.6185\n","Epoch 20/150\n","90/90 - 0s - loss: 7.4319 - val_loss: 7.5135\n","Epoch 21/150\n","90/90 - 0s - loss: 7.3078 - val_loss: 7.3968\n","Epoch 22/150\n","90/90 - 0s - loss: 7.2024 - val_loss: 7.3166\n","Epoch 23/150\n","90/90 - 0s - loss: 7.1052 - val_loss: 7.2397\n","Epoch 24/150\n","90/90 - 0s - loss: 7.0259 - val_loss: 7.1544\n","Epoch 25/150\n","90/90 - 0s - loss: 6.9379 - val_loss: 7.0901\n","Epoch 26/150\n","90/90 - 0s - loss: 6.8682 - val_loss: 7.0560\n","Epoch 27/150\n","90/90 - 0s - loss: 6.8052 - val_loss: 6.9802\n","Epoch 28/150\n","90/90 - 0s - loss: 6.7475 - val_loss: 6.9157\n","Epoch 29/150\n","90/90 - 0s - loss: 6.6859 - val_loss: 6.8720\n","Epoch 30/150\n","90/90 - 0s - loss: 6.6191 - val_loss: 6.8580\n","Epoch 31/150\n","90/90 - 0s - loss: 6.5804 - val_loss: 6.7859\n","Epoch 32/150\n","90/90 - 0s - loss: 6.5365 - val_loss: 6.7419\n","Epoch 33/150\n","90/90 - 0s - loss: 6.5006 - val_loss: 6.6991\n","Epoch 34/150\n","90/90 - 0s - loss: 6.4606 - val_loss: 6.6788\n","Epoch 35/150\n","90/90 - 0s - loss: 6.4257 - val_loss: 6.6498\n","Epoch 36/150\n","90/90 - 0s - loss: 6.3792 - val_loss: 6.6253\n","Epoch 37/150\n","90/90 - 0s - loss: 6.3517 - val_loss: 6.6070\n","Epoch 38/150\n","90/90 - 0s - loss: 6.3331 - val_loss: 6.5561\n","Epoch 39/150\n","90/90 - 0s - loss: 6.3060 - val_loss: 6.5287\n","Epoch 40/150\n","90/90 - 0s - loss: 6.2712 - val_loss: 6.5151\n","Epoch 41/150\n","90/90 - 0s - loss: 6.2552 - val_loss: 6.4896\n","Epoch 42/150\n","90/90 - 0s - loss: 6.2349 - val_loss: 6.4843\n","Epoch 43/150\n","90/90 - 0s - loss: 6.2099 - val_loss: 6.4772\n","Epoch 44/150\n","90/90 - 0s - loss: 6.1969 - val_loss: 6.4587\n","Epoch 45/150\n","90/90 - 0s - loss: 6.1706 - val_loss: 6.4175\n","Epoch 46/150\n","90/90 - 0s - loss: 6.1493 - val_loss: 6.4059\n","Epoch 47/150\n","90/90 - 0s - loss: 6.1367 - val_loss: 6.3892\n","Epoch 48/150\n","90/90 - 0s - loss: 6.1138 - val_loss: 6.3918\n","Epoch 49/150\n","90/90 - 0s - loss: 6.1019 - val_loss: 6.3475\n","Epoch 50/150\n","90/90 - 0s - loss: 6.0797 - val_loss: 6.3879\n","Epoch 51/150\n","90/90 - 0s - loss: 6.0660 - val_loss: 6.3296\n","Epoch 52/150\n","90/90 - 0s - loss: 6.0392 - val_loss: 6.3386\n","Epoch 53/150\n","90/90 - 0s - loss: 6.0347 - val_loss: 6.3231\n","Epoch 54/150\n","90/90 - 0s - loss: 6.0232 - val_loss: 6.3211\n","Epoch 55/150\n","90/90 - 0s - loss: 6.0085 - val_loss: 6.2845\n","Epoch 56/150\n","90/90 - 0s - loss: 6.0019 - val_loss: 6.2815\n","Epoch 57/150\n","90/90 - 0s - loss: 5.9952 - val_loss: 6.2547\n","Epoch 58/150\n","90/90 - 0s - loss: 5.9738 - val_loss: 6.2444\n","Epoch 59/150\n","90/90 - 0s - loss: 5.9648 - val_loss: 6.2900\n","Epoch 60/150\n","90/90 - 0s - loss: 5.9564 - val_loss: 6.2416\n","Epoch 61/150\n","90/90 - 0s - loss: 5.9393 - val_loss: 6.2574\n","Epoch 62/150\n","90/90 - 0s - loss: 5.9473 - val_loss: 6.2334\n","Epoch 63/150\n","90/90 - 0s - loss: 5.9416 - val_loss: 6.2077\n","Epoch 64/150\n","90/90 - 0s - loss: 5.9255 - val_loss: 6.2195\n","Epoch 65/150\n","90/90 - 0s - loss: 5.9180 - val_loss: 6.2079\n","Epoch 66/150\n","90/90 - 0s - loss: 5.9147 - val_loss: 6.2134\n","Epoch 67/150\n","90/90 - 0s - loss: 5.8965 - val_loss: 6.2540\n","Epoch 68/150\n","90/90 - 0s - loss: 5.9005 - val_loss: 6.1975\n","Epoch 69/150\n","90/90 - 0s - loss: 5.8820 - val_loss: 6.1947\n","Epoch 70/150\n","90/90 - 0s - loss: 5.8911 - val_loss: 6.1899\n","Epoch 71/150\n","90/90 - 0s - loss: 5.8867 - val_loss: 6.1731\n","Epoch 72/150\n","90/90 - 0s - loss: 5.8694 - val_loss: 6.1504\n","Epoch 73/150\n","90/90 - 0s - loss: 5.8623 - val_loss: 6.1689\n","Epoch 74/150\n","90/90 - 0s - loss: 5.8671 - val_loss: 6.1489\n","Epoch 75/150\n","90/90 - 0s - loss: 5.8481 - val_loss: 6.1492\n","Epoch 76/150\n","90/90 - 0s - loss: 5.8424 - val_loss: 6.1525\n","Epoch 77/150\n","90/90 - 0s - loss: 5.8444 - val_loss: 6.1497\n","Epoch 78/150\n","90/90 - 0s - loss: 5.8459 - val_loss: 6.1644\n","Epoch 79/150\n","90/90 - 0s - loss: 5.8336 - val_loss: 6.1355\n","Epoch 80/150\n","90/90 - 0s - loss: 5.8208 - val_loss: 6.1339\n","Epoch 81/150\n","90/90 - 0s - loss: 5.8209 - val_loss: 6.1274\n","Epoch 82/150\n","90/90 - 0s - loss: 5.8119 - val_loss: 6.1231\n","Epoch 83/150\n","90/90 - 0s - loss: 5.8059 - val_loss: 6.1568\n","Epoch 84/150\n","90/90 - 0s - loss: 5.8161 - val_loss: 6.1242\n","Epoch 85/150\n","90/90 - 0s - loss: 5.7945 - val_loss: 6.1547\n","Epoch 86/150\n","90/90 - 0s - loss: 5.7925 - val_loss: 6.1642\n","Epoch 87/150\n","90/90 - 0s - loss: 5.8033 - val_loss: 6.1542\n","Epoch 88/150\n","90/90 - 0s - loss: 5.7911 - val_loss: 6.1149\n","Epoch 89/150\n","90/90 - 0s - loss: 5.7800 - val_loss: 6.0908\n","Epoch 90/150\n","90/90 - 0s - loss: 5.7745 - val_loss: 6.1170\n","Epoch 91/150\n","90/90 - 0s - loss: 5.7740 - val_loss: 6.1364\n","Epoch 92/150\n","90/90 - 0s - loss: 5.7785 - val_loss: 6.1350\n","Epoch 93/150\n","90/90 - 0s - loss: 5.7737 - val_loss: 6.1279\n","Epoch 94/150\n","90/90 - 0s - loss: 5.7688 - val_loss: 6.1285\n","Epoch 95/150\n","90/90 - 0s - loss: 5.7688 - val_loss: 6.0951\n","Epoch 96/150\n","90/90 - 0s - loss: 5.7609 - val_loss: 6.0730\n","Epoch 97/150\n","90/90 - 0s - loss: 5.7498 - val_loss: 6.1091\n","Epoch 98/150\n","90/90 - 0s - loss: 5.7563 - val_loss: 6.0945\n","Epoch 99/150\n","90/90 - 0s - loss: 5.7458 - val_loss: 6.0841\n","Epoch 100/150\n","90/90 - 0s - loss: 5.7470 - val_loss: 6.0804\n","Epoch 101/150\n","90/90 - 0s - loss: 5.7336 - val_loss: 6.1041\n","Epoch 102/150\n","90/90 - 0s - loss: 5.7475 - val_loss: 6.1276\n","Epoch 103/150\n","90/90 - 0s - loss: 5.7385 - val_loss: 6.0975\n","Epoch 104/150\n","90/90 - 0s - loss: 5.7306 - val_loss: 6.0847\n","Epoch 105/150\n","90/90 - 0s - loss: 5.7234 - val_loss: 6.0631\n","Epoch 106/150\n","90/90 - 0s - loss: 5.7248 - val_loss: 6.0693\n","Epoch 107/150\n","90/90 - 0s - loss: 5.7259 - val_loss: 6.0359\n","Epoch 108/150\n","90/90 - 0s - loss: 5.7227 - val_loss: 6.0788\n","Epoch 109/150\n","90/90 - 0s - loss: 5.7205 - val_loss: 6.0579\n","Epoch 110/150\n","90/90 - 0s - loss: 5.7223 - val_loss: 6.0369\n","Epoch 111/150\n","90/90 - 0s - loss: 5.7187 - val_loss: 6.0602\n","Epoch 112/150\n","90/90 - 0s - loss: 5.7090 - val_loss: 6.0616\n","Epoch 113/150\n","90/90 - 0s - loss: 5.7090 - val_loss: 6.0655\n","Epoch 114/150\n","90/90 - 0s - loss: 5.7027 - val_loss: 6.0695\n","Epoch 115/150\n","90/90 - 0s - loss: 5.7105 - val_loss: 6.0483\n","Epoch 116/150\n","90/90 - 0s - loss: 5.7036 - val_loss: 6.0451\n","Epoch 117/150\n","90/90 - 0s - loss: 5.7095 - val_loss: 6.0706\n","Epoch 118/150\n","90/90 - 0s - loss: 5.7144 - val_loss: 6.0319\n","Epoch 119/150\n","90/90 - 0s - loss: 5.6967 - val_loss: 6.0488\n","Epoch 120/150\n","90/90 - 0s - loss: 5.6977 - val_loss: 6.0492\n","Epoch 121/150\n","90/90 - 0s - loss: 5.6920 - val_loss: 6.0468\n","Epoch 122/150\n","90/90 - 0s - loss: 5.6939 - val_loss: 6.0391\n","Epoch 123/150\n","90/90 - 0s - loss: 5.6952 - val_loss: 6.1015\n","Epoch 124/150\n","90/90 - 0s - loss: 5.6986 - val_loss: 6.0644\n","Epoch 125/150\n","90/90 - 0s - loss: 5.6892 - val_loss: 6.0354\n","Epoch 126/150\n","90/90 - 0s - loss: 5.6906 - val_loss: 6.0591\n","Epoch 127/150\n","90/90 - 0s - loss: 5.6751 - val_loss: 6.0432\n","Epoch 128/150\n","90/90 - 0s - loss: 5.6848 - val_loss: 6.0393\n","Epoch 129/150\n","90/90 - 0s - loss: 5.6695 - val_loss: 6.0351\n","Epoch 130/150\n","90/90 - 0s - loss: 5.6753 - val_loss: 6.0182\n","Epoch 131/150\n","90/90 - 0s - loss: 5.6743 - val_loss: 6.0421\n","Epoch 132/150\n","90/90 - 0s - loss: 5.6792 - val_loss: 6.0393\n","Epoch 133/150\n","90/90 - 0s - loss: 5.6725 - val_loss: 6.0179\n","Epoch 134/150\n","90/90 - 0s - loss: 5.6818 - val_loss: 6.0413\n","Epoch 135/150\n","90/90 - 0s - loss: 5.6758 - val_loss: 6.0221\n","Epoch 136/150\n","90/90 - 0s - loss: 5.6674 - val_loss: 6.0580\n","Epoch 137/150\n","90/90 - 0s - loss: 5.6627 - val_loss: 6.0496\n","Epoch 138/150\n","90/90 - 0s - loss: 5.6674 - val_loss: 6.0386\n","Epoch 139/150\n","90/90 - 0s - loss: 5.6604 - val_loss: 6.0301\n","Epoch 140/150\n","90/90 - 0s - loss: 5.6594 - val_loss: 6.0183\n","Epoch 141/150\n","90/90 - 0s - loss: 5.6535 - val_loss: 6.0340\n","Epoch 142/150\n","90/90 - 0s - loss: 5.6438 - val_loss: 6.0373\n","Epoch 143/150\n","90/90 - 0s - loss: 5.6565 - val_loss: 6.0233\n","Epoch 144/150\n","90/90 - 0s - loss: 5.6480 - val_loss: 6.0248\n","Epoch 145/150\n","90/90 - 0s - loss: 5.6458 - val_loss: 6.0098\n","Epoch 146/150\n","90/90 - 0s - loss: 5.6465 - val_loss: 6.0285\n","Epoch 147/150\n","90/90 - 0s - loss: 5.6357 - val_loss: 6.0242\n","Epoch 148/150\n","90/90 - 0s - loss: 5.6320 - val_loss: 6.0304\n","Epoch 149/150\n","90/90 - 0s - loss: 5.6326 - val_loss: 6.0380\n","Epoch 150/150\n","90/90 - 0s - loss: 5.6506 - val_loss: 6.0543\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M4REHxZ_4ekk","executionInfo":{"status":"ok","timestamp":1604627000677,"user_tz":180,"elapsed":52515,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"6b8e9770-d77b-4ed4-faa7-abecbb325496","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 9.789\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pqdiq4mc4j_W","executionInfo":{"status":"ok","timestamp":1604627000679,"user_tz":180,"elapsed":52513,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"2cc70232-a232-4617-daac-c8d36f3fa356","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 6.054\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jlRCEQfr4oiO","executionInfo":{"status":"ok","timestamp":1604627000680,"user_tz":180,"elapsed":52510,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"af9221fe-4e70-4712-df34-ee7a28c3d4d9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.832\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U8koG2gsMd7u"},"source":["#**(spline) janela=2**"]},{"cell_type":"code","metadata":{"id":"G3LrV5V4NCLw","executionInfo":{"status":"ok","timestamp":1604627172713,"user_tz":180,"elapsed":54922,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"24205b53-1fc4-483b-c520-bbd564d9c7ff","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 2 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 1)\n","reframed = series_to_supervised(scaled, n_hours, 2) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(300, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 18.6968 - val_loss: 17.9202\n","Epoch 2/150\n","90/90 - 0s - loss: 16.7208 - val_loss: 16.3746\n","Epoch 3/150\n","90/90 - 0s - loss: 15.4136 - val_loss: 15.1727\n","Epoch 4/150\n","90/90 - 0s - loss: 14.4315 - val_loss: 14.2557\n","Epoch 5/150\n","90/90 - 0s - loss: 13.6249 - val_loss: 13.4887\n","Epoch 6/150\n","90/90 - 0s - loss: 13.0052 - val_loss: 12.8662\n","Epoch 7/150\n","90/90 - 0s - loss: 12.4696 - val_loss: 12.3487\n","Epoch 8/150\n","90/90 - 0s - loss: 11.9592 - val_loss: 11.9347\n","Epoch 9/150\n","90/90 - 0s - loss: 11.5718 - val_loss: 11.5834\n","Epoch 10/150\n","90/90 - 0s - loss: 11.2295 - val_loss: 11.2134\n","Epoch 11/150\n","90/90 - 0s - loss: 10.9525 - val_loss: 10.9744\n","Epoch 12/150\n","90/90 - 0s - loss: 10.6917 - val_loss: 10.7059\n","Epoch 13/150\n","90/90 - 0s - loss: 10.4946 - val_loss: 10.4990\n","Epoch 14/150\n","90/90 - 0s - loss: 10.2741 - val_loss: 10.2697\n","Epoch 15/150\n","90/90 - 0s - loss: 10.0650 - val_loss: 10.1189\n","Epoch 16/150\n","90/90 - 0s - loss: 9.9165 - val_loss: 9.9771\n","Epoch 17/150\n","90/90 - 0s - loss: 9.7978 - val_loss: 9.8303\n","Epoch 18/150\n","90/90 - 0s - loss: 9.6754 - val_loss: 9.6941\n","Epoch 19/150\n","90/90 - 0s - loss: 9.5429 - val_loss: 9.6796\n","Epoch 20/150\n","90/90 - 0s - loss: 9.4908 - val_loss: 9.5412\n","Epoch 21/150\n","90/90 - 0s - loss: 9.3858 - val_loss: 9.4971\n","Epoch 22/150\n","90/90 - 0s - loss: 9.3159 - val_loss: 9.4144\n","Epoch 23/150\n","90/90 - 0s - loss: 9.2396 - val_loss: 9.4505\n","Epoch 24/150\n","90/90 - 0s - loss: 9.2114 - val_loss: 9.3310\n","Epoch 25/150\n","90/90 - 0s - loss: 9.1403 - val_loss: 9.2951\n","Epoch 26/150\n","90/90 - 0s - loss: 9.0579 - val_loss: 9.2928\n","Epoch 27/150\n","90/90 - 0s - loss: 9.0675 - val_loss: 9.2272\n","Epoch 28/150\n","90/90 - 0s - loss: 8.9937 - val_loss: 9.1920\n","Epoch 29/150\n","90/90 - 0s - loss: 8.9592 - val_loss: 9.1769\n","Epoch 30/150\n","90/90 - 0s - loss: 8.9173 - val_loss: 9.1686\n","Epoch 31/150\n","90/90 - 0s - loss: 8.8843 - val_loss: 9.1029\n","Epoch 32/150\n","90/90 - 0s - loss: 8.8242 - val_loss: 9.0536\n","Epoch 33/150\n","90/90 - 0s - loss: 8.8018 - val_loss: 9.0481\n","Epoch 34/150\n","90/90 - 0s - loss: 8.7845 - val_loss: 9.0059\n","Epoch 35/150\n","90/90 - 0s - loss: 8.7303 - val_loss: 9.0402\n","Epoch 36/150\n","90/90 - 0s - loss: 8.7122 - val_loss: 8.9934\n","Epoch 37/150\n","90/90 - 0s - loss: 8.6950 - val_loss: 8.9600\n","Epoch 38/150\n","90/90 - 0s - loss: 8.6632 - val_loss: 8.9468\n","Epoch 39/150\n","90/90 - 0s - loss: 8.6413 - val_loss: 8.8875\n","Epoch 40/150\n","90/90 - 0s - loss: 8.6251 - val_loss: 8.9166\n","Epoch 41/150\n","90/90 - 0s - loss: 8.6062 - val_loss: 8.8797\n","Epoch 42/150\n","90/90 - 0s - loss: 8.5668 - val_loss: 8.8946\n","Epoch 43/150\n","90/90 - 0s - loss: 8.5529 - val_loss: 8.8623\n","Epoch 44/150\n","90/90 - 0s - loss: 8.5268 - val_loss: 8.7958\n","Epoch 45/150\n","90/90 - 0s - loss: 8.5255 - val_loss: 8.7966\n","Epoch 46/150\n","90/90 - 0s - loss: 8.4977 - val_loss: 8.8139\n","Epoch 47/150\n","90/90 - 0s - loss: 8.4728 - val_loss: 8.7710\n","Epoch 48/150\n","90/90 - 0s - loss: 8.4475 - val_loss: 8.7852\n","Epoch 49/150\n","90/90 - 0s - loss: 8.4416 - val_loss: 8.7710\n","Epoch 50/150\n","90/90 - 0s - loss: 8.4426 - val_loss: 8.7331\n","Epoch 51/150\n","90/90 - 0s - loss: 8.4156 - val_loss: 8.7462\n","Epoch 52/150\n","90/90 - 0s - loss: 8.4121 - val_loss: 8.7223\n","Epoch 53/150\n","90/90 - 0s - loss: 8.3939 - val_loss: 8.7138\n","Epoch 54/150\n","90/90 - 0s - loss: 8.3860 - val_loss: 8.7108\n","Epoch 55/150\n","90/90 - 0s - loss: 8.3853 - val_loss: 8.6975\n","Epoch 56/150\n","90/90 - 0s - loss: 8.3823 - val_loss: 8.6801\n","Epoch 57/150\n","90/90 - 0s - loss: 8.3507 - val_loss: 8.6926\n","Epoch 58/150\n","90/90 - 0s - loss: 8.3430 - val_loss: 8.6825\n","Epoch 59/150\n","90/90 - 0s - loss: 8.3440 - val_loss: 8.6659\n","Epoch 60/150\n","90/90 - 0s - loss: 8.3262 - val_loss: 8.7048\n","Epoch 61/150\n","90/90 - 0s - loss: 8.3318 - val_loss: 8.6708\n","Epoch 62/150\n","90/90 - 0s - loss: 8.3255 - val_loss: 8.6686\n","Epoch 63/150\n","90/90 - 0s - loss: 8.3084 - val_loss: 8.6583\n","Epoch 64/150\n","90/90 - 0s - loss: 8.3033 - val_loss: 8.6798\n","Epoch 65/150\n","90/90 - 0s - loss: 8.3091 - val_loss: 8.6361\n","Epoch 66/150\n","90/90 - 0s - loss: 8.2972 - val_loss: 8.6693\n","Epoch 67/150\n","90/90 - 0s - loss: 8.2984 - val_loss: 8.6889\n","Epoch 68/150\n","90/90 - 0s - loss: 8.2980 - val_loss: 8.6740\n","Epoch 69/150\n","90/90 - 0s - loss: 8.2788 - val_loss: 8.6752\n","Epoch 70/150\n","90/90 - 0s - loss: 8.2618 - val_loss: 8.6186\n","Epoch 71/150\n","90/90 - 0s - loss: 8.2674 - val_loss: 8.6242\n","Epoch 72/150\n","90/90 - 0s - loss: 8.2544 - val_loss: 8.6295\n","Epoch 73/150\n","90/90 - 0s - loss: 8.2516 - val_loss: 8.6477\n","Epoch 74/150\n","90/90 - 0s - loss: 8.2442 - val_loss: 8.6264\n","Epoch 75/150\n","90/90 - 0s - loss: 8.2476 - val_loss: 8.6065\n","Epoch 76/150\n","90/90 - 0s - loss: 8.2232 - val_loss: 8.6064\n","Epoch 77/150\n","90/90 - 0s - loss: 8.2339 - val_loss: 8.6124\n","Epoch 78/150\n","90/90 - 0s - loss: 8.2358 - val_loss: 8.6095\n","Epoch 79/150\n","90/90 - 0s - loss: 8.2164 - val_loss: 8.5629\n","Epoch 80/150\n","90/90 - 0s - loss: 8.2160 - val_loss: 8.6125\n","Epoch 81/150\n","90/90 - 0s - loss: 8.2089 - val_loss: 8.5903\n","Epoch 82/150\n","90/90 - 0s - loss: 8.2158 - val_loss: 8.5740\n","Epoch 83/150\n","90/90 - 0s - loss: 8.1892 - val_loss: 8.5976\n","Epoch 84/150\n","90/90 - 0s - loss: 8.1913 - val_loss: 8.5803\n","Epoch 85/150\n","90/90 - 0s - loss: 8.1747 - val_loss: 8.5575\n","Epoch 86/150\n","90/90 - 0s - loss: 8.1830 - val_loss: 8.5834\n","Epoch 87/150\n","90/90 - 0s - loss: 8.1806 - val_loss: 8.5594\n","Epoch 88/150\n","90/90 - 0s - loss: 8.1675 - val_loss: 8.5281\n","Epoch 89/150\n","90/90 - 0s - loss: 8.1600 - val_loss: 8.5733\n","Epoch 90/150\n","90/90 - 0s - loss: 8.1635 - val_loss: 8.5389\n","Epoch 91/150\n","90/90 - 0s - loss: 8.1498 - val_loss: 8.5462\n","Epoch 92/150\n","90/90 - 0s - loss: 8.1453 - val_loss: 8.5271\n","Epoch 93/150\n","90/90 - 0s - loss: 8.1514 - val_loss: 8.5619\n","Epoch 94/150\n","90/90 - 0s - loss: 8.1481 - val_loss: 8.5516\n","Epoch 95/150\n","90/90 - 0s - loss: 8.1513 - val_loss: 8.5473\n","Epoch 96/150\n","90/90 - 0s - loss: 8.1491 - val_loss: 8.5306\n","Epoch 97/150\n","90/90 - 0s - loss: 8.1314 - val_loss: 8.5432\n","Epoch 98/150\n","90/90 - 0s - loss: 8.1293 - val_loss: 8.5345\n","Epoch 99/150\n","90/90 - 0s - loss: 8.1368 - val_loss: 8.5365\n","Epoch 100/150\n","90/90 - 0s - loss: 8.1312 - val_loss: 8.5327\n","Epoch 101/150\n","90/90 - 0s - loss: 8.1182 - val_loss: 8.5412\n","Epoch 102/150\n","90/90 - 0s - loss: 8.1286 - val_loss: 8.5269\n","Epoch 103/150\n","90/90 - 0s - loss: 8.1241 - val_loss: 8.5172\n","Epoch 104/150\n","90/90 - 0s - loss: 8.1127 - val_loss: 8.5003\n","Epoch 105/150\n","90/90 - 0s - loss: 8.1190 - val_loss: 8.5034\n","Epoch 106/150\n","90/90 - 0s - loss: 8.1097 - val_loss: 8.5061\n","Epoch 107/150\n","90/90 - 0s - loss: 8.1095 - val_loss: 8.5071\n","Epoch 108/150\n","90/90 - 0s - loss: 8.1075 - val_loss: 8.4912\n","Epoch 109/150\n","90/90 - 0s - loss: 8.0886 - val_loss: 8.4840\n","Epoch 110/150\n","90/90 - 0s - loss: 8.0999 - val_loss: 8.4824\n","Epoch 111/150\n","90/90 - 0s - loss: 8.0812 - val_loss: 8.5088\n","Epoch 112/150\n","90/90 - 0s - loss: 8.0969 - val_loss: 8.5018\n","Epoch 113/150\n","90/90 - 0s - loss: 8.0948 - val_loss: 8.5022\n","Epoch 114/150\n","90/90 - 0s - loss: 8.0890 - val_loss: 8.4829\n","Epoch 115/150\n","90/90 - 0s - loss: 8.0812 - val_loss: 8.5006\n","Epoch 116/150\n","90/90 - 0s - loss: 8.0842 - val_loss: 8.4990\n","Epoch 117/150\n","90/90 - 0s - loss: 8.0781 - val_loss: 8.5386\n","Epoch 118/150\n","90/90 - 0s - loss: 8.0800 - val_loss: 8.4828\n","Epoch 119/150\n","90/90 - 0s - loss: 8.0642 - val_loss: 8.4702\n","Epoch 120/150\n","90/90 - 0s - loss: 8.0724 - val_loss: 8.5015\n","Epoch 121/150\n","90/90 - 0s - loss: 8.0672 - val_loss: 8.4700\n","Epoch 122/150\n","90/90 - 0s - loss: 8.0504 - val_loss: 8.4803\n","Epoch 123/150\n","90/90 - 0s - loss: 8.0426 - val_loss: 8.4665\n","Epoch 124/150\n","90/90 - 0s - loss: 8.0439 - val_loss: 8.4763\n","Epoch 125/150\n","90/90 - 0s - loss: 8.0384 - val_loss: 8.4883\n","Epoch 126/150\n","90/90 - 0s - loss: 8.0418 - val_loss: 8.4509\n","Epoch 127/150\n","90/90 - 0s - loss: 8.0369 - val_loss: 8.4658\n","Epoch 128/150\n","90/90 - 0s - loss: 8.0437 - val_loss: 8.4762\n","Epoch 129/150\n","90/90 - 0s - loss: 8.0142 - val_loss: 8.4567\n","Epoch 130/150\n","90/90 - 0s - loss: 8.0260 - val_loss: 8.4847\n","Epoch 131/150\n","90/90 - 0s - loss: 8.0308 - val_loss: 8.4434\n","Epoch 132/150\n","90/90 - 0s - loss: 8.0407 - val_loss: 8.4597\n","Epoch 133/150\n","90/90 - 0s - loss: 8.0273 - val_loss: 8.4537\n","Epoch 134/150\n","90/90 - 0s - loss: 8.0296 - val_loss: 8.4609\n","Epoch 135/150\n","90/90 - 0s - loss: 8.0231 - val_loss: 8.4501\n","Epoch 136/150\n","90/90 - 0s - loss: 8.0266 - val_loss: 8.4594\n","Epoch 137/150\n","90/90 - 0s - loss: 8.0393 - val_loss: 8.4522\n","Epoch 138/150\n","90/90 - 0s - loss: 8.0141 - val_loss: 8.4545\n","Epoch 139/150\n","90/90 - 0s - loss: 8.0145 - val_loss: 8.4732\n","Epoch 140/150\n","90/90 - 0s - loss: 8.0162 - val_loss: 8.4557\n","Epoch 141/150\n","90/90 - 0s - loss: 8.0011 - val_loss: 8.4806\n","Epoch 142/150\n","90/90 - 0s - loss: 8.0077 - val_loss: 8.4792\n","Epoch 143/150\n","90/90 - 0s - loss: 8.0111 - val_loss: 8.4311\n","Epoch 144/150\n","90/90 - 0s - loss: 8.0156 - val_loss: 8.4507\n","Epoch 145/150\n","90/90 - 0s - loss: 8.0073 - val_loss: 8.4189\n","Epoch 146/150\n","90/90 - 0s - loss: 7.9976 - val_loss: 8.4453\n","Epoch 147/150\n","90/90 - 0s - loss: 8.0068 - val_loss: 8.4593\n","Epoch 148/150\n","90/90 - 0s - loss: 7.9995 - val_loss: 8.4479\n","Epoch 149/150\n","90/90 - 0s - loss: 7.9925 - val_loss: 8.4559\n","Epoch 150/150\n","90/90 - 0s - loss: 7.9922 - val_loss: 8.4546\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5qzGWyW-Qepw","executionInfo":{"status":"ok","timestamp":1604627173733,"user_tz":180,"elapsed":55915,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"a15ba539-5515-4905-90f4-3a12cdd3231d","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 13.288\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MDob3s2qQ_Nl","executionInfo":{"status":"ok","timestamp":1604627173734,"user_tz":180,"elapsed":55911,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"04f7bf3a-1d06-42d9-8a51-97de07a64d70","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 8.455\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xHsLv4QsQ-_W","executionInfo":{"status":"ok","timestamp":1604627173735,"user_tz":180,"elapsed":55906,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"19b68b27-041e-44a4-c81c-8fbc9aaa125f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.691\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q_0aOwFsRtyK"},"source":["#**(spline) janela=3**"]},{"cell_type":"code","metadata":{"id":"RBIIAczc44-O","executionInfo":{"status":"ok","timestamp":1604627285770,"user_tz":180,"elapsed":59495,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"28ce2742-830a-4546-c6b8-3a8c08a44ace","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 3 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 1)\n","reframed = series_to_supervised(scaled, n_hours, 3) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(300, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 18.7842 - val_loss: 18.0098\n","Epoch 2/150\n","90/90 - 0s - loss: 17.1761 - val_loss: 17.0412\n","Epoch 3/150\n","90/90 - 0s - loss: 16.3739 - val_loss: 16.2493\n","Epoch 4/150\n","90/90 - 0s - loss: 15.6419 - val_loss: 15.5124\n","Epoch 5/150\n","90/90 - 0s - loss: 14.9723 - val_loss: 14.9108\n","Epoch 6/150\n","90/90 - 0s - loss: 14.4408 - val_loss: 14.4085\n","Epoch 7/150\n","90/90 - 0s - loss: 13.9676 - val_loss: 13.9757\n","Epoch 8/150\n","90/90 - 0s - loss: 13.5797 - val_loss: 13.6224\n","Epoch 9/150\n","90/90 - 0s - loss: 13.2677 - val_loss: 13.3197\n","Epoch 10/150\n","90/90 - 0s - loss: 12.9293 - val_loss: 12.9635\n","Epoch 11/150\n","90/90 - 0s - loss: 12.6362 - val_loss: 12.7464\n","Epoch 12/150\n","90/90 - 0s - loss: 12.5126 - val_loss: 12.5398\n","Epoch 13/150\n","90/90 - 0s - loss: 12.2700 - val_loss: 12.3906\n","Epoch 14/150\n","90/90 - 0s - loss: 12.1425 - val_loss: 12.2583\n","Epoch 15/150\n","90/90 - 0s - loss: 11.9738 - val_loss: 12.0650\n","Epoch 16/150\n","90/90 - 0s - loss: 11.8195 - val_loss: 11.9716\n","Epoch 17/150\n","90/90 - 0s - loss: 11.7300 - val_loss: 11.8771\n","Epoch 18/150\n","90/90 - 0s - loss: 11.6621 - val_loss: 11.7411\n","Epoch 19/150\n","90/90 - 0s - loss: 11.4990 - val_loss: 11.6782\n","Epoch 20/150\n","90/90 - 0s - loss: 11.4348 - val_loss: 11.5933\n","Epoch 21/150\n","90/90 - 0s - loss: 11.3635 - val_loss: 11.4991\n","Epoch 22/150\n","90/90 - 0s - loss: 11.2760 - val_loss: 11.4286\n","Epoch 23/150\n","90/90 - 0s - loss: 11.1966 - val_loss: 11.3720\n","Epoch 24/150\n","90/90 - 0s - loss: 11.1392 - val_loss: 11.3033\n","Epoch 25/150\n","90/90 - 0s - loss: 11.0527 - val_loss: 11.2938\n","Epoch 26/150\n","90/90 - 0s - loss: 11.0362 - val_loss: 11.2322\n","Epoch 27/150\n","90/90 - 0s - loss: 10.9783 - val_loss: 11.2204\n","Epoch 28/150\n","90/90 - 0s - loss: 10.9137 - val_loss: 11.1477\n","Epoch 29/150\n","90/90 - 0s - loss: 10.8521 - val_loss: 11.1113\n","Epoch 30/150\n","90/90 - 0s - loss: 10.7860 - val_loss: 11.0952\n","Epoch 31/150\n","90/90 - 0s - loss: 10.7606 - val_loss: 11.0446\n","Epoch 32/150\n","90/90 - 0s - loss: 10.7120 - val_loss: 11.0058\n","Epoch 33/150\n","90/90 - 0s - loss: 10.6705 - val_loss: 10.9561\n","Epoch 34/150\n","90/90 - 0s - loss: 10.6392 - val_loss: 10.9546\n","Epoch 35/150\n","90/90 - 0s - loss: 10.5924 - val_loss: 10.9241\n","Epoch 36/150\n","90/90 - 0s - loss: 10.5804 - val_loss: 10.9138\n","Epoch 37/150\n","90/90 - 0s - loss: 10.5398 - val_loss: 10.8861\n","Epoch 38/150\n","90/90 - 0s - loss: 10.5708 - val_loss: 10.8979\n","Epoch 39/150\n","90/90 - 0s - loss: 10.4924 - val_loss: 10.8524\n","Epoch 40/150\n","90/90 - 0s - loss: 10.4793 - val_loss: 10.8428\n","Epoch 41/150\n","90/90 - 0s - loss: 10.4721 - val_loss: 10.8398\n","Epoch 42/150\n","90/90 - 0s - loss: 10.4375 - val_loss: 10.8844\n","Epoch 43/150\n","90/90 - 0s - loss: 10.4930 - val_loss: 10.7926\n","Epoch 44/150\n","90/90 - 0s - loss: 10.3898 - val_loss: 10.8059\n","Epoch 45/150\n","90/90 - 0s - loss: 10.4228 - val_loss: 10.7774\n","Epoch 46/150\n","90/90 - 0s - loss: 10.3873 - val_loss: 10.7751\n","Epoch 47/150\n","90/90 - 0s - loss: 10.3662 - val_loss: 10.7251\n","Epoch 48/150\n","90/90 - 0s - loss: 10.3453 - val_loss: 10.8014\n","Epoch 49/150\n","90/90 - 0s - loss: 10.3914 - val_loss: 10.7384\n","Epoch 50/150\n","90/90 - 0s - loss: 10.3187 - val_loss: 10.6883\n","Epoch 51/150\n","90/90 - 0s - loss: 10.3122 - val_loss: 10.7313\n","Epoch 52/150\n","90/90 - 0s - loss: 10.3113 - val_loss: 10.7824\n","Epoch 53/150\n","90/90 - 0s - loss: 10.3066 - val_loss: 10.6949\n","Epoch 54/150\n","90/90 - 0s - loss: 10.2981 - val_loss: 10.7573\n","Epoch 55/150\n","90/90 - 0s - loss: 10.3384 - val_loss: 10.6976\n","Epoch 56/150\n","90/90 - 0s - loss: 10.2908 - val_loss: 10.6568\n","Epoch 57/150\n","90/90 - 0s - loss: 10.2773 - val_loss: 10.7262\n","Epoch 58/150\n","90/90 - 0s - loss: 10.2718 - val_loss: 10.6570\n","Epoch 59/150\n","90/90 - 0s - loss: 10.2066 - val_loss: 10.6813\n","Epoch 60/150\n","90/90 - 0s - loss: 10.2222 - val_loss: 10.6246\n","Epoch 61/150\n","90/90 - 0s - loss: 10.1939 - val_loss: 10.6381\n","Epoch 62/150\n","90/90 - 0s - loss: 10.2050 - val_loss: 10.6438\n","Epoch 63/150\n","90/90 - 0s - loss: 10.2258 - val_loss: 10.6170\n","Epoch 64/150\n","90/90 - 0s - loss: 10.2041 - val_loss: 10.5870\n","Epoch 65/150\n","90/90 - 0s - loss: 10.1873 - val_loss: 10.7041\n","Epoch 66/150\n","90/90 - 0s - loss: 10.2299 - val_loss: 10.6494\n","Epoch 67/150\n","90/90 - 0s - loss: 10.2031 - val_loss: 10.6129\n","Epoch 68/150\n","90/90 - 0s - loss: 10.2190 - val_loss: 10.6588\n","Epoch 69/150\n","90/90 - 0s - loss: 10.2086 - val_loss: 10.6566\n","Epoch 70/150\n","90/90 - 0s - loss: 10.1884 - val_loss: 10.5756\n","Epoch 71/150\n","90/90 - 0s - loss: 10.1502 - val_loss: 10.6028\n","Epoch 72/150\n","90/90 - 0s - loss: 10.1677 - val_loss: 10.6214\n","Epoch 73/150\n","90/90 - 0s - loss: 10.1639 - val_loss: 10.5690\n","Epoch 74/150\n","90/90 - 0s - loss: 10.1542 - val_loss: 10.6024\n","Epoch 75/150\n","90/90 - 0s - loss: 10.1423 - val_loss: 10.5708\n","Epoch 76/150\n","90/90 - 0s - loss: 10.1296 - val_loss: 10.5479\n","Epoch 77/150\n","90/90 - 0s - loss: 10.1068 - val_loss: 10.5738\n","Epoch 78/150\n","90/90 - 0s - loss: 10.1314 - val_loss: 10.5984\n","Epoch 79/150\n","90/90 - 0s - loss: 10.1594 - val_loss: 10.5640\n","Epoch 80/150\n","90/90 - 0s - loss: 10.1126 - val_loss: 10.5260\n","Epoch 81/150\n","90/90 - 0s - loss: 10.0836 - val_loss: 10.5547\n","Epoch 82/150\n","90/90 - 0s - loss: 10.0917 - val_loss: 10.5157\n","Epoch 83/150\n","90/90 - 0s - loss: 10.0690 - val_loss: 10.5457\n","Epoch 84/150\n","90/90 - 0s - loss: 10.0625 - val_loss: 10.4742\n","Epoch 85/150\n","90/90 - 0s - loss: 10.0588 - val_loss: 10.5283\n","Epoch 86/150\n","90/90 - 0s - loss: 10.0377 - val_loss: 10.5156\n","Epoch 87/150\n","90/90 - 0s - loss: 10.0408 - val_loss: 10.4759\n","Epoch 88/150\n","90/90 - 0s - loss: 10.0343 - val_loss: 10.4537\n","Epoch 89/150\n","90/90 - 0s - loss: 10.0054 - val_loss: 10.4366\n","Epoch 90/150\n","90/90 - 0s - loss: 9.9945 - val_loss: 10.4946\n","Epoch 91/150\n","90/90 - 0s - loss: 10.0060 - val_loss: 10.4413\n","Epoch 92/150\n","90/90 - 0s - loss: 10.0028 - val_loss: 10.4394\n","Epoch 93/150\n","90/90 - 0s - loss: 10.0008 - val_loss: 10.4407\n","Epoch 94/150\n","90/90 - 0s - loss: 9.9750 - val_loss: 10.4326\n","Epoch 95/150\n","90/90 - 0s - loss: 9.9718 - val_loss: 10.4731\n","Epoch 96/150\n","90/90 - 0s - loss: 9.9906 - val_loss: 10.4316\n","Epoch 97/150\n","90/90 - 0s - loss: 9.9795 - val_loss: 10.4069\n","Epoch 98/150\n","90/90 - 0s - loss: 9.9876 - val_loss: 10.4561\n","Epoch 99/150\n","90/90 - 0s - loss: 9.9938 - val_loss: 10.4058\n","Epoch 100/150\n","90/90 - 0s - loss: 9.9496 - val_loss: 10.4137\n","Epoch 101/150\n","90/90 - 0s - loss: 9.9649 - val_loss: 10.3985\n","Epoch 102/150\n","90/90 - 0s - loss: 9.9374 - val_loss: 10.3760\n","Epoch 103/150\n","90/90 - 0s - loss: 9.9357 - val_loss: 10.3939\n","Epoch 104/150\n","90/90 - 0s - loss: 9.9767 - val_loss: 10.4064\n","Epoch 105/150\n","90/90 - 0s - loss: 9.9173 - val_loss: 10.4228\n","Epoch 106/150\n","90/90 - 0s - loss: 9.9450 - val_loss: 10.3785\n","Epoch 107/150\n","90/90 - 0s - loss: 9.9310 - val_loss: 10.4213\n","Epoch 108/150\n","90/90 - 0s - loss: 9.8966 - val_loss: 10.3585\n","Epoch 109/150\n","90/90 - 0s - loss: 9.8914 - val_loss: 10.3539\n","Epoch 110/150\n","90/90 - 0s - loss: 9.8568 - val_loss: 10.3316\n","Epoch 111/150\n","90/90 - 0s - loss: 9.8890 - val_loss: 10.3424\n","Epoch 112/150\n","90/90 - 0s - loss: 9.8904 - val_loss: 10.3313\n","Epoch 113/150\n","90/90 - 0s - loss: 9.8762 - val_loss: 10.3500\n","Epoch 114/150\n","90/90 - 0s - loss: 9.8844 - val_loss: 10.3312\n","Epoch 115/150\n","90/90 - 0s - loss: 9.8675 - val_loss: 10.3598\n","Epoch 116/150\n","90/90 - 0s - loss: 9.8881 - val_loss: 10.3461\n","Epoch 117/150\n","90/90 - 0s - loss: 9.8491 - val_loss: 10.3357\n","Epoch 118/150\n","90/90 - 0s - loss: 9.8756 - val_loss: 10.3603\n","Epoch 119/150\n","90/90 - 0s - loss: 9.8958 - val_loss: 10.3798\n","Epoch 120/150\n","90/90 - 0s - loss: 9.9113 - val_loss: 10.3112\n","Epoch 121/150\n","90/90 - 0s - loss: 9.8594 - val_loss: 10.3440\n","Epoch 122/150\n","90/90 - 0s - loss: 9.8589 - val_loss: 10.3273\n","Epoch 123/150\n","90/90 - 0s - loss: 9.8564 - val_loss: 10.3326\n","Epoch 124/150\n","90/90 - 0s - loss: 9.8474 - val_loss: 10.3136\n","Epoch 125/150\n","90/90 - 0s - loss: 9.8323 - val_loss: 10.3091\n","Epoch 126/150\n","90/90 - 0s - loss: 9.8281 - val_loss: 10.3163\n","Epoch 127/150\n","90/90 - 0s - loss: 9.8359 - val_loss: 10.2607\n","Epoch 128/150\n","90/90 - 0s - loss: 9.7845 - val_loss: 10.2909\n","Epoch 129/150\n","90/90 - 0s - loss: 9.8287 - val_loss: 10.2897\n","Epoch 130/150\n","90/90 - 0s - loss: 9.7875 - val_loss: 10.3324\n","Epoch 131/150\n","90/90 - 0s - loss: 9.8175 - val_loss: 10.4011\n","Epoch 132/150\n","90/90 - 0s - loss: 9.8441 - val_loss: 10.3153\n","Epoch 133/150\n","90/90 - 0s - loss: 9.8217 - val_loss: 10.3376\n","Epoch 134/150\n","90/90 - 0s - loss: 9.8037 - val_loss: 10.2988\n","Epoch 135/150\n","90/90 - 0s - loss: 9.8328 - val_loss: 10.2341\n","Epoch 136/150\n","90/90 - 0s - loss: 9.7589 - val_loss: 10.2984\n","Epoch 137/150\n","90/90 - 0s - loss: 9.7779 - val_loss: 10.2713\n","Epoch 138/150\n","90/90 - 0s - loss: 9.7626 - val_loss: 10.2653\n","Epoch 139/150\n","90/90 - 0s - loss: 9.7670 - val_loss: 10.2629\n","Epoch 140/150\n","90/90 - 0s - loss: 9.7813 - val_loss: 10.3265\n","Epoch 141/150\n","90/90 - 0s - loss: 9.7754 - val_loss: 10.2866\n","Epoch 142/150\n","90/90 - 0s - loss: 9.7547 - val_loss: 10.2986\n","Epoch 143/150\n","90/90 - 0s - loss: 9.7567 - val_loss: 10.2512\n","Epoch 144/150\n","90/90 - 0s - loss: 9.7507 - val_loss: 10.3171\n","Epoch 145/150\n","90/90 - 0s - loss: 9.7405 - val_loss: 10.2731\n","Epoch 146/150\n","90/90 - 0s - loss: 9.7656 - val_loss: 10.3007\n","Epoch 147/150\n","90/90 - 0s - loss: 9.7676 - val_loss: 10.2512\n","Epoch 148/150\n","90/90 - 0s - loss: 9.7326 - val_loss: 10.2363\n","Epoch 149/150\n","90/90 - 0s - loss: 9.7119 - val_loss: 10.2325\n","Epoch 150/150\n","90/90 - 0s - loss: 9.7184 - val_loss: 10.2186\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WEI3FRy345ly","executionInfo":{"status":"ok","timestamp":1604627286892,"user_tz":180,"elapsed":60593,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"86c5b00c-319d-43c1-ccee-0408e419ff20","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 15.545\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6C1UfGhU451L","executionInfo":{"status":"ok","timestamp":1604627286893,"user_tz":180,"elapsed":60589,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"fd0f5362-dad3-4323-fcec-18f8c5047e87","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 10.219\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jhhm2f-B45se","executionInfo":{"status":"ok","timestamp":1604627286894,"user_tz":180,"elapsed":60585,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"41e2b5ea-b150-4dd3-c645-d5ca1b901493","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.577\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KBSDcg1kR6rM"},"source":["#**(spline) janela=4**"]},{"cell_type":"code","metadata":{"id":"5zbZ4XTT5KHH","executionInfo":{"status":"ok","timestamp":1604627416428,"user_tz":180,"elapsed":65793,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"eeac9346-0998-4ea1-88cf-bff7169712d8","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 4 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 1)\n","reframed = series_to_supervised(scaled, n_hours, 4) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(300, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 18.8333 - val_loss: 18.2134\n","Epoch 2/150\n","90/90 - 0s - loss: 17.4647 - val_loss: 17.3999\n","Epoch 3/150\n","90/90 - 0s - loss: 16.8039 - val_loss: 16.7524\n","Epoch 4/150\n","90/90 - 0s - loss: 16.2952 - val_loss: 16.2402\n","Epoch 5/150\n","90/90 - 0s - loss: 15.8390 - val_loss: 15.8042\n","Epoch 6/150\n","90/90 - 0s - loss: 15.4441 - val_loss: 15.4556\n","Epoch 7/150\n","90/90 - 0s - loss: 15.1148 - val_loss: 15.1129\n","Epoch 8/150\n","90/90 - 0s - loss: 14.7811 - val_loss: 14.8330\n","Epoch 9/150\n","90/90 - 0s - loss: 14.5178 - val_loss: 14.5889\n","Epoch 10/150\n","90/90 - 0s - loss: 14.2946 - val_loss: 14.3862\n","Epoch 11/150\n","90/90 - 0s - loss: 14.0965 - val_loss: 14.1994\n","Epoch 12/150\n","90/90 - 0s - loss: 13.9123 - val_loss: 14.0241\n","Epoch 13/150\n","90/90 - 0s - loss: 13.7751 - val_loss: 13.9090\n","Epoch 14/150\n","90/90 - 0s - loss: 13.6394 - val_loss: 13.7659\n","Epoch 15/150\n","90/90 - 0s - loss: 13.5291 - val_loss: 13.6474\n","Epoch 16/150\n","90/90 - 0s - loss: 13.4050 - val_loss: 13.5433\n","Epoch 17/150\n","90/90 - 0s - loss: 13.3033 - val_loss: 13.5090\n","Epoch 18/150\n","90/90 - 0s - loss: 13.2751 - val_loss: 13.4027\n","Epoch 19/150\n","90/90 - 0s - loss: 13.1671 - val_loss: 13.3785\n","Epoch 20/150\n","90/90 - 0s - loss: 13.1377 - val_loss: 13.2978\n","Epoch 21/150\n","90/90 - 0s - loss: 13.1090 - val_loss: 13.2548\n","Epoch 22/150\n","90/90 - 0s - loss: 13.0407 - val_loss: 13.2215\n","Epoch 23/150\n","90/90 - 0s - loss: 12.9518 - val_loss: 13.1629\n","Epoch 24/150\n","90/90 - 0s - loss: 12.9452 - val_loss: 13.1085\n","Epoch 25/150\n","90/90 - 0s - loss: 12.9072 - val_loss: 13.0931\n","Epoch 26/150\n","90/90 - 0s - loss: 12.8947 - val_loss: 13.0773\n","Epoch 27/150\n","90/90 - 0s - loss: 12.8238 - val_loss: 13.0331\n","Epoch 28/150\n","90/90 - 0s - loss: 12.7786 - val_loss: 13.0328\n","Epoch 29/150\n","90/90 - 0s - loss: 12.7900 - val_loss: 12.9607\n","Epoch 30/150\n","90/90 - 0s - loss: 12.7261 - val_loss: 12.9410\n","Epoch 31/150\n","90/90 - 0s - loss: 12.7042 - val_loss: 12.8920\n","Epoch 32/150\n","90/90 - 0s - loss: 12.6614 - val_loss: 12.8681\n","Epoch 33/150\n","90/90 - 0s - loss: 12.6153 - val_loss: 12.8531\n","Epoch 34/150\n","90/90 - 0s - loss: 12.6274 - val_loss: 12.8323\n","Epoch 35/150\n","90/90 - 0s - loss: 12.5971 - val_loss: 12.8049\n","Epoch 36/150\n","90/90 - 0s - loss: 12.5631 - val_loss: 12.7865\n","Epoch 37/150\n","90/90 - 0s - loss: 12.5325 - val_loss: 12.7612\n","Epoch 38/150\n","90/90 - 0s - loss: 12.5069 - val_loss: 12.7515\n","Epoch 39/150\n","90/90 - 0s - loss: 12.4998 - val_loss: 12.7333\n","Epoch 40/150\n","90/90 - 0s - loss: 12.4585 - val_loss: 12.6933\n","Epoch 41/150\n","90/90 - 0s - loss: 12.4398 - val_loss: 12.7388\n","Epoch 42/150\n","90/90 - 0s - loss: 12.4547 - val_loss: 12.6924\n","Epoch 43/150\n","90/90 - 0s - loss: 12.4238 - val_loss: 12.6706\n","Epoch 44/150\n","90/90 - 0s - loss: 12.3858 - val_loss: 12.6534\n","Epoch 45/150\n","90/90 - 0s - loss: 12.3730 - val_loss: 12.6426\n","Epoch 46/150\n","90/90 - 0s - loss: 12.3612 - val_loss: 12.6081\n","Epoch 47/150\n","90/90 - 0s - loss: 12.3214 - val_loss: 12.5880\n","Epoch 48/150\n","90/90 - 0s - loss: 12.2957 - val_loss: 12.5770\n","Epoch 49/150\n","90/90 - 0s - loss: 12.2691 - val_loss: 12.5549\n","Epoch 50/150\n","90/90 - 0s - loss: 12.2571 - val_loss: 12.5171\n","Epoch 51/150\n","90/90 - 0s - loss: 12.1956 - val_loss: 12.4955\n","Epoch 52/150\n","90/90 - 0s - loss: 12.2189 - val_loss: 12.5097\n","Epoch 53/150\n","90/90 - 0s - loss: 12.1597 - val_loss: 12.4595\n","Epoch 54/150\n","90/90 - 0s - loss: 12.1352 - val_loss: 12.4326\n","Epoch 55/150\n","90/90 - 0s - loss: 12.0855 - val_loss: 12.3672\n","Epoch 56/150\n","90/90 - 0s - loss: 12.0384 - val_loss: 12.3527\n","Epoch 57/150\n","90/90 - 0s - loss: 12.0072 - val_loss: 12.3214\n","Epoch 58/150\n","90/90 - 0s - loss: 11.9632 - val_loss: 12.2977\n","Epoch 59/150\n","90/90 - 0s - loss: 11.9687 - val_loss: 12.2915\n","Epoch 60/150\n","90/90 - 0s - loss: 11.9457 - val_loss: 12.2617\n","Epoch 61/150\n","90/90 - 0s - loss: 11.9093 - val_loss: 12.2569\n","Epoch 62/150\n","90/90 - 0s - loss: 11.8771 - val_loss: 12.2562\n","Epoch 63/150\n","90/90 - 0s - loss: 11.8918 - val_loss: 12.2189\n","Epoch 64/150\n","90/90 - 0s - loss: 11.8409 - val_loss: 12.2329\n","Epoch 65/150\n","90/90 - 0s - loss: 11.8421 - val_loss: 12.1651\n","Epoch 66/150\n","90/90 - 0s - loss: 11.7946 - val_loss: 12.1815\n","Epoch 67/150\n","90/90 - 0s - loss: 11.7780 - val_loss: 12.1710\n","Epoch 68/150\n","90/90 - 0s - loss: 11.7703 - val_loss: 12.1410\n","Epoch 69/150\n","90/90 - 0s - loss: 11.7465 - val_loss: 12.1168\n","Epoch 70/150\n","90/90 - 0s - loss: 11.7414 - val_loss: 12.0911\n","Epoch 71/150\n","90/90 - 0s - loss: 11.6834 - val_loss: 12.0682\n","Epoch 72/150\n","90/90 - 0s - loss: 11.6715 - val_loss: 12.0363\n","Epoch 73/150\n","90/90 - 0s - loss: 11.6472 - val_loss: 12.0323\n","Epoch 74/150\n","90/90 - 0s - loss: 11.6397 - val_loss: 12.0025\n","Epoch 75/150\n","90/90 - 0s - loss: 11.6062 - val_loss: 11.9632\n","Epoch 76/150\n","90/90 - 0s - loss: 11.5946 - val_loss: 12.0492\n","Epoch 77/150\n","90/90 - 0s - loss: 11.5739 - val_loss: 11.9833\n","Epoch 78/150\n","90/90 - 0s - loss: 11.5768 - val_loss: 12.0028\n","Epoch 79/150\n","90/90 - 0s - loss: 11.5577 - val_loss: 11.9131\n","Epoch 80/150\n","90/90 - 0s - loss: 11.5195 - val_loss: 11.9106\n","Epoch 81/150\n","90/90 - 0s - loss: 11.5067 - val_loss: 11.9015\n","Epoch 82/150\n","90/90 - 0s - loss: 11.4874 - val_loss: 11.8698\n","Epoch 83/150\n","90/90 - 0s - loss: 11.4701 - val_loss: 11.8693\n","Epoch 84/150\n","90/90 - 0s - loss: 11.4523 - val_loss: 11.8289\n","Epoch 85/150\n","90/90 - 0s - loss: 11.4183 - val_loss: 11.8314\n","Epoch 86/150\n","90/90 - 0s - loss: 11.4302 - val_loss: 11.8246\n","Epoch 87/150\n","90/90 - 0s - loss: 11.3961 - val_loss: 11.7999\n","Epoch 88/150\n","90/90 - 0s - loss: 11.3853 - val_loss: 11.7945\n","Epoch 89/150\n","90/90 - 0s - loss: 11.3708 - val_loss: 11.8091\n","Epoch 90/150\n","90/90 - 0s - loss: 11.3629 - val_loss: 11.7840\n","Epoch 91/150\n","90/90 - 0s - loss: 11.3314 - val_loss: 11.7609\n","Epoch 92/150\n","90/90 - 0s - loss: 11.3246 - val_loss: 11.7811\n","Epoch 93/150\n","90/90 - 0s - loss: 11.3138 - val_loss: 11.7400\n","Epoch 94/150\n","90/90 - 0s - loss: 11.2990 - val_loss: 11.7513\n","Epoch 95/150\n","90/90 - 0s - loss: 11.2884 - val_loss: 11.7359\n","Epoch 96/150\n","90/90 - 0s - loss: 11.2732 - val_loss: 11.7334\n","Epoch 97/150\n","90/90 - 0s - loss: 11.2641 - val_loss: 11.6881\n","Epoch 98/150\n","90/90 - 0s - loss: 11.2422 - val_loss: 11.6870\n","Epoch 99/150\n","90/90 - 0s - loss: 11.2321 - val_loss: 11.6505\n","Epoch 100/150\n","90/90 - 0s - loss: 11.2095 - val_loss: 11.6778\n","Epoch 101/150\n","90/90 - 0s - loss: 11.2062 - val_loss: 11.6561\n","Epoch 102/150\n","90/90 - 0s - loss: 11.1911 - val_loss: 11.6223\n","Epoch 103/150\n","90/90 - 0s - loss: 11.1696 - val_loss: 11.6524\n","Epoch 104/150\n","90/90 - 0s - loss: 11.1863 - val_loss: 11.6269\n","Epoch 105/150\n","90/90 - 0s - loss: 11.1634 - val_loss: 11.6184\n","Epoch 106/150\n","90/90 - 0s - loss: 11.1489 - val_loss: 11.6139\n","Epoch 107/150\n","90/90 - 0s - loss: 11.1332 - val_loss: 11.6202\n","Epoch 108/150\n","90/90 - 0s - loss: 11.1418 - val_loss: 11.5934\n","Epoch 109/150\n","90/90 - 0s - loss: 11.1329 - val_loss: 11.5936\n","Epoch 110/150\n","90/90 - 0s - loss: 11.1071 - val_loss: 11.5749\n","Epoch 111/150\n","90/90 - 0s - loss: 11.1020 - val_loss: 11.5969\n","Epoch 112/150\n","90/90 - 0s - loss: 11.1109 - val_loss: 11.5727\n","Epoch 113/150\n","90/90 - 0s - loss: 11.0845 - val_loss: 11.5728\n","Epoch 114/150\n","90/90 - 0s - loss: 11.0863 - val_loss: 11.5695\n","Epoch 115/150\n","90/90 - 0s - loss: 11.0625 - val_loss: 11.5627\n","Epoch 116/150\n","90/90 - 0s - loss: 11.0631 - val_loss: 11.5428\n","Epoch 117/150\n","90/90 - 0s - loss: 11.0440 - val_loss: 11.5274\n","Epoch 118/150\n","90/90 - 0s - loss: 11.0442 - val_loss: 11.5298\n","Epoch 119/150\n","90/90 - 0s - loss: 11.0286 - val_loss: 11.5480\n","Epoch 120/150\n","90/90 - 0s - loss: 11.0341 - val_loss: 11.5169\n","Epoch 121/150\n","90/90 - 0s - loss: 11.0296 - val_loss: 11.5188\n","Epoch 122/150\n","90/90 - 0s - loss: 11.0149 - val_loss: 11.5276\n","Epoch 123/150\n","90/90 - 0s - loss: 11.0072 - val_loss: 11.5127\n","Epoch 124/150\n","90/90 - 0s - loss: 11.0206 - val_loss: 11.5171\n","Epoch 125/150\n","90/90 - 0s - loss: 10.9874 - val_loss: 11.5130\n","Epoch 126/150\n","90/90 - 0s - loss: 10.9911 - val_loss: 11.4727\n","Epoch 127/150\n","90/90 - 0s - loss: 10.9660 - val_loss: 11.4798\n","Epoch 128/150\n","90/90 - 0s - loss: 10.9607 - val_loss: 11.4852\n","Epoch 129/150\n","90/90 - 0s - loss: 10.9683 - val_loss: 11.4920\n","Epoch 130/150\n","90/90 - 0s - loss: 10.9631 - val_loss: 11.4915\n","Epoch 131/150\n","90/90 - 0s - loss: 10.9512 - val_loss: 11.4903\n","Epoch 132/150\n","90/90 - 0s - loss: 10.9402 - val_loss: 11.4493\n","Epoch 133/150\n","90/90 - 0s - loss: 10.9199 - val_loss: 11.4716\n","Epoch 134/150\n","90/90 - 0s - loss: 10.9371 - val_loss: 11.4819\n","Epoch 135/150\n","90/90 - 0s - loss: 10.9255 - val_loss: 11.4630\n","Epoch 136/150\n","90/90 - 0s - loss: 10.9161 - val_loss: 11.4630\n","Epoch 137/150\n","90/90 - 0s - loss: 10.9154 - val_loss: 11.4769\n","Epoch 138/150\n","90/90 - 0s - loss: 10.9055 - val_loss: 11.4807\n","Epoch 139/150\n","90/90 - 0s - loss: 10.9254 - val_loss: 11.4461\n","Epoch 140/150\n","90/90 - 0s - loss: 10.8928 - val_loss: 11.4518\n","Epoch 141/150\n","90/90 - 0s - loss: 10.9173 - val_loss: 11.4124\n","Epoch 142/150\n","90/90 - 0s - loss: 10.8872 - val_loss: 11.4069\n","Epoch 143/150\n","90/90 - 0s - loss: 10.8687 - val_loss: 11.4083\n","Epoch 144/150\n","90/90 - 0s - loss: 10.8674 - val_loss: 11.3856\n","Epoch 145/150\n","90/90 - 0s - loss: 10.8558 - val_loss: 11.4070\n","Epoch 146/150\n","90/90 - 0s - loss: 10.8652 - val_loss: 11.3856\n","Epoch 147/150\n","90/90 - 0s - loss: 10.8448 - val_loss: 11.3811\n","Epoch 148/150\n","90/90 - 0s - loss: 10.8352 - val_loss: 11.3890\n","Epoch 149/150\n","90/90 - 0s - loss: 10.8338 - val_loss: 11.3891\n","Epoch 150/150\n","90/90 - 0s - loss: 10.8324 - val_loss: 11.3799\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AVDZ_NeT5KQ3","executionInfo":{"status":"ok","timestamp":1604627417348,"user_tz":180,"elapsed":66706,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"844b0ab7-3c23-46d0-cc46-af4bfb7b2894","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 16.950\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FJUTAb9B5KaK","executionInfo":{"status":"ok","timestamp":1604627417350,"user_tz":180,"elapsed":66703,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"30054eee-d255-441c-d829-352545121574","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 11.380\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bo62JeJX5Kjp","executionInfo":{"status":"ok","timestamp":1604627417351,"user_tz":180,"elapsed":66699,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"ff6d7897-1ae6-4e76-afd0-a52b97226cf5","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.498\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lnHTYKYyT9wX"},"source":["#**(mask) janela=1**"]},{"cell_type":"code","metadata":{"id":"0TJmKupW91GB","executionInfo":{"status":"ok","timestamp":1604627555704,"user_tz":180,"elapsed":88654,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"5f94b028-c159-4735-dc29-151803a6844f","colab":{"base_uri":"https://localhost:8080/"}},"source":["df_sc_bruto=df_sc_2\n","df_sc_bruto.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","df_sc_bruto_copy=df_sc_bruto.copy()\n","df_sc_bruto_copy=df_sc_bruto_copy[['O3']]\n","df_sc_bruto.drop(columns=['O3'],inplace=True)\n","df_sc_bruto = pd.concat([df_sc_bruto_copy,df_sc_bruto], axis=1, ignore_index=False)\n","\n","# prepare data for lstm\n","# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","from keras.layers import Masking\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","import numpy as np\n","\n","values = df_sc_bruto.values\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(df_sc_bruto)\n","scaled=values\n","df=pd.DataFrame(scaled)\n","df = df.replace(np.NaN,-9999)\n","values = df.values\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 1\n","n_features = 10\n","\n","# frame as supervised learning\n","reframed = series_to_supervised(scaled, n_hours, 1) #janela\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(Masking(mask_value=-9999, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(LSTM(300))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt, sample_weight_mode='temporal')\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 2s - loss: 19.1054 - val_loss: 23.6370\n","Epoch 2/150\n","90/90 - 1s - loss: 17.0125 - val_loss: 21.6635\n","Epoch 3/150\n","90/90 - 1s - loss: 15.5304 - val_loss: 20.0050\n","Epoch 4/150\n","90/90 - 1s - loss: 14.2512 - val_loss: 18.5535\n","Epoch 5/150\n","90/90 - 1s - loss: 13.1329 - val_loss: 17.2487\n","Epoch 6/150\n","90/90 - 1s - loss: 12.2315 - val_loss: 16.2223\n","Epoch 7/150\n","90/90 - 1s - loss: 11.4601 - val_loss: 15.2963\n","Epoch 8/150\n","90/90 - 1s - loss: 10.8016 - val_loss: 14.4880\n","Epoch 9/150\n","90/90 - 1s - loss: 10.2670 - val_loss: 13.7827\n","Epoch 10/150\n","90/90 - 1s - loss: 9.8056 - val_loss: 13.1900\n","Epoch 11/150\n","90/90 - 1s - loss: 9.4007 - val_loss: 12.7047\n","Epoch 12/150\n","90/90 - 1s - loss: 9.0418 - val_loss: 12.2022\n","Epoch 13/150\n","90/90 - 1s - loss: 8.7495 - val_loss: 11.8078\n","Epoch 14/150\n","90/90 - 1s - loss: 8.4950 - val_loss: 11.4624\n","Epoch 15/150\n","90/90 - 1s - loss: 8.2502 - val_loss: 11.1106\n","Epoch 16/150\n","90/90 - 1s - loss: 8.0518 - val_loss: 10.8646\n","Epoch 17/150\n","90/90 - 1s - loss: 7.8776 - val_loss: 10.6326\n","Epoch 18/150\n","90/90 - 1s - loss: 7.7239 - val_loss: 10.4552\n","Epoch 19/150\n","90/90 - 1s - loss: 7.5893 - val_loss: 10.1669\n","Epoch 20/150\n","90/90 - 1s - loss: 7.4549 - val_loss: 10.0325\n","Epoch 21/150\n","90/90 - 1s - loss: 7.3536 - val_loss: 9.8492\n","Epoch 22/150\n","90/90 - 1s - loss: 7.2411 - val_loss: 9.7656\n","Epoch 23/150\n","90/90 - 1s - loss: 7.1649 - val_loss: 9.6700\n","Epoch 24/150\n","90/90 - 1s - loss: 7.0767 - val_loss: 9.4550\n","Epoch 25/150\n","90/90 - 1s - loss: 7.0024 - val_loss: 9.3427\n","Epoch 26/150\n","90/90 - 1s - loss: 6.9847 - val_loss: 9.3994\n","Epoch 27/150\n","90/90 - 1s - loss: 6.9517 - val_loss: 9.2835\n","Epoch 28/150\n","90/90 - 1s - loss: 6.8719 - val_loss: 9.2017\n","Epoch 29/150\n","90/90 - 1s - loss: 6.8107 - val_loss: 9.1596\n","Epoch 30/150\n","90/90 - 1s - loss: 6.7633 - val_loss: 9.0305\n","Epoch 31/150\n","90/90 - 1s - loss: 6.7018 - val_loss: 8.9885\n","Epoch 32/150\n","90/90 - 1s - loss: 6.6688 - val_loss: 8.9166\n","Epoch 33/150\n","90/90 - 1s - loss: 6.6195 - val_loss: 8.8185\n","Epoch 34/150\n","90/90 - 1s - loss: 6.5676 - val_loss: 8.7910\n","Epoch 35/150\n","90/90 - 1s - loss: 6.5478 - val_loss: 8.7309\n","Epoch 36/150\n","90/90 - 1s - loss: 6.5183 - val_loss: 8.7043\n","Epoch 37/150\n","90/90 - 1s - loss: 6.4862 - val_loss: 8.6382\n","Epoch 38/150\n","90/90 - 1s - loss: 6.4454 - val_loss: 8.5811\n","Epoch 39/150\n","90/90 - 1s - loss: 6.4254 - val_loss: 8.5526\n","Epoch 40/150\n","90/90 - 1s - loss: 6.3950 - val_loss: 8.4963\n","Epoch 41/150\n","90/90 - 1s - loss: 6.3646 - val_loss: 8.4583\n","Epoch 42/150\n","90/90 - 1s - loss: 6.3470 - val_loss: 8.3880\n","Epoch 43/150\n","90/90 - 1s - loss: 6.3208 - val_loss: 8.3857\n","Epoch 44/150\n","90/90 - 1s - loss: 6.3003 - val_loss: 8.3363\n","Epoch 45/150\n","90/90 - 1s - loss: 6.2720 - val_loss: 8.3310\n","Epoch 46/150\n","90/90 - 1s - loss: 6.2605 - val_loss: 8.2920\n","Epoch 47/150\n","90/90 - 1s - loss: 6.2516 - val_loss: 8.2755\n","Epoch 48/150\n","90/90 - 1s - loss: 6.2226 - val_loss: 8.2302\n","Epoch 49/150\n","90/90 - 1s - loss: 6.2222 - val_loss: 8.2258\n","Epoch 50/150\n","90/90 - 1s - loss: 6.1996 - val_loss: 8.1739\n","Epoch 51/150\n","90/90 - 1s - loss: 6.1825 - val_loss: 8.2024\n","Epoch 52/150\n","90/90 - 1s - loss: 6.1763 - val_loss: 8.1447\n","Epoch 53/150\n","90/90 - 1s - loss: 6.1612 - val_loss: 8.1063\n","Epoch 54/150\n","90/90 - 1s - loss: 6.1437 - val_loss: 8.1150\n","Epoch 55/150\n","90/90 - 1s - loss: 6.1440 - val_loss: 8.0874\n","Epoch 56/150\n","90/90 - 1s - loss: 6.1242 - val_loss: 8.0431\n","Epoch 57/150\n","90/90 - 1s - loss: 6.1098 - val_loss: 8.0330\n","Epoch 58/150\n","90/90 - 1s - loss: 6.1028 - val_loss: 8.0018\n","Epoch 59/150\n","90/90 - 1s - loss: 6.0975 - val_loss: 7.9951\n","Epoch 60/150\n","90/90 - 1s - loss: 6.0916 - val_loss: 7.9900\n","Epoch 61/150\n","90/90 - 1s - loss: 6.0721 - val_loss: 7.9699\n","Epoch 62/150\n","90/90 - 1s - loss: 6.0707 - val_loss: 7.9565\n","Epoch 63/150\n","90/90 - 1s - loss: 6.0564 - val_loss: 7.9691\n","Epoch 64/150\n","90/90 - 1s - loss: 6.0472 - val_loss: 7.9271\n","Epoch 65/150\n","90/90 - 1s - loss: 6.0540 - val_loss: 7.8960\n","Epoch 66/150\n","90/90 - 1s - loss: 6.0422 - val_loss: 7.9213\n","Epoch 67/150\n","90/90 - 1s - loss: 6.0366 - val_loss: 7.8951\n","Epoch 68/150\n","90/90 - 1s - loss: 6.0245 - val_loss: 7.8803\n","Epoch 69/150\n","90/90 - 1s - loss: 6.0110 - val_loss: 7.8468\n","Epoch 70/150\n","90/90 - 1s - loss: 6.0105 - val_loss: 7.8376\n","Epoch 71/150\n","90/90 - 1s - loss: 6.0099 - val_loss: 7.8387\n","Epoch 72/150\n","90/90 - 1s - loss: 5.9966 - val_loss: 7.8202\n","Epoch 73/150\n","90/90 - 1s - loss: 5.9933 - val_loss: 7.8103\n","Epoch 74/150\n","90/90 - 1s - loss: 5.9918 - val_loss: 7.8415\n","Epoch 75/150\n","90/90 - 1s - loss: 5.9785 - val_loss: 7.8087\n","Epoch 76/150\n","90/90 - 1s - loss: 5.9671 - val_loss: 7.8130\n","Epoch 77/150\n","90/90 - 1s - loss: 5.9744 - val_loss: 7.8093\n","Epoch 78/150\n","90/90 - 1s - loss: 5.9617 - val_loss: 7.7868\n","Epoch 79/150\n","90/90 - 1s - loss: 5.9588 - val_loss: 7.7815\n","Epoch 80/150\n","90/90 - 1s - loss: 5.9691 - val_loss: 7.7612\n","Epoch 81/150\n","90/90 - 1s - loss: 5.9578 - val_loss: 7.7637\n","Epoch 82/150\n","90/90 - 1s - loss: 5.9590 - val_loss: 7.7513\n","Epoch 83/150\n","90/90 - 1s - loss: 5.9440 - val_loss: 7.7428\n","Epoch 84/150\n","90/90 - 1s - loss: 5.9413 - val_loss: 7.7425\n","Epoch 85/150\n","90/90 - 1s - loss: 5.9290 - val_loss: 7.7304\n","Epoch 86/150\n","90/90 - 1s - loss: 5.9414 - val_loss: 7.7083\n","Epoch 87/150\n","90/90 - 1s - loss: 5.9296 - val_loss: 7.6807\n","Epoch 88/150\n","90/90 - 1s - loss: 5.9222 - val_loss: 7.6794\n","Epoch 89/150\n","90/90 - 1s - loss: 5.9231 - val_loss: 7.7009\n","Epoch 90/150\n","90/90 - 1s - loss: 5.9186 - val_loss: 7.6899\n","Epoch 91/150\n","90/90 - 1s - loss: 5.9257 - val_loss: 7.6726\n","Epoch 92/150\n","90/90 - 1s - loss: 5.9178 - val_loss: 7.6710\n","Epoch 93/150\n","90/90 - 1s - loss: 5.9110 - val_loss: 7.6547\n","Epoch 94/150\n","90/90 - 1s - loss: 5.8966 - val_loss: 7.6631\n","Epoch 95/150\n","90/90 - 1s - loss: 5.8932 - val_loss: 7.6431\n","Epoch 96/150\n","90/90 - 1s - loss: 5.8949 - val_loss: 7.6461\n","Epoch 97/150\n","90/90 - 1s - loss: 5.8870 - val_loss: 7.6248\n","Epoch 98/150\n","90/90 - 1s - loss: 5.8835 - val_loss: 7.6172\n","Epoch 99/150\n","90/90 - 1s - loss: 5.8763 - val_loss: 7.6181\n","Epoch 100/150\n","90/90 - 1s - loss: 5.8830 - val_loss: 7.6044\n","Epoch 101/150\n","90/90 - 1s - loss: 5.8787 - val_loss: 7.5995\n","Epoch 102/150\n","90/90 - 1s - loss: 5.8866 - val_loss: 7.6019\n","Epoch 103/150\n","90/90 - 1s - loss: 5.8714 - val_loss: 7.5943\n","Epoch 104/150\n","90/90 - 1s - loss: 5.8657 - val_loss: 7.5879\n","Epoch 105/150\n","90/90 - 1s - loss: 5.8557 - val_loss: 7.5821\n","Epoch 106/150\n","90/90 - 1s - loss: 5.8535 - val_loss: 7.5811\n","Epoch 107/150\n","90/90 - 1s - loss: 5.8687 - val_loss: 7.5790\n","Epoch 108/150\n","90/90 - 1s - loss: 5.8538 - val_loss: 7.5780\n","Epoch 109/150\n","90/90 - 1s - loss: 5.8596 - val_loss: 7.5678\n","Epoch 110/150\n","90/90 - 1s - loss: 5.8434 - val_loss: 7.5777\n","Epoch 111/150\n","90/90 - 1s - loss: 5.8529 - val_loss: 7.5707\n","Epoch 112/150\n","90/90 - 1s - loss: 5.8532 - val_loss: 7.5510\n","Epoch 113/150\n","90/90 - 1s - loss: 5.8483 - val_loss: 7.5550\n","Epoch 114/150\n","90/90 - 1s - loss: 5.8537 - val_loss: 7.5615\n","Epoch 115/150\n","90/90 - 1s - loss: 5.8444 - val_loss: 7.5832\n","Epoch 116/150\n","90/90 - 1s - loss: 5.8468 - val_loss: 7.6057\n","Epoch 117/150\n","90/90 - 1s - loss: 5.8386 - val_loss: 7.5595\n","Epoch 118/150\n","90/90 - 1s - loss: 5.8378 - val_loss: 7.5650\n","Epoch 119/150\n","90/90 - 1s - loss: 5.8407 - val_loss: 7.5722\n","Epoch 120/150\n","90/90 - 1s - loss: 5.8307 - val_loss: 7.5459\n","Epoch 121/150\n","90/90 - 1s - loss: 5.8322 - val_loss: 7.5333\n","Epoch 122/150\n","90/90 - 1s - loss: 5.8356 - val_loss: 7.5415\n","Epoch 123/150\n","90/90 - 1s - loss: 5.8249 - val_loss: 7.5286\n","Epoch 124/150\n","90/90 - 1s - loss: 5.8338 - val_loss: 7.5449\n","Epoch 125/150\n","90/90 - 1s - loss: 5.8187 - val_loss: 7.5398\n","Epoch 126/150\n","90/90 - 1s - loss: 5.8208 - val_loss: 7.5228\n","Epoch 127/150\n","90/90 - 1s - loss: 5.8189 - val_loss: 7.5218\n","Epoch 128/150\n","90/90 - 1s - loss: 5.8210 - val_loss: 7.5252\n","Epoch 129/150\n","90/90 - 1s - loss: 5.8122 - val_loss: 7.4860\n","Epoch 130/150\n","90/90 - 1s - loss: 5.8114 - val_loss: 7.5089\n","Epoch 131/150\n","90/90 - 1s - loss: 5.8097 - val_loss: 7.5271\n","Epoch 132/150\n","90/90 - 1s - loss: 5.8075 - val_loss: 7.5067\n","Epoch 133/150\n","90/90 - 1s - loss: 5.8133 - val_loss: 7.5378\n","Epoch 134/150\n","90/90 - 1s - loss: 5.8171 - val_loss: 7.5294\n","Epoch 135/150\n","90/90 - 1s - loss: 5.8092 - val_loss: 7.5043\n","Epoch 136/150\n","90/90 - 1s - loss: 5.7957 - val_loss: 7.5167\n","Epoch 137/150\n","90/90 - 1s - loss: 5.7982 - val_loss: 7.4918\n","Epoch 138/150\n","90/90 - 1s - loss: 5.7963 - val_loss: 7.5135\n","Epoch 139/150\n","90/90 - 1s - loss: 5.7968 - val_loss: 7.5149\n","Epoch 140/150\n","90/90 - 1s - loss: 5.7951 - val_loss: 7.5115\n","Epoch 141/150\n","90/90 - 1s - loss: 5.7977 - val_loss: 7.4839\n","Epoch 142/150\n","90/90 - 1s - loss: 5.7972 - val_loss: 7.5145\n","Epoch 143/150\n","90/90 - 1s - loss: 5.8056 - val_loss: 7.5180\n","Epoch 144/150\n","90/90 - 1s - loss: 5.8011 - val_loss: 7.5018\n","Epoch 145/150\n","90/90 - 1s - loss: 5.7930 - val_loss: 7.5024\n","Epoch 146/150\n","90/90 - 1s - loss: 5.7937 - val_loss: 7.4665\n","Epoch 147/150\n","90/90 - 1s - loss: 5.7999 - val_loss: 7.4755\n","Epoch 148/150\n","90/90 - 1s - loss: 5.7952 - val_loss: 7.4949\n","Epoch 149/150\n","90/90 - 1s - loss: 5.8006 - val_loss: 7.4773\n","Epoch 150/150\n","90/90 - 1s - loss: 5.7940 - val_loss: 7.4738\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ps_pd0W8A4l9","executionInfo":{"status":"ok","timestamp":1604627557337,"user_tz":180,"elapsed":90261,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"aecce34d-f6be-4fda-aca6-fe0573a0ceb3","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 12.310\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DBw0H0quA4Up","executionInfo":{"status":"ok","timestamp":1604627557340,"user_tz":180,"elapsed":90259,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"ecb22b1f-e2db-4a42-a9fd-2d5cfa4e6da7","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 7.474\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0OaaquzGA5vd","executionInfo":{"status":"ok","timestamp":1604627557342,"user_tz":180,"elapsed":90256,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"92a08e0c-ccab-4268-f5d8-c78be82cfb99","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.816\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rVxsl8ZYT9ne"},"source":["#**(mask) janela=2**"]},{"cell_type":"code","metadata":{"id":"7huCHgPeEghf","executionInfo":{"status":"ok","timestamp":1604628496653,"user_tz":180,"elapsed":90386,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"8584ff6a-8835-4be2-ab1a-9f623b64a48c","colab":{"base_uri":"https://localhost:8080/"}},"source":["df_sc_bruto=df_sc_2\n","df_sc_bruto.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","df_sc_bruto_copy=df_sc_bruto.copy()\n","df_sc_bruto_copy=df_sc_bruto_copy[['O3']]\n","df_sc_bruto.drop(columns=['O3'],inplace=True)\n","df_sc_bruto = pd.concat([df_sc_bruto_copy,df_sc_bruto], axis=1, ignore_index=False)\n","\n","# prepare data for lstm\n","# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","from keras.layers import Masking\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","import numpy as np\n","\n","values = df_sc_bruto.values\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(df_sc_bruto)\n","scaled=values\n","df=pd.DataFrame(scaled)\n","df = df.replace(np.NaN,-9999)\n","values = df.values\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 2\n","n_features = 10\n","\n","# frame as supervised learning\n","reframed = series_to_supervised(scaled, n_hours, 2) #janela\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(Masking(mask_value=-9999, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(LSTM(300))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt, sample_weight_mode='temporal')\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 2s - loss: 19.3857 - val_loss: 25.2930\n","Epoch 2/150\n","90/90 - 1s - loss: 16.7847 - val_loss: 22.9869\n","Epoch 3/150\n","90/90 - 1s - loss: 15.4236 - val_loss: 21.4638\n","Epoch 4/150\n","90/90 - 1s - loss: 14.3298 - val_loss: 20.0416\n","Epoch 5/150\n","90/90 - 1s - loss: 13.4321 - val_loss: 18.9668\n","Epoch 6/150\n","90/90 - 1s - loss: 12.7176 - val_loss: 18.0733\n","Epoch 7/150\n","90/90 - 1s - loss: 12.1053 - val_loss: 17.2617\n","Epoch 8/150\n","90/90 - 1s - loss: 11.6211 - val_loss: 16.7186\n","Epoch 9/150\n","90/90 - 1s - loss: 11.1818 - val_loss: 16.1021\n","Epoch 10/150\n","90/90 - 1s - loss: 10.8732 - val_loss: 15.6428\n","Epoch 11/150\n","90/90 - 1s - loss: 10.5903 - val_loss: 15.2283\n","Epoch 12/150\n","90/90 - 1s - loss: 10.3580 - val_loss: 14.8993\n","Epoch 13/150\n","90/90 - 1s - loss: 10.1688 - val_loss: 14.5684\n","Epoch 14/150\n","90/90 - 1s - loss: 9.9879 - val_loss: 14.3331\n","Epoch 15/150\n","90/90 - 1s - loss: 9.8634 - val_loss: 14.1320\n","Epoch 16/150\n","90/90 - 1s - loss: 9.7201 - val_loss: 13.9439\n","Epoch 17/150\n","90/90 - 1s - loss: 9.6147 - val_loss: 13.8285\n","Epoch 18/150\n","90/90 - 1s - loss: 9.5083 - val_loss: 13.6940\n","Epoch 19/150\n","90/90 - 1s - loss: 9.4473 - val_loss: 13.5315\n","Epoch 20/150\n","90/90 - 1s - loss: 9.3476 - val_loss: 13.4703\n","Epoch 21/150\n","90/90 - 1s - loss: 9.2849 - val_loss: 13.3345\n","Epoch 22/150\n","90/90 - 1s - loss: 9.2049 - val_loss: 13.2747\n","Epoch 23/150\n","90/90 - 1s - loss: 9.1389 - val_loss: 13.1432\n","Epoch 24/150\n","90/90 - 1s - loss: 9.0987 - val_loss: 13.0603\n","Epoch 25/150\n","90/90 - 1s - loss: 9.0824 - val_loss: 13.0191\n","Epoch 26/150\n","90/90 - 1s - loss: 9.0468 - val_loss: 12.9885\n","Epoch 27/150\n","90/90 - 1s - loss: 9.0078 - val_loss: 12.9172\n","Epoch 28/150\n","90/90 - 1s - loss: 8.9445 - val_loss: 12.8640\n","Epoch 29/150\n","90/90 - 1s - loss: 8.9217 - val_loss: 12.7807\n","Epoch 30/150\n","90/90 - 1s - loss: 8.8903 - val_loss: 12.8270\n","Epoch 31/150\n","90/90 - 1s - loss: 8.8601 - val_loss: 12.8253\n","Epoch 32/150\n","90/90 - 1s - loss: 8.8398 - val_loss: 12.7375\n","Epoch 33/150\n","90/90 - 1s - loss: 8.8247 - val_loss: 12.6677\n","Epoch 34/150\n","90/90 - 1s - loss: 8.7949 - val_loss: 12.6306\n","Epoch 35/150\n","90/90 - 1s - loss: 8.7757 - val_loss: 12.6494\n","Epoch 36/150\n","90/90 - 1s - loss: 8.7427 - val_loss: 12.6024\n","Epoch 37/150\n","90/90 - 1s - loss: 8.7468 - val_loss: 12.6083\n","Epoch 38/150\n","90/90 - 1s - loss: 8.7140 - val_loss: 12.5142\n","Epoch 39/150\n","90/90 - 1s - loss: 8.7097 - val_loss: 12.4840\n","Epoch 40/150\n","90/90 - 1s - loss: 8.7043 - val_loss: 12.4002\n","Epoch 41/150\n","90/90 - 1s - loss: 8.6686 - val_loss: 12.4376\n","Epoch 42/150\n","90/90 - 1s - loss: 8.6541 - val_loss: 12.4574\n","Epoch 43/150\n","90/90 - 1s - loss: 8.6356 - val_loss: 12.4724\n","Epoch 44/150\n","90/90 - 1s - loss: 8.6306 - val_loss: 12.4161\n","Epoch 45/150\n","90/90 - 1s - loss: 8.6294 - val_loss: 12.3234\n","Epoch 46/150\n","90/90 - 1s - loss: 8.6177 - val_loss: 12.2904\n","Epoch 47/150\n","90/90 - 1s - loss: 8.5987 - val_loss: 12.2498\n","Epoch 48/150\n","90/90 - 1s - loss: 8.5758 - val_loss: 12.2465\n","Epoch 49/150\n","90/90 - 1s - loss: 8.5632 - val_loss: 12.2759\n","Epoch 50/150\n","90/90 - 1s - loss: 8.5538 - val_loss: 12.2899\n","Epoch 51/150\n","90/90 - 1s - loss: 8.5414 - val_loss: 12.2322\n","Epoch 52/150\n","90/90 - 1s - loss: 8.5378 - val_loss: 12.2536\n","Epoch 53/150\n","90/90 - 1s - loss: 8.5431 - val_loss: 12.2192\n","Epoch 54/150\n","90/90 - 1s - loss: 8.5011 - val_loss: 12.2193\n","Epoch 55/150\n","90/90 - 1s - loss: 8.5016 - val_loss: 12.1841\n","Epoch 56/150\n","90/90 - 1s - loss: 8.4951 - val_loss: 12.1817\n","Epoch 57/150\n","90/90 - 1s - loss: 8.4894 - val_loss: 12.2428\n","Epoch 58/150\n","90/90 - 1s - loss: 8.4798 - val_loss: 12.1965\n","Epoch 59/150\n","90/90 - 1s - loss: 8.4888 - val_loss: 12.1902\n","Epoch 60/150\n","90/90 - 1s - loss: 8.4633 - val_loss: 12.1559\n","Epoch 61/150\n","90/90 - 1s - loss: 8.4747 - val_loss: 12.1279\n","Epoch 62/150\n","90/90 - 1s - loss: 8.4449 - val_loss: 12.1805\n","Epoch 63/150\n","90/90 - 1s - loss: 8.4376 - val_loss: 12.1163\n","Epoch 64/150\n","90/90 - 1s - loss: 8.4426 - val_loss: 12.0878\n","Epoch 65/150\n","90/90 - 1s - loss: 8.4274 - val_loss: 12.1323\n","Epoch 66/150\n","90/90 - 1s - loss: 8.4244 - val_loss: 12.0916\n","Epoch 67/150\n","90/90 - 1s - loss: 8.4023 - val_loss: 12.1663\n","Epoch 68/150\n","90/90 - 1s - loss: 8.4192 - val_loss: 12.1052\n","Epoch 69/150\n","90/90 - 1s - loss: 8.4128 - val_loss: 12.1252\n","Epoch 70/150\n","90/90 - 1s - loss: 8.4040 - val_loss: 12.0370\n","Epoch 71/150\n","90/90 - 1s - loss: 8.3765 - val_loss: 12.0562\n","Epoch 72/150\n","90/90 - 1s - loss: 8.3902 - val_loss: 12.0325\n","Epoch 73/150\n","90/90 - 1s - loss: 8.3842 - val_loss: 12.0361\n","Epoch 74/150\n","90/90 - 1s - loss: 8.3812 - val_loss: 11.9768\n","Epoch 75/150\n","90/90 - 1s - loss: 8.3746 - val_loss: 12.0108\n","Epoch 76/150\n","90/90 - 1s - loss: 8.3708 - val_loss: 11.9796\n","Epoch 77/150\n","90/90 - 1s - loss: 8.3690 - val_loss: 11.9907\n","Epoch 78/150\n","90/90 - 1s - loss: 8.3643 - val_loss: 12.0030\n","Epoch 79/150\n","90/90 - 1s - loss: 8.3503 - val_loss: 11.9433\n","Epoch 80/150\n","90/90 - 1s - loss: 8.3543 - val_loss: 11.9679\n","Epoch 81/150\n","90/90 - 1s - loss: 8.3508 - val_loss: 12.0378\n","Epoch 82/150\n","90/90 - 1s - loss: 8.3530 - val_loss: 11.9962\n","Epoch 83/150\n","90/90 - 1s - loss: 8.3450 - val_loss: 12.0141\n","Epoch 84/150\n","90/90 - 1s - loss: 8.3352 - val_loss: 11.9767\n","Epoch 85/150\n","90/90 - 1s - loss: 8.3372 - val_loss: 11.9690\n","Epoch 86/150\n","90/90 - 1s - loss: 8.3252 - val_loss: 11.9004\n","Epoch 87/150\n","90/90 - 1s - loss: 8.3276 - val_loss: 11.8843\n","Epoch 88/150\n","90/90 - 1s - loss: 8.3380 - val_loss: 11.9147\n","Epoch 89/150\n","90/90 - 1s - loss: 8.3216 - val_loss: 11.8709\n","Epoch 90/150\n","90/90 - 1s - loss: 8.3184 - val_loss: 11.9165\n","Epoch 91/150\n","90/90 - 1s - loss: 8.3182 - val_loss: 11.8622\n","Epoch 92/150\n","90/90 - 1s - loss: 8.3207 - val_loss: 11.8603\n","Epoch 93/150\n","90/90 - 1s - loss: 8.3056 - val_loss: 11.8525\n","Epoch 94/150\n","90/90 - 1s - loss: 8.3002 - val_loss: 11.8869\n","Epoch 95/150\n","90/90 - 1s - loss: 8.2884 - val_loss: 11.8856\n","Epoch 96/150\n","90/90 - 1s - loss: 8.2939 - val_loss: 11.8815\n","Epoch 97/150\n","90/90 - 1s - loss: 8.2955 - val_loss: 11.8080\n","Epoch 98/150\n","90/90 - 1s - loss: 8.2851 - val_loss: 11.8568\n","Epoch 99/150\n","90/90 - 1s - loss: 8.2911 - val_loss: 11.8451\n","Epoch 100/150\n","90/90 - 1s - loss: 8.2856 - val_loss: 11.7964\n","Epoch 101/150\n","90/90 - 1s - loss: 8.2836 - val_loss: 11.8658\n","Epoch 102/150\n","90/90 - 1s - loss: 8.2814 - val_loss: 11.8058\n","Epoch 103/150\n","90/90 - 1s - loss: 8.2681 - val_loss: 11.8413\n","Epoch 104/150\n","90/90 - 1s - loss: 8.2823 - val_loss: 11.8827\n","Epoch 105/150\n","90/90 - 1s - loss: 8.2651 - val_loss: 11.8465\n","Epoch 106/150\n","90/90 - 1s - loss: 8.2762 - val_loss: 11.8057\n","Epoch 107/150\n","90/90 - 1s - loss: 8.2725 - val_loss: 11.8032\n","Epoch 108/150\n","90/90 - 1s - loss: 8.2645 - val_loss: 11.9009\n","Epoch 109/150\n","90/90 - 1s - loss: 8.2800 - val_loss: 11.8581\n","Epoch 110/150\n","90/90 - 1s - loss: 8.2766 - val_loss: 11.9051\n","Epoch 111/150\n","90/90 - 1s - loss: 8.2660 - val_loss: 11.8878\n","Epoch 112/150\n","90/90 - 1s - loss: 8.2764 - val_loss: 11.8594\n","Epoch 113/150\n","90/90 - 1s - loss: 8.2647 - val_loss: 11.7948\n","Epoch 114/150\n","90/90 - 1s - loss: 8.2499 - val_loss: 11.7966\n","Epoch 115/150\n","90/90 - 1s - loss: 8.2537 - val_loss: 11.7986\n","Epoch 116/150\n","90/90 - 1s - loss: 8.2613 - val_loss: 11.8194\n","Epoch 117/150\n","90/90 - 1s - loss: 8.2516 - val_loss: 11.7925\n","Epoch 118/150\n","90/90 - 1s - loss: 8.2459 - val_loss: 11.7663\n","Epoch 119/150\n","90/90 - 1s - loss: 8.2513 - val_loss: 11.7284\n","Epoch 120/150\n","90/90 - 1s - loss: 8.2388 - val_loss: 11.8160\n","Epoch 121/150\n","90/90 - 1s - loss: 8.2352 - val_loss: 11.7507\n","Epoch 122/150\n","90/90 - 1s - loss: 8.2209 - val_loss: 11.6980\n","Epoch 123/150\n","90/90 - 1s - loss: 8.2298 - val_loss: 11.6799\n","Epoch 124/150\n","90/90 - 1s - loss: 8.2088 - val_loss: 11.7008\n","Epoch 125/150\n","90/90 - 1s - loss: 8.2175 - val_loss: 11.6610\n","Epoch 126/150\n","90/90 - 1s - loss: 8.2078 - val_loss: 11.6560\n","Epoch 127/150\n","90/90 - 1s - loss: 8.2085 - val_loss: 11.7084\n","Epoch 128/150\n","90/90 - 1s - loss: 8.2026 - val_loss: 11.7111\n","Epoch 129/150\n","90/90 - 1s - loss: 8.2070 - val_loss: 11.6798\n","Epoch 130/150\n","90/90 - 1s - loss: 8.2092 - val_loss: 11.6757\n","Epoch 131/150\n","90/90 - 1s - loss: 8.1917 - val_loss: 11.6895\n","Epoch 132/150\n","90/90 - 1s - loss: 8.2009 - val_loss: 11.6861\n","Epoch 133/150\n","90/90 - 1s - loss: 8.2103 - val_loss: 11.6785\n","Epoch 134/150\n","90/90 - 1s - loss: 8.1979 - val_loss: 11.6969\n","Epoch 135/150\n","90/90 - 1s - loss: 8.2090 - val_loss: 11.6684\n","Epoch 136/150\n","90/90 - 1s - loss: 8.2062 - val_loss: 11.6540\n","Epoch 137/150\n","90/90 - 1s - loss: 8.1864 - val_loss: 11.6652\n","Epoch 138/150\n","90/90 - 1s - loss: 8.2078 - val_loss: 11.7325\n","Epoch 139/150\n","90/90 - 1s - loss: 8.2094 - val_loss: 11.6263\n","Epoch 140/150\n","90/90 - 1s - loss: 8.1915 - val_loss: 11.6603\n","Epoch 141/150\n","90/90 - 1s - loss: 8.1787 - val_loss: 11.6879\n","Epoch 142/150\n","90/90 - 1s - loss: 8.1745 - val_loss: 11.6711\n","Epoch 143/150\n","90/90 - 1s - loss: 8.1794 - val_loss: 11.6651\n","Epoch 144/150\n","90/90 - 1s - loss: 8.1770 - val_loss: 11.6466\n","Epoch 145/150\n","90/90 - 1s - loss: 8.1756 - val_loss: 11.7070\n","Epoch 146/150\n","90/90 - 1s - loss: 8.1698 - val_loss: 11.6715\n","Epoch 147/150\n","90/90 - 1s - loss: 8.1684 - val_loss: 11.6427\n","Epoch 148/150\n","90/90 - 1s - loss: 8.1617 - val_loss: 11.6470\n","Epoch 149/150\n","90/90 - 1s - loss: 8.1625 - val_loss: 11.6140\n","Epoch 150/150\n","90/90 - 1s - loss: 8.1580 - val_loss: 11.6867\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QPXRI6I_EniW","executionInfo":{"status":"ok","timestamp":1604628497785,"user_tz":180,"elapsed":91502,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"33b758d7-ff57-4f3b-cfa0-7da3893fd07d","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 18.340\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fDlAJo8-EnrJ","executionInfo":{"status":"ok","timestamp":1604628497788,"user_tz":180,"elapsed":91500,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"83ba1355-5478-45f5-f490-f75aee9716f2","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 11.687\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PpCBcoV2EnXH","executionInfo":{"status":"ok","timestamp":1604628497790,"user_tz":180,"elapsed":91497,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"081885de-eb78-4baa-ee8f-b01bb2818136","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.636\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AOhRRU55T9j8"},"source":["#**(mask) janela=3**"]},{"cell_type":"code","metadata":{"id":"3eSh-WhfF1IJ","executionInfo":{"status":"ok","timestamp":1604628751029,"user_tz":180,"elapsed":93550,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"c80ef5af-815c-4ee5-b86c-d3ecba0dee71","colab":{"base_uri":"https://localhost:8080/"}},"source":["df_sc_bruto=df_sc_2\n","df_sc_bruto.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","df_sc_bruto_copy=df_sc_bruto.copy()\n","df_sc_bruto_copy=df_sc_bruto_copy[['O3']]\n","df_sc_bruto.drop(columns=['O3'],inplace=True)\n","df_sc_bruto = pd.concat([df_sc_bruto_copy,df_sc_bruto], axis=1, ignore_index=False)\n","\n","# prepare data for lstm\n","# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","from keras.layers import Masking\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","import numpy as np\n","\n","values = df_sc_bruto.values\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(df_sc_bruto)\n","scaled=values\n","df=pd.DataFrame(scaled)\n","df = df.replace(np.NaN,-9999)\n","values = df.values\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 3\n","n_features = 10\n","\n","# frame as supervised learning\n","reframed = series_to_supervised(scaled, n_hours, 3) #janela\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(Masking(mask_value=-9999, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(LSTM(300))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt, sample_weight_mode='temporal')\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 2s - loss: 20.5942 - val_loss: 26.9646\n","Epoch 2/150\n","90/90 - 1s - loss: 18.8291 - val_loss: 25.2856\n","Epoch 3/150\n","90/90 - 1s - loss: 17.9305 - val_loss: 24.0609\n","Epoch 4/150\n","90/90 - 1s - loss: 17.1192 - val_loss: 23.0319\n","Epoch 5/150\n","90/90 - 1s - loss: 16.4611 - val_loss: 22.1988\n","Epoch 6/150\n","90/90 - 1s - loss: 15.8941 - val_loss: 21.5061\n","Epoch 7/150\n","90/90 - 1s - loss: 15.3923 - val_loss: 20.8992\n","Epoch 8/150\n","90/90 - 1s - loss: 14.9609 - val_loss: 20.3777\n","Epoch 9/150\n","90/90 - 1s - loss: 14.6274 - val_loss: 19.9224\n","Epoch 10/150\n","90/90 - 1s - loss: 14.2732 - val_loss: 19.5107\n","Epoch 11/150\n","90/90 - 1s - loss: 13.9823 - val_loss: 19.1250\n","Epoch 12/150\n","90/90 - 1s - loss: 13.6985 - val_loss: 18.7682\n","Epoch 13/150\n","90/90 - 1s - loss: 13.4645 - val_loss: 18.4383\n","Epoch 14/150\n","90/90 - 1s - loss: 13.2451 - val_loss: 18.1352\n","Epoch 15/150\n","90/90 - 1s - loss: 13.0863 - val_loss: 17.9022\n","Epoch 16/150\n","90/90 - 1s - loss: 12.9094 - val_loss: 17.6425\n","Epoch 17/150\n","90/90 - 1s - loss: 12.7803 - val_loss: 17.3996\n","Epoch 18/150\n","90/90 - 1s - loss: 12.6294 - val_loss: 17.1634\n","Epoch 19/150\n","90/90 - 1s - loss: 12.5181 - val_loss: 16.9809\n","Epoch 20/150\n","90/90 - 1s - loss: 12.4208 - val_loss: 16.8090\n","Epoch 21/150\n","90/90 - 1s - loss: 12.3261 - val_loss: 16.6785\n","Epoch 22/150\n","90/90 - 1s - loss: 12.2509 - val_loss: 16.5311\n","Epoch 23/150\n","90/90 - 1s - loss: 12.1971 - val_loss: 16.4310\n","Epoch 24/150\n","90/90 - 1s - loss: 12.1254 - val_loss: 16.2841\n","Epoch 25/150\n","90/90 - 1s - loss: 12.0950 - val_loss: 16.2062\n","Epoch 26/150\n","90/90 - 1s - loss: 12.0556 - val_loss: 16.1385\n","Epoch 27/150\n","90/90 - 1s - loss: 12.0144 - val_loss: 16.0801\n","Epoch 28/150\n","90/90 - 1s - loss: 11.9153 - val_loss: 15.9841\n","Epoch 29/150\n","90/90 - 1s - loss: 11.8611 - val_loss: 15.8907\n","Epoch 30/150\n","90/90 - 1s - loss: 11.8598 - val_loss: 15.8651\n","Epoch 31/150\n","90/90 - 1s - loss: 11.8115 - val_loss: 15.7568\n","Epoch 32/150\n","90/90 - 1s - loss: 11.8018 - val_loss: 15.7267\n","Epoch 33/150\n","90/90 - 1s - loss: 11.7329 - val_loss: 15.6924\n","Epoch 34/150\n","90/90 - 1s - loss: 11.7054 - val_loss: 15.6450\n","Epoch 35/150\n","90/90 - 1s - loss: 11.6754 - val_loss: 15.6073\n","Epoch 36/150\n","90/90 - 1s - loss: 11.6355 - val_loss: 15.5715\n","Epoch 37/150\n","90/90 - 1s - loss: 11.6138 - val_loss: 15.6552\n","Epoch 38/150\n","90/90 - 1s - loss: 11.6059 - val_loss: 15.6021\n","Epoch 39/150\n","90/90 - 1s - loss: 11.5768 - val_loss: 15.5075\n","Epoch 40/150\n","90/90 - 1s - loss: 11.5494 - val_loss: 15.4926\n","Epoch 41/150\n","90/90 - 1s - loss: 11.5240 - val_loss: 15.4574\n","Epoch 42/150\n","90/90 - 1s - loss: 11.4969 - val_loss: 15.4434\n","Epoch 43/150\n","90/90 - 1s - loss: 11.4989 - val_loss: 15.4318\n","Epoch 44/150\n","90/90 - 1s - loss: 11.4316 - val_loss: 15.3935\n","Epoch 45/150\n","90/90 - 1s - loss: 11.4404 - val_loss: 15.3499\n","Epoch 46/150\n","90/90 - 1s - loss: 11.4010 - val_loss: 15.3176\n","Epoch 47/150\n","90/90 - 1s - loss: 11.3780 - val_loss: 15.2629\n","Epoch 48/150\n","90/90 - 1s - loss: 11.3548 - val_loss: 15.2838\n","Epoch 49/150\n","90/90 - 1s - loss: 11.3200 - val_loss: 15.2944\n","Epoch 50/150\n","90/90 - 1s - loss: 11.3274 - val_loss: 15.2469\n","Epoch 51/150\n","90/90 - 1s - loss: 11.2993 - val_loss: 15.1932\n","Epoch 52/150\n","90/90 - 1s - loss: 11.2815 - val_loss: 15.2254\n","Epoch 53/150\n","90/90 - 1s - loss: 11.2645 - val_loss: 15.0966\n","Epoch 54/150\n","90/90 - 1s - loss: 11.2514 - val_loss: 15.1894\n","Epoch 55/150\n","90/90 - 1s - loss: 11.2278 - val_loss: 15.1236\n","Epoch 56/150\n","90/90 - 1s - loss: 11.1932 - val_loss: 15.0861\n","Epoch 57/150\n","90/90 - 1s - loss: 11.2012 - val_loss: 15.1379\n","Epoch 58/150\n","90/90 - 1s - loss: 11.1752 - val_loss: 15.0801\n","Epoch 59/150\n","90/90 - 1s - loss: 11.1485 - val_loss: 15.0952\n","Epoch 60/150\n","90/90 - 1s - loss: 11.1536 - val_loss: 15.0705\n","Epoch 61/150\n","90/90 - 1s - loss: 11.1438 - val_loss: 15.1306\n","Epoch 62/150\n","90/90 - 1s - loss: 11.1080 - val_loss: 15.0763\n","Epoch 63/150\n","90/90 - 1s - loss: 11.0970 - val_loss: 15.0766\n","Epoch 64/150\n","90/90 - 1s - loss: 11.0904 - val_loss: 15.0439\n","Epoch 65/150\n","90/90 - 1s - loss: 11.1078 - val_loss: 15.1066\n","Epoch 66/150\n","90/90 - 1s - loss: 11.0947 - val_loss: 15.0548\n","Epoch 67/150\n","90/90 - 1s - loss: 11.0968 - val_loss: 15.0497\n","Epoch 68/150\n","90/90 - 1s - loss: 11.0389 - val_loss: 15.0317\n","Epoch 69/150\n","90/90 - 1s - loss: 11.0479 - val_loss: 15.0358\n","Epoch 70/150\n","90/90 - 1s - loss: 11.0300 - val_loss: 14.9713\n","Epoch 71/150\n","90/90 - 1s - loss: 10.9988 - val_loss: 14.9776\n","Epoch 72/150\n","90/90 - 1s - loss: 10.9644 - val_loss: 14.9436\n","Epoch 73/150\n","90/90 - 1s - loss: 10.9606 - val_loss: 14.9988\n","Epoch 74/150\n","90/90 - 1s - loss: 10.9324 - val_loss: 14.9443\n","Epoch 75/150\n","90/90 - 1s - loss: 10.9324 - val_loss: 14.9145\n","Epoch 76/150\n","90/90 - 1s - loss: 10.9040 - val_loss: 14.9375\n","Epoch 77/150\n","90/90 - 1s - loss: 10.9061 - val_loss: 14.9293\n","Epoch 78/150\n","90/90 - 1s - loss: 10.8711 - val_loss: 14.8929\n","Epoch 79/150\n","90/90 - 1s - loss: 10.8508 - val_loss: 14.8923\n","Epoch 80/150\n","90/90 - 1s - loss: 10.8486 - val_loss: 14.8870\n","Epoch 81/150\n","90/90 - 1s - loss: 10.8485 - val_loss: 14.9022\n","Epoch 82/150\n","90/90 - 1s - loss: 10.8231 - val_loss: 14.8486\n","Epoch 83/150\n","90/90 - 1s - loss: 10.8097 - val_loss: 14.8092\n","Epoch 84/150\n","90/90 - 1s - loss: 10.8052 - val_loss: 14.8026\n","Epoch 85/150\n","90/90 - 1s - loss: 10.7859 - val_loss: 14.7517\n","Epoch 86/150\n","90/90 - 1s - loss: 10.7848 - val_loss: 14.7589\n","Epoch 87/150\n","90/90 - 1s - loss: 10.7637 - val_loss: 14.7952\n","Epoch 88/150\n","90/90 - 1s - loss: 10.7490 - val_loss: 14.7575\n","Epoch 89/150\n","90/90 - 1s - loss: 10.7585 - val_loss: 14.7752\n","Epoch 90/150\n","90/90 - 1s - loss: 10.7297 - val_loss: 14.7099\n","Epoch 91/150\n","90/90 - 1s - loss: 10.7308 - val_loss: 14.7445\n","Epoch 92/150\n","90/90 - 1s - loss: 10.7222 - val_loss: 14.6778\n","Epoch 93/150\n","90/90 - 1s - loss: 10.7127 - val_loss: 14.6681\n","Epoch 94/150\n","90/90 - 1s - loss: 10.6945 - val_loss: 14.6119\n","Epoch 95/150\n","90/90 - 1s - loss: 10.6991 - val_loss: 14.6905\n","Epoch 96/150\n","90/90 - 1s - loss: 10.7224 - val_loss: 14.7000\n","Epoch 97/150\n","90/90 - 1s - loss: 10.6918 - val_loss: 14.6768\n","Epoch 98/150\n","90/90 - 1s - loss: 10.6926 - val_loss: 14.6493\n","Epoch 99/150\n","90/90 - 1s - loss: 10.6723 - val_loss: 14.6493\n","Epoch 100/150\n","90/90 - 1s - loss: 10.6699 - val_loss: 14.6106\n","Epoch 101/150\n","90/90 - 1s - loss: 10.6460 - val_loss: 14.5730\n","Epoch 102/150\n","90/90 - 1s - loss: 10.6504 - val_loss: 14.6339\n","Epoch 103/150\n","90/90 - 1s - loss: 10.6524 - val_loss: 14.5914\n","Epoch 104/150\n","90/90 - 1s - loss: 10.6345 - val_loss: 14.5538\n","Epoch 105/150\n","90/90 - 1s - loss: 10.6284 - val_loss: 14.6060\n","Epoch 106/150\n","90/90 - 1s - loss: 10.6374 - val_loss: 14.5887\n","Epoch 107/150\n","90/90 - 1s - loss: 10.6154 - val_loss: 14.5234\n","Epoch 108/150\n","90/90 - 1s - loss: 10.6060 - val_loss: 14.5688\n","Epoch 109/150\n","90/90 - 1s - loss: 10.5996 - val_loss: 14.5497\n","Epoch 110/150\n","90/90 - 1s - loss: 10.5863 - val_loss: 14.5642\n","Epoch 111/150\n","90/90 - 1s - loss: 10.5689 - val_loss: 14.5203\n","Epoch 112/150\n","90/90 - 1s - loss: 10.5805 - val_loss: 14.5485\n","Epoch 113/150\n","90/90 - 1s - loss: 10.5637 - val_loss: 14.5437\n","Epoch 114/150\n","90/90 - 1s - loss: 10.5599 - val_loss: 14.5221\n","Epoch 115/150\n","90/90 - 1s - loss: 10.5451 - val_loss: 14.4747\n","Epoch 116/150\n","90/90 - 1s - loss: 10.5389 - val_loss: 14.5536\n","Epoch 117/150\n","90/90 - 1s - loss: 10.5412 - val_loss: 14.4208\n","Epoch 118/150\n","90/90 - 1s - loss: 10.5286 - val_loss: 14.4549\n","Epoch 119/150\n","90/90 - 1s - loss: 10.5338 - val_loss: 14.4157\n","Epoch 120/150\n","90/90 - 1s - loss: 10.5200 - val_loss: 14.4579\n","Epoch 121/150\n","90/90 - 1s - loss: 10.5161 - val_loss: 14.4252\n","Epoch 122/150\n","90/90 - 1s - loss: 10.4961 - val_loss: 14.4685\n","Epoch 123/150\n","90/90 - 1s - loss: 10.4847 - val_loss: 14.4259\n","Epoch 124/150\n","90/90 - 1s - loss: 10.4812 - val_loss: 14.4240\n","Epoch 125/150\n","90/90 - 1s - loss: 10.4790 - val_loss: 14.4603\n","Epoch 126/150\n","90/90 - 1s - loss: 10.4709 - val_loss: 14.3765\n","Epoch 127/150\n","90/90 - 1s - loss: 10.4670 - val_loss: 14.4060\n","Epoch 128/150\n","90/90 - 1s - loss: 10.4453 - val_loss: 14.3311\n","Epoch 129/150\n","90/90 - 1s - loss: 10.4547 - val_loss: 14.3942\n","Epoch 130/150\n","90/90 - 1s - loss: 10.4588 - val_loss: 14.3652\n","Epoch 131/150\n","90/90 - 1s - loss: 10.4407 - val_loss: 14.3697\n","Epoch 132/150\n","90/90 - 1s - loss: 10.4385 - val_loss: 14.3536\n","Epoch 133/150\n","90/90 - 1s - loss: 10.4289 - val_loss: 14.3250\n","Epoch 134/150\n","90/90 - 1s - loss: 10.4351 - val_loss: 14.3795\n","Epoch 135/150\n","90/90 - 1s - loss: 10.4200 - val_loss: 14.3240\n","Epoch 136/150\n","90/90 - 1s - loss: 10.4076 - val_loss: 14.3197\n","Epoch 137/150\n","90/90 - 1s - loss: 10.3954 - val_loss: 14.3190\n","Epoch 138/150\n","90/90 - 1s - loss: 10.3936 - val_loss: 14.3608\n","Epoch 139/150\n","90/90 - 1s - loss: 10.3914 - val_loss: 14.3094\n","Epoch 140/150\n","90/90 - 1s - loss: 10.3816 - val_loss: 14.2203\n","Epoch 141/150\n","90/90 - 1s - loss: 10.3880 - val_loss: 14.3017\n","Epoch 142/150\n","90/90 - 1s - loss: 10.3758 - val_loss: 14.2480\n","Epoch 143/150\n","90/90 - 1s - loss: 10.3677 - val_loss: 14.2653\n","Epoch 144/150\n","90/90 - 1s - loss: 10.3925 - val_loss: 14.2730\n","Epoch 145/150\n","90/90 - 1s - loss: 10.3593 - val_loss: 14.2617\n","Epoch 146/150\n","90/90 - 1s - loss: 10.3482 - val_loss: 14.1995\n","Epoch 147/150\n","90/90 - 1s - loss: 10.3590 - val_loss: 14.2234\n","Epoch 148/150\n","90/90 - 1s - loss: 10.3435 - val_loss: 14.2334\n","Epoch 149/150\n","90/90 - 1s - loss: 10.3422 - val_loss: 14.2189\n","Epoch 150/150\n","90/90 - 1s - loss: 10.3431 - val_loss: 14.1847\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oHu8MP18Ga2C","executionInfo":{"status":"ok","timestamp":1604628752860,"user_tz":180,"elapsed":95352,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"e4571602-131f-4999-b546-8ff5cacd7d87","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 21.913\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ab9ha8_-Gbw5","executionInfo":{"status":"ok","timestamp":1604628752862,"user_tz":180,"elapsed":95349,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"dc471df9-26c1-492d-baa1-23a3bfc73b95","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 14.185\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l3oXJ-IpGbtg","executionInfo":{"status":"ok","timestamp":1604628752864,"user_tz":180,"elapsed":95347,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"a7734486-ec02-4e96-c78f-012518f35c19","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.505\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DaqaOUvPT9g_"},"source":["#**(mask) janela=4**"]},{"cell_type":"code","metadata":{"id":"njDYJL0tF86Q","executionInfo":{"status":"ok","timestamp":1604626043402,"user_tz":180,"elapsed":103203,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"7b14ddd8-65bb-4842-e6af-fe95f08256ac","colab":{"base_uri":"https://localhost:8080/"}},"source":["df_sc_bruto=df_sc_2\n","df_sc_bruto.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","df_sc_bruto_copy=df_sc_bruto.copy()\n","df_sc_bruto_copy=df_sc_bruto_copy[['O3']]\n","df_sc_bruto.drop(columns=['O3'],inplace=True)\n","df_sc_bruto = pd.concat([df_sc_bruto_copy,df_sc_bruto], axis=1, ignore_index=False)\n","\n","# prepare data for lstm\n","# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","from keras.layers import Masking\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","import numpy as np\n","\n","values = df_sc_bruto.values\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(df_sc_bruto)\n","scaled=values\n","df=pd.DataFrame(scaled)\n","df = df.replace(np.NaN,-9999)\n","values = df.values\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 4\n","n_features = 10\n","\n","# frame as supervised learning\n","reframed = series_to_supervised(scaled, n_hours, 4) #janela\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(Masking(mask_value=-9999, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(LSTM(300))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt, sample_weight_mode='temporal')\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 2s - loss: 21.8837 - val_loss: 28.2444\n","Epoch 2/150\n","90/90 - 1s - loss: 20.0129 - val_loss: 27.3833\n","Epoch 3/150\n","90/90 - 1s - loss: 19.6104 - val_loss: 26.8552\n","Epoch 4/150\n","90/90 - 1s - loss: 19.0888 - val_loss: 26.3138\n","Epoch 5/150\n","90/90 - 1s - loss: 18.6205 - val_loss: 25.7796\n","Epoch 6/150\n","90/90 - 1s - loss: 18.1455 - val_loss: 25.2315\n","Epoch 7/150\n","90/90 - 1s - loss: 17.7389 - val_loss: 24.6968\n","Epoch 8/150\n","90/90 - 1s - loss: 17.4179 - val_loss: 24.2925\n","Epoch 9/150\n","90/90 - 1s - loss: 17.1002 - val_loss: 23.9114\n","Epoch 10/150\n","90/90 - 1s - loss: 16.7869 - val_loss: 23.5142\n","Epoch 11/150\n","90/90 - 1s - loss: 16.5205 - val_loss: 23.2106\n","Epoch 12/150\n","90/90 - 1s - loss: 16.3388 - val_loss: 22.9666\n","Epoch 13/150\n","90/90 - 1s - loss: 16.1410 - val_loss: 22.7365\n","Epoch 14/150\n","90/90 - 1s - loss: 15.9156 - val_loss: 22.4590\n","Epoch 15/150\n","90/90 - 1s - loss: 15.7608 - val_loss: 22.2852\n","Epoch 16/150\n","90/90 - 1s - loss: 15.6355 - val_loss: 22.1121\n","Epoch 17/150\n","90/90 - 1s - loss: 15.4787 - val_loss: 21.9333\n","Epoch 18/150\n","90/90 - 1s - loss: 15.3687 - val_loss: 21.8203\n","Epoch 19/150\n","90/90 - 1s - loss: 15.3807 - val_loss: 21.7099\n","Epoch 20/150\n","90/90 - 1s - loss: 15.3031 - val_loss: 21.6499\n","Epoch 21/150\n","90/90 - 1s - loss: 15.1880 - val_loss: 21.4912\n","Epoch 22/150\n","90/90 - 1s - loss: 15.1172 - val_loss: 21.4312\n","Epoch 23/150\n","90/90 - 1s - loss: 15.0147 - val_loss: 21.2717\n","Epoch 24/150\n","90/90 - 1s - loss: 14.9286 - val_loss: 21.1705\n","Epoch 25/150\n","90/90 - 1s - loss: 14.8663 - val_loss: 21.1595\n","Epoch 26/150\n","90/90 - 1s - loss: 14.7994 - val_loss: 21.0483\n","Epoch 27/150\n","90/90 - 1s - loss: 14.8191 - val_loss: 20.9761\n","Epoch 28/150\n","90/90 - 1s - loss: 14.7604 - val_loss: 20.9350\n","Epoch 29/150\n","90/90 - 1s - loss: 14.7015 - val_loss: 20.8415\n","Epoch 30/150\n","90/90 - 1s - loss: 14.6653 - val_loss: 20.7141\n","Epoch 31/150\n","90/90 - 1s - loss: 14.6433 - val_loss: 20.6905\n","Epoch 32/150\n","90/90 - 1s - loss: 14.6132 - val_loss: 20.7108\n","Epoch 33/150\n","90/90 - 1s - loss: 14.5952 - val_loss: 20.6521\n","Epoch 34/150\n","90/90 - 1s - loss: 14.5401 - val_loss: 20.4778\n","Epoch 35/150\n","90/90 - 1s - loss: 14.5036 - val_loss: 20.4419\n","Epoch 36/150\n","90/90 - 1s - loss: 14.5058 - val_loss: 20.4014\n","Epoch 37/150\n","90/90 - 1s - loss: 14.5152 - val_loss: 20.5383\n","Epoch 38/150\n","90/90 - 1s - loss: 14.4852 - val_loss: 20.4303\n","Epoch 39/150\n","90/90 - 1s - loss: 14.4711 - val_loss: 20.3809\n","Epoch 40/150\n","90/90 - 1s - loss: 14.4493 - val_loss: 20.3275\n","Epoch 41/150\n","90/90 - 1s - loss: 14.4507 - val_loss: 20.3242\n","Epoch 42/150\n","90/90 - 1s - loss: 14.4265 - val_loss: 20.2263\n","Epoch 43/150\n","90/90 - 1s - loss: 14.4544 - val_loss: 20.2627\n","Epoch 44/150\n","90/90 - 1s - loss: 14.4141 - val_loss: 20.2672\n","Epoch 45/150\n","90/90 - 1s - loss: 14.3640 - val_loss: 20.3023\n","Epoch 46/150\n","90/90 - 1s - loss: 14.3619 - val_loss: 20.1691\n","Epoch 47/150\n","90/90 - 1s - loss: 14.3534 - val_loss: 20.2619\n","Epoch 48/150\n","90/90 - 1s - loss: 14.3520 - val_loss: 20.1996\n","Epoch 49/150\n","90/90 - 1s - loss: 14.3013 - val_loss: 20.2072\n","Epoch 50/150\n","90/90 - 1s - loss: 14.3091 - val_loss: 20.1698\n","Epoch 51/150\n","90/90 - 1s - loss: 14.2884 - val_loss: 20.1748\n","Epoch 52/150\n","90/90 - 1s - loss: 14.2623 - val_loss: 20.1502\n","Epoch 53/150\n","90/90 - 1s - loss: 14.2513 - val_loss: 20.2222\n","Epoch 54/150\n","90/90 - 1s - loss: 14.2188 - val_loss: 20.2074\n","Epoch 55/150\n","90/90 - 1s - loss: 14.1815 - val_loss: 20.3537\n","Epoch 56/150\n","90/90 - 1s - loss: 14.2092 - val_loss: 20.3599\n","Epoch 57/150\n","90/90 - 1s - loss: 14.1919 - val_loss: 20.3824\n","Epoch 58/150\n","90/90 - 1s - loss: 14.1396 - val_loss: 20.4758\n","Epoch 59/150\n","90/90 - 1s - loss: 14.1232 - val_loss: 20.4599\n","Epoch 60/150\n","90/90 - 1s - loss: 14.0894 - val_loss: 20.4956\n","Epoch 61/150\n","90/90 - 1s - loss: 14.0587 - val_loss: 20.5300\n","Epoch 62/150\n","90/90 - 1s - loss: 14.0409 - val_loss: 20.4685\n","Epoch 63/150\n","90/90 - 1s - loss: 14.0818 - val_loss: 20.2622\n","Epoch 64/150\n","90/90 - 1s - loss: 14.0384 - val_loss: 20.2465\n","Epoch 65/150\n","90/90 - 1s - loss: 13.9999 - val_loss: 20.3903\n","Epoch 66/150\n","90/90 - 1s - loss: 13.9843 - val_loss: 20.3377\n","Epoch 67/150\n","90/90 - 1s - loss: 13.9358 - val_loss: 20.4085\n","Epoch 68/150\n","90/90 - 1s - loss: 13.9608 - val_loss: 20.5402\n","Epoch 69/150\n","90/90 - 1s - loss: 13.9178 - val_loss: 20.4926\n","Epoch 70/150\n","90/90 - 1s - loss: 13.8631 - val_loss: 20.4066\n","Epoch 71/150\n","90/90 - 1s - loss: 13.8564 - val_loss: 20.3970\n","Epoch 72/150\n","90/90 - 1s - loss: 13.8366 - val_loss: 20.2966\n","Epoch 73/150\n","90/90 - 1s - loss: 13.7782 - val_loss: 20.4123\n","Epoch 74/150\n","90/90 - 1s - loss: 13.7724 - val_loss: 20.4412\n","Epoch 75/150\n","90/90 - 1s - loss: 13.8228 - val_loss: 20.5790\n","Epoch 76/150\n","90/90 - 1s - loss: 13.7576 - val_loss: 20.2023\n","Epoch 77/150\n","90/90 - 1s - loss: 13.7132 - val_loss: 20.1699\n","Epoch 78/150\n","90/90 - 1s - loss: 13.6257 - val_loss: 20.3245\n","Epoch 79/150\n","90/90 - 1s - loss: 13.6176 - val_loss: 20.3149\n","Epoch 80/150\n","90/90 - 1s - loss: 13.6025 - val_loss: 20.1334\n","Epoch 81/150\n","90/90 - 1s - loss: 13.5712 - val_loss: 20.3006\n","Epoch 82/150\n","90/90 - 1s - loss: 13.5442 - val_loss: 20.4057\n","Epoch 83/150\n","90/90 - 1s - loss: 13.4498 - val_loss: 20.3107\n","Epoch 84/150\n","90/90 - 1s - loss: 13.3785 - val_loss: 19.9558\n","Epoch 85/150\n","90/90 - 1s - loss: 13.3199 - val_loss: 19.9831\n","Epoch 86/150\n","90/90 - 1s - loss: 13.3701 - val_loss: 20.0853\n","Epoch 87/150\n","90/90 - 1s - loss: 13.3748 - val_loss: 19.9065\n","Epoch 88/150\n","90/90 - 1s - loss: 13.3295 - val_loss: 20.0385\n","Epoch 89/150\n","90/90 - 1s - loss: 13.2426 - val_loss: 20.0113\n","Epoch 90/150\n","90/90 - 1s - loss: 13.1825 - val_loss: 20.2133\n","Epoch 91/150\n","90/90 - 1s - loss: 13.1777 - val_loss: 19.8377\n","Epoch 92/150\n","90/90 - 1s - loss: 13.1647 - val_loss: 19.8722\n","Epoch 93/150\n","90/90 - 1s - loss: 13.0898 - val_loss: 19.6087\n","Epoch 94/150\n","90/90 - 1s - loss: 13.0645 - val_loss: 19.8424\n","Epoch 95/150\n","90/90 - 1s - loss: 13.0612 - val_loss: 19.6980\n","Epoch 96/150\n","90/90 - 1s - loss: 13.0647 - val_loss: 19.6985\n","Epoch 97/150\n","90/90 - 1s - loss: 13.0023 - val_loss: 19.5667\n","Epoch 98/150\n","90/90 - 1s - loss: 12.9920 - val_loss: 19.6661\n","Epoch 99/150\n","90/90 - 1s - loss: 12.9789 - val_loss: 19.5836\n","Epoch 100/150\n","90/90 - 1s - loss: 12.9585 - val_loss: 19.6423\n","Epoch 101/150\n","90/90 - 1s - loss: 12.9238 - val_loss: 19.5729\n","Epoch 102/150\n","90/90 - 1s - loss: 12.8983 - val_loss: 19.5630\n","Epoch 103/150\n","90/90 - 1s - loss: 12.8860 - val_loss: 19.4517\n","Epoch 104/150\n","90/90 - 1s - loss: 12.8766 - val_loss: 19.5181\n","Epoch 105/150\n","90/90 - 1s - loss: 12.8370 - val_loss: 19.4585\n","Epoch 106/150\n","90/90 - 1s - loss: 12.8030 - val_loss: 19.3074\n","Epoch 107/150\n","90/90 - 1s - loss: 12.8066 - val_loss: 19.1833\n","Epoch 108/150\n","90/90 - 1s - loss: 12.7911 - val_loss: 19.3728\n","Epoch 109/150\n","90/90 - 1s - loss: 12.7606 - val_loss: 19.2452\n","Epoch 110/150\n","90/90 - 1s - loss: 12.7515 - val_loss: 19.1070\n","Epoch 111/150\n","90/90 - 1s - loss: 12.7177 - val_loss: 19.1033\n","Epoch 112/150\n","90/90 - 1s - loss: 12.7236 - val_loss: 19.0805\n","Epoch 113/150\n","90/90 - 1s - loss: 12.7188 - val_loss: 19.1104\n","Epoch 114/150\n","90/90 - 1s - loss: 12.6724 - val_loss: 19.0235\n","Epoch 115/150\n","90/90 - 1s - loss: 12.6401 - val_loss: 18.9770\n","Epoch 116/150\n","90/90 - 1s - loss: 12.6333 - val_loss: 18.9333\n","Epoch 117/150\n","90/90 - 1s - loss: 12.6298 - val_loss: 18.8970\n","Epoch 118/150\n","90/90 - 1s - loss: 12.5875 - val_loss: 19.0131\n","Epoch 119/150\n","90/90 - 1s - loss: 12.5710 - val_loss: 18.9638\n","Epoch 120/150\n","90/90 - 1s - loss: 12.5702 - val_loss: 18.9360\n","Epoch 121/150\n","90/90 - 1s - loss: 12.5566 - val_loss: 18.8544\n","Epoch 122/150\n","90/90 - 1s - loss: 12.5493 - val_loss: 18.7857\n","Epoch 123/150\n","90/90 - 1s - loss: 12.5384 - val_loss: 18.8006\n","Epoch 124/150\n","90/90 - 1s - loss: 12.5102 - val_loss: 18.8440\n","Epoch 125/150\n","90/90 - 1s - loss: 12.5260 - val_loss: 18.7437\n","Epoch 126/150\n","90/90 - 1s - loss: 12.5140 - val_loss: 18.7293\n","Epoch 127/150\n","90/90 - 1s - loss: 12.5037 - val_loss: 18.6660\n","Epoch 128/150\n","90/90 - 1s - loss: 12.4762 - val_loss: 18.8003\n","Epoch 129/150\n","90/90 - 1s - loss: 12.4627 - val_loss: 18.7022\n","Epoch 130/150\n","90/90 - 1s - loss: 12.4558 - val_loss: 18.7114\n","Epoch 131/150\n","90/90 - 1s - loss: 12.4256 - val_loss: 18.6381\n","Epoch 132/150\n","90/90 - 1s - loss: 12.4190 - val_loss: 18.6541\n","Epoch 133/150\n","90/90 - 1s - loss: 12.4021 - val_loss: 18.7365\n","Epoch 134/150\n","90/90 - 1s - loss: 12.4173 - val_loss: 18.7094\n","Epoch 135/150\n","90/90 - 1s - loss: 12.4086 - val_loss: 18.6237\n","Epoch 136/150\n","90/90 - 1s - loss: 12.4004 - val_loss: 18.6128\n","Epoch 137/150\n","90/90 - 1s - loss: 12.3486 - val_loss: 18.5812\n","Epoch 138/150\n","90/90 - 1s - loss: 12.3686 - val_loss: 18.6925\n","Epoch 139/150\n","90/90 - 1s - loss: 12.3791 - val_loss: 18.5563\n","Epoch 140/150\n","90/90 - 1s - loss: 12.4144 - val_loss: 18.5540\n","Epoch 141/150\n","90/90 - 1s - loss: 12.3780 - val_loss: 18.5040\n","Epoch 142/150\n","90/90 - 1s - loss: 12.3592 - val_loss: 18.4680\n","Epoch 143/150\n","90/90 - 1s - loss: 12.3472 - val_loss: 18.4765\n","Epoch 144/150\n","90/90 - 1s - loss: 12.3098 - val_loss: 18.3963\n","Epoch 145/150\n","90/90 - 1s - loss: 12.3222 - val_loss: 18.4414\n","Epoch 146/150\n","90/90 - 1s - loss: 12.3447 - val_loss: 18.4416\n","Epoch 147/150\n","90/90 - 1s - loss: 12.3420 - val_loss: 18.3631\n","Epoch 148/150\n","90/90 - 1s - loss: 12.3585 - val_loss: 18.4219\n","Epoch 149/150\n","90/90 - 1s - loss: 12.2875 - val_loss: 18.4081\n","Epoch 150/150\n","90/90 - 1s - loss: 12.2875 - val_loss: 18.3313\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ty2RKHcVGdOk","executionInfo":{"status":"ok","timestamp":1604626200157,"user_tz":180,"elapsed":1706,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"016c4d1d-9a0d-4e2f-8f26-505a13a5ebbd","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 27.357\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TLwwMmCoGeVd","executionInfo":{"status":"ok","timestamp":1604626237902,"user_tz":180,"elapsed":827,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"568a8e99-e39a-439d-9ab2-b4bbec4232d0","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 18.331\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WkbjB8j-GeJ5","executionInfo":{"status":"ok","timestamp":1604626241173,"user_tz":180,"elapsed":758,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"2ebb2a5c-bd8d-4775-ed6f-ee3dad869269","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.417\n"],"name":"stdout"}]}]}