{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Etapa 3: melhor modelo com vários LAG.ipynb","provenance":[],"authorship_tag":"ABX9TyO4fTR66tr1LxHGhdiXTOY2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"E40manLAqUmX"},"source":["#**Pré-processamento**"]},{"cell_type":"code","metadata":{"id":"KtXmXeGTqLtb","executionInfo":{"status":"ok","timestamp":1604887144141,"user_tz":180,"elapsed":24272,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"acbe2d38-585b-4981-e08e-1a89f11f1aab","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive \n","drive.mount('/content/gdrive')\n","\n","import pandas as pd\n","import seaborn as sns\n","import missingno as msno\n","import warnings\n","from matplotlib import pyplot as plt\n","import datetime\n","import numpy as np\n","from statsmodels.graphics.tsaplots import plot_acf\n","from statsmodels.graphics.tsaplots import plot_pacf\n","from matplotlib import pyplot\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns',100)\n","\n","df = pd.read_csv(\"gdrive/My Drive/dissertação/Qualidade_do_ar_-_Dados_horarios.csv\",sep=\",\")\n","df['Data']=df['Data'].astype('str')\n","df['Data']=df['Data'].apply(lambda x: x[:-3])\n","\n","#conversão para datetime\n","def convert_to_datetime(x):\n","    return(datetime.datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\"))\n","df['Data']=df['Data'].apply(convert_to_datetime) \n","df['Hora-minuto']=df['Data'].apply(lambda x: str(x)[-8:-3])\n","# #extrair ano, mes, hora \n","df['Ano']=df['Data'].apply(lambda x: x.year)\n","df['Mês']=df['Data'].apply(lambda x: x.month)\n","df['Dia']=df['Data'].apply(lambda x: x.day)\n","df['Hora']=df['Data'].apply(lambda x: x.hour)\n","df['Hora-minuto']=df['Data'].apply(lambda x: str(x)[-11:-6])\n","\n","df_bg=df[df['Estação']=='BG'] #Bangu\n","df_gc=df[df['Estação']=='CG'] #Campo Grande\n","df_ca=df[df['Estação']=='CA'] #Centro\n","df_av=df[df['Estação']=='AV'] #Copacabana\n","df_ir=df[df['Estação']=='IR'] #Irajá\n","df_pg=df[df['Estação']=='PG'] #Pedra de Guaratiba\n","df_sc=df[df['Estação']=='SC'] #São Cristóvão\n","df_sp=df[df['Estação']=='SP'] #Tijuca\n","\n","df_sc.set_index('Data',inplace=True)\n","df_sc.drop(columns=['OBJECTID','CodNum','Estação','Dir_Vento','NO2','HCNM','HCT','CH4','NO','NOx',\\\n","                   'PM2_5', 'Lat', 'Lon', 'X_UTM_Sirgas2000','Y_UTM_Sirgas2000'], inplace=True)\n","df_sc_2=df_sc.copy() #para fazer distribuição sem eliminar ausentes\n","\n","df_sc=df_sc['2011-01-01':'2018-01-01']\n","#explicar que usou interpolação linear com spline e justificar\n","df_sc['Chuva'].interpolate(method='slinear', inplace=True)\n","df_sc['Pres'].interpolate(method='slinear', inplace=True)\n","df_sc['RS'].interpolate(method='slinear', inplace=True)\n","df_sc['Temp'].interpolate(method='slinear', inplace=True)\n","df_sc['UR'].interpolate(method='slinear', inplace=True)\n","df_sc['Vel_Vento'].interpolate(method='slinear', inplace=True)\n","df_sc['SO2'].interpolate(method='slinear', inplace=True)\n","df_sc['CO'].interpolate(method='slinear', inplace=True)\n","df_sc['O3'].interpolate(method='slinear', inplace=True)\n","df_sc['PM10'].interpolate(method='slinear', inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BOkzHEjnq7La"},"source":["#**(spline) LAG=1**"]},{"cell_type":"code","metadata":{"id":"hSIChFjTrBa9","executionInfo":{"status":"ok","timestamp":1604886889586,"user_tz":180,"elapsed":60892,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"d58819ef-024a-49f3-ad0b-76b9f598ba72","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 1 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 1)\n","reframed = series_to_supervised(scaled, n_hours, 1) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 19.8062 - val_loss: 19.8556\n","Epoch 2/150\n","90/90 - 0s - loss: 18.8252 - val_loss: 19.0649\n","Epoch 3/150\n","90/90 - 0s - loss: 18.1781 - val_loss: 18.3770\n","Epoch 4/150\n","90/90 - 0s - loss: 17.6250 - val_loss: 17.8171\n","Epoch 5/150\n","90/90 - 0s - loss: 17.1063 - val_loss: 17.2910\n","Epoch 6/150\n","90/90 - 0s - loss: 16.6127 - val_loss: 16.8260\n","Epoch 7/150\n","90/90 - 0s - loss: 16.1934 - val_loss: 16.4056\n","Epoch 8/150\n","90/90 - 0s - loss: 15.8227 - val_loss: 16.0234\n","Epoch 9/150\n","90/90 - 0s - loss: 15.4791 - val_loss: 15.6696\n","Epoch 10/150\n","90/90 - 0s - loss: 15.1658 - val_loss: 15.3447\n","Epoch 11/150\n","90/90 - 0s - loss: 14.8744 - val_loss: 15.0320\n","Epoch 12/150\n","90/90 - 0s - loss: 14.5916 - val_loss: 14.7522\n","Epoch 13/150\n","90/90 - 0s - loss: 14.3230 - val_loss: 14.4723\n","Epoch 14/150\n","90/90 - 0s - loss: 14.0234 - val_loss: 14.1719\n","Epoch 15/150\n","90/90 - 0s - loss: 13.7645 - val_loss: 13.8875\n","Epoch 16/150\n","90/90 - 0s - loss: 13.5286 - val_loss: 13.6332\n","Epoch 17/150\n","90/90 - 0s - loss: 13.3035 - val_loss: 13.4332\n","Epoch 18/150\n","90/90 - 0s - loss: 13.0931 - val_loss: 13.1940\n","Epoch 19/150\n","90/90 - 0s - loss: 12.8914 - val_loss: 12.9896\n","Epoch 20/150\n","90/90 - 0s - loss: 12.7001 - val_loss: 12.7873\n","Epoch 21/150\n","90/90 - 0s - loss: 12.5001 - val_loss: 12.5842\n","Epoch 22/150\n","90/90 - 0s - loss: 12.3164 - val_loss: 12.3895\n","Epoch 23/150\n","90/90 - 0s - loss: 12.1384 - val_loss: 12.2174\n","Epoch 24/150\n","90/90 - 0s - loss: 11.9695 - val_loss: 12.0282\n","Epoch 25/150\n","90/90 - 0s - loss: 11.7799 - val_loss: 11.8491\n","Epoch 26/150\n","90/90 - 0s - loss: 11.6055 - val_loss: 11.6687\n","Epoch 27/150\n","90/90 - 0s - loss: 11.4308 - val_loss: 11.4986\n","Epoch 28/150\n","90/90 - 0s - loss: 11.2682 - val_loss: 11.3349\n","Epoch 29/150\n","90/90 - 0s - loss: 11.1120 - val_loss: 11.1850\n","Epoch 30/150\n","90/90 - 0s - loss: 10.9678 - val_loss: 11.0246\n","Epoch 31/150\n","90/90 - 0s - loss: 10.8191 - val_loss: 10.8962\n","Epoch 32/150\n","90/90 - 0s - loss: 10.6790 - val_loss: 10.7506\n","Epoch 33/150\n","90/90 - 0s - loss: 10.5352 - val_loss: 10.5961\n","Epoch 34/150\n","90/90 - 0s - loss: 10.3485 - val_loss: 10.4152\n","Epoch 35/150\n","90/90 - 0s - loss: 10.1832 - val_loss: 10.2361\n","Epoch 36/150\n","90/90 - 0s - loss: 10.0324 - val_loss: 10.0949\n","Epoch 37/150\n","90/90 - 0s - loss: 9.8913 - val_loss: 9.9555\n","Epoch 38/150\n","90/90 - 0s - loss: 9.7544 - val_loss: 9.8203\n","Epoch 39/150\n","90/90 - 0s - loss: 9.6325 - val_loss: 9.7102\n","Epoch 40/150\n","90/90 - 0s - loss: 9.5135 - val_loss: 9.5747\n","Epoch 41/150\n","90/90 - 0s - loss: 9.3932 - val_loss: 9.4542\n","Epoch 42/150\n","90/90 - 0s - loss: 9.2787 - val_loss: 9.3496\n","Epoch 43/150\n","90/90 - 0s - loss: 9.1824 - val_loss: 9.2312\n","Epoch 44/150\n","90/90 - 0s - loss: 9.0911 - val_loss: 9.1346\n","Epoch 45/150\n","90/90 - 0s - loss: 8.9795 - val_loss: 9.0302\n","Epoch 46/150\n","90/90 - 0s - loss: 8.8843 - val_loss: 8.9293\n","Epoch 47/150\n","90/90 - 0s - loss: 8.7851 - val_loss: 8.8464\n","Epoch 48/150\n","90/90 - 0s - loss: 8.7068 - val_loss: 8.7497\n","Epoch 49/150\n","90/90 - 0s - loss: 8.6198 - val_loss: 8.6586\n","Epoch 50/150\n","90/90 - 0s - loss: 8.5393 - val_loss: 8.5905\n","Epoch 51/150\n","90/90 - 0s - loss: 8.4535 - val_loss: 8.5039\n","Epoch 52/150\n","90/90 - 0s - loss: 8.3867 - val_loss: 8.4261\n","Epoch 53/150\n","90/90 - 0s - loss: 8.3080 - val_loss: 8.3523\n","Epoch 54/150\n","90/90 - 0s - loss: 8.2269 - val_loss: 8.2858\n","Epoch 55/150\n","90/90 - 0s - loss: 8.1597 - val_loss: 8.2165\n","Epoch 56/150\n","90/90 - 0s - loss: 8.0896 - val_loss: 8.1488\n","Epoch 57/150\n","90/90 - 0s - loss: 8.0241 - val_loss: 8.0882\n","Epoch 58/150\n","90/90 - 0s - loss: 7.9597 - val_loss: 8.0301\n","Epoch 59/150\n","90/90 - 0s - loss: 7.8984 - val_loss: 7.9723\n","Epoch 60/150\n","90/90 - 0s - loss: 7.8446 - val_loss: 7.9147\n","Epoch 61/150\n","90/90 - 0s - loss: 7.7810 - val_loss: 7.8529\n","Epoch 62/150\n","90/90 - 0s - loss: 7.7318 - val_loss: 7.8054\n","Epoch 63/150\n","90/90 - 0s - loss: 7.6693 - val_loss: 7.7590\n","Epoch 64/150\n","90/90 - 0s - loss: 7.6231 - val_loss: 7.7098\n","Epoch 65/150\n","90/90 - 0s - loss: 7.5768 - val_loss: 7.6709\n","Epoch 66/150\n","90/90 - 0s - loss: 7.5298 - val_loss: 7.6225\n","Epoch 67/150\n","90/90 - 0s - loss: 7.4839 - val_loss: 7.5822\n","Epoch 68/150\n","90/90 - 0s - loss: 7.4502 - val_loss: 7.5386\n","Epoch 69/150\n","90/90 - 0s - loss: 7.3931 - val_loss: 7.5037\n","Epoch 70/150\n","90/90 - 0s - loss: 7.3574 - val_loss: 7.4600\n","Epoch 71/150\n","90/90 - 0s - loss: 7.3101 - val_loss: 7.4296\n","Epoch 72/150\n","90/90 - 0s - loss: 7.2735 - val_loss: 7.3917\n","Epoch 73/150\n","90/90 - 0s - loss: 7.2435 - val_loss: 7.3631\n","Epoch 74/150\n","90/90 - 0s - loss: 7.1973 - val_loss: 7.3248\n","Epoch 75/150\n","90/90 - 0s - loss: 7.1610 - val_loss: 7.2969\n","Epoch 76/150\n","90/90 - 0s - loss: 7.1313 - val_loss: 7.2644\n","Epoch 77/150\n","90/90 - 0s - loss: 7.0905 - val_loss: 7.2486\n","Epoch 78/150\n","90/90 - 0s - loss: 7.0661 - val_loss: 7.2239\n","Epoch 79/150\n","90/90 - 0s - loss: 7.0293 - val_loss: 7.1853\n","Epoch 80/150\n","90/90 - 0s - loss: 7.0100 - val_loss: 7.1720\n","Epoch 81/150\n","90/90 - 0s - loss: 6.9790 - val_loss: 7.1512\n","Epoch 82/150\n","90/90 - 0s - loss: 6.9549 - val_loss: 7.1241\n","Epoch 83/150\n","90/90 - 0s - loss: 6.9292 - val_loss: 7.1058\n","Epoch 84/150\n","90/90 - 0s - loss: 6.9066 - val_loss: 7.0815\n","Epoch 85/150\n","90/90 - 0s - loss: 6.8904 - val_loss: 7.0751\n","Epoch 86/150\n","90/90 - 0s - loss: 6.8622 - val_loss: 7.0510\n","Epoch 87/150\n","90/90 - 0s - loss: 6.8349 - val_loss: 7.0148\n","Epoch 88/150\n","90/90 - 0s - loss: 6.8074 - val_loss: 7.0109\n","Epoch 89/150\n","90/90 - 0s - loss: 6.7878 - val_loss: 6.9784\n","Epoch 90/150\n","90/90 - 0s - loss: 6.7584 - val_loss: 6.9596\n","Epoch 91/150\n","90/90 - 0s - loss: 6.7377 - val_loss: 6.9418\n","Epoch 92/150\n","90/90 - 0s - loss: 6.7170 - val_loss: 6.9199\n","Epoch 93/150\n","90/90 - 0s - loss: 6.7019 - val_loss: 6.9023\n","Epoch 94/150\n","90/90 - 0s - loss: 6.6836 - val_loss: 6.9250\n","Epoch 95/150\n","90/90 - 0s - loss: 6.6674 - val_loss: 6.8638\n","Epoch 96/150\n","90/90 - 0s - loss: 6.6384 - val_loss: 6.8391\n","Epoch 97/150\n","90/90 - 0s - loss: 6.6310 - val_loss: 6.8190\n","Epoch 98/150\n","90/90 - 0s - loss: 6.5983 - val_loss: 6.8202\n","Epoch 99/150\n","90/90 - 0s - loss: 6.5850 - val_loss: 6.7989\n","Epoch 100/150\n","90/90 - 0s - loss: 6.5718 - val_loss: 6.7907\n","Epoch 101/150\n","90/90 - 0s - loss: 6.5493 - val_loss: 6.7859\n","Epoch 102/150\n","90/90 - 0s - loss: 6.5360 - val_loss: 6.7877\n","Epoch 103/150\n","90/90 - 0s - loss: 6.5205 - val_loss: 6.7745\n","Epoch 104/150\n","90/90 - 0s - loss: 6.5063 - val_loss: 6.7604\n","Epoch 105/150\n","90/90 - 0s - loss: 6.5025 - val_loss: 6.7618\n","Epoch 106/150\n","90/90 - 0s - loss: 6.4804 - val_loss: 6.7157\n","Epoch 107/150\n","90/90 - 0s - loss: 6.4805 - val_loss: 6.7212\n","Epoch 108/150\n","90/90 - 0s - loss: 6.4566 - val_loss: 6.6979\n","Epoch 109/150\n","90/90 - 0s - loss: 6.4445 - val_loss: 6.6782\n","Epoch 110/150\n","90/90 - 0s - loss: 6.4333 - val_loss: 6.6614\n","Epoch 111/150\n","90/90 - 0s - loss: 6.4273 - val_loss: 6.6459\n","Epoch 112/150\n","90/90 - 0s - loss: 6.4086 - val_loss: 6.6461\n","Epoch 113/150\n","90/90 - 0s - loss: 6.4056 - val_loss: 6.6427\n","Epoch 114/150\n","90/90 - 0s - loss: 6.3962 - val_loss: 6.6384\n","Epoch 115/150\n","90/90 - 0s - loss: 6.3828 - val_loss: 6.6167\n","Epoch 116/150\n","90/90 - 0s - loss: 6.3674 - val_loss: 6.6095\n","Epoch 117/150\n","90/90 - 0s - loss: 6.3614 - val_loss: 6.6042\n","Epoch 118/150\n","90/90 - 0s - loss: 6.3498 - val_loss: 6.5966\n","Epoch 119/150\n","90/90 - 0s - loss: 6.3427 - val_loss: 6.5939\n","Epoch 120/150\n","90/90 - 0s - loss: 6.3301 - val_loss: 6.5739\n","Epoch 121/150\n","90/90 - 0s - loss: 6.3232 - val_loss: 6.5782\n","Epoch 122/150\n","90/90 - 0s - loss: 6.3172 - val_loss: 6.5723\n","Epoch 123/150\n","90/90 - 0s - loss: 6.3050 - val_loss: 6.5711\n","Epoch 124/150\n","90/90 - 0s - loss: 6.2988 - val_loss: 6.5547\n","Epoch 125/150\n","90/90 - 0s - loss: 6.3023 - val_loss: 6.5431\n","Epoch 126/150\n","90/90 - 0s - loss: 6.2752 - val_loss: 6.5493\n","Epoch 127/150\n","90/90 - 0s - loss: 6.2758 - val_loss: 6.5461\n","Epoch 128/150\n","90/90 - 0s - loss: 6.2601 - val_loss: 6.5171\n","Epoch 129/150\n","90/90 - 0s - loss: 6.2465 - val_loss: 6.5124\n","Epoch 130/150\n","90/90 - 0s - loss: 6.2468 - val_loss: 6.5088\n","Epoch 131/150\n","90/90 - 0s - loss: 6.2457 - val_loss: 6.5091\n","Epoch 132/150\n","90/90 - 0s - loss: 6.2351 - val_loss: 6.4976\n","Epoch 133/150\n","90/90 - 0s - loss: 6.2286 - val_loss: 6.4825\n","Epoch 134/150\n","90/90 - 0s - loss: 6.2190 - val_loss: 6.4936\n","Epoch 135/150\n","90/90 - 0s - loss: 6.2126 - val_loss: 6.4895\n","Epoch 136/150\n","90/90 - 0s - loss: 6.2105 - val_loss: 6.4946\n","Epoch 137/150\n","90/90 - 0s - loss: 6.2006 - val_loss: 6.5039\n","Epoch 138/150\n","90/90 - 0s - loss: 6.1945 - val_loss: 6.4724\n","Epoch 139/150\n","90/90 - 0s - loss: 6.1745 - val_loss: 6.4613\n","Epoch 140/150\n","90/90 - 0s - loss: 6.1913 - val_loss: 6.4670\n","Epoch 141/150\n","90/90 - 0s - loss: 6.1731 - val_loss: 6.4612\n","Epoch 142/150\n","90/90 - 0s - loss: 6.1704 - val_loss: 6.4677\n","Epoch 143/150\n","90/90 - 0s - loss: 6.1753 - val_loss: 6.4518\n","Epoch 144/150\n","90/90 - 0s - loss: 6.1580 - val_loss: 6.4516\n","Epoch 145/150\n","90/90 - 0s - loss: 6.1476 - val_loss: 6.4591\n","Epoch 146/150\n","90/90 - 0s - loss: 6.1449 - val_loss: 6.4366\n","Epoch 147/150\n","90/90 - 0s - loss: 6.1497 - val_loss: 6.4356\n","Epoch 148/150\n","90/90 - 0s - loss: 6.1382 - val_loss: 6.4249\n","Epoch 149/150\n","90/90 - 0s - loss: 6.1174 - val_loss: 6.4132\n","Epoch 150/150\n","90/90 - 0s - loss: 6.1228 - val_loss: 6.3883\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yz9vPlnEsGt8","executionInfo":{"status":"ok","timestamp":1604886904015,"user_tz":180,"elapsed":1580,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"bff02e86-0466-465d-e483-f85694bbbecd","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 10.802\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UJEf1fhNsHIs","executionInfo":{"status":"ok","timestamp":1604886911568,"user_tz":180,"elapsed":732,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"938fecc9-8766-42d4-d392-c3880e81f34a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 6.388\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lMpj-NgVsG7_","executionInfo":{"status":"ok","timestamp":1604886915150,"user_tz":180,"elapsed":694,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"aa958d28-cb32-433e-95b5-34df5e5d5443","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.796\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iupp2D8bXou_"},"source":["# plot history\n","plt.style.use('seaborn')\n","plt.figure(figsize=(18,9))\n","pyplot.plot(history.history['loss'], label='train', color='blue')\n","pyplot.plot(history.history['val_loss'], label='test', color='red')\n","plt.xlabel('epoch',fontsize=20)\n","plt.ylabel('loss',fontsize=20) \n","# plt.title('train and test loss evolution',fontsize=20)\n","plt.xticks(fontsize=20)\n","plt.yticks(fontsize=20)\n","plt.tight_layout()\n","pyplot.legend(fontsize=20)\n","pyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TkL4_L5XpHS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8tJ7o-EXo_V"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWpGF1SJXobp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tXAoAZX-rCgN"},"source":["#**(spline) LAG=2**"]},{"cell_type":"code","metadata":{"id":"mkBHYwmdrHFN","executionInfo":{"status":"ok","timestamp":1604887023295,"user_tz":180,"elapsed":49557,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"eb9b7f89-97e3-44c7-e99d-a04d082cfc49","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 2 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 2)\n","reframed = series_to_supervised(scaled, n_hours, 2) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 19.6868 - val_loss: 19.4623\n","Epoch 2/150\n","90/90 - 0s - loss: 18.4972 - val_loss: 18.6856\n","Epoch 3/150\n","90/90 - 0s - loss: 17.8976 - val_loss: 18.1207\n","Epoch 4/150\n","90/90 - 0s - loss: 17.4174 - val_loss: 17.6421\n","Epoch 5/150\n","90/90 - 0s - loss: 16.9921 - val_loss: 17.2127\n","Epoch 6/150\n","90/90 - 0s - loss: 16.5792 - val_loss: 16.7782\n","Epoch 7/150\n","90/90 - 0s - loss: 16.1141 - val_loss: 16.2924\n","Epoch 8/150\n","90/90 - 0s - loss: 15.7135 - val_loss: 15.8792\n","Epoch 9/150\n","90/90 - 0s - loss: 15.3590 - val_loss: 15.5254\n","Epoch 10/150\n","90/90 - 0s - loss: 15.0279 - val_loss: 15.1785\n","Epoch 11/150\n","90/90 - 0s - loss: 14.6929 - val_loss: 14.8397\n","Epoch 12/150\n","90/90 - 0s - loss: 14.3638 - val_loss: 14.5616\n","Epoch 13/150\n","90/90 - 0s - loss: 14.0905 - val_loss: 14.2326\n","Epoch 14/150\n","90/90 - 0s - loss: 13.8347 - val_loss: 13.9760\n","Epoch 15/150\n","90/90 - 0s - loss: 13.5923 - val_loss: 13.7486\n","Epoch 16/150\n","90/90 - 0s - loss: 13.3719 - val_loss: 13.5095\n","Epoch 17/150\n","90/90 - 0s - loss: 13.1576 - val_loss: 13.3140\n","Epoch 18/150\n","90/90 - 0s - loss: 12.9494 - val_loss: 13.1013\n","Epoch 19/150\n","90/90 - 0s - loss: 12.7567 - val_loss: 12.9112\n","Epoch 20/150\n","90/90 - 0s - loss: 12.5838 - val_loss: 12.7321\n","Epoch 21/150\n","90/90 - 0s - loss: 12.4108 - val_loss: 12.5452\n","Epoch 22/150\n","90/90 - 0s - loss: 12.2405 - val_loss: 12.3949\n","Epoch 23/150\n","90/90 - 0s - loss: 12.0753 - val_loss: 12.2368\n","Epoch 24/150\n","90/90 - 0s - loss: 11.9224 - val_loss: 12.0843\n","Epoch 25/150\n","90/90 - 0s - loss: 11.7829 - val_loss: 11.9598\n","Epoch 26/150\n","90/90 - 0s - loss: 11.6136 - val_loss: 11.7360\n","Epoch 27/150\n","90/90 - 0s - loss: 11.4540 - val_loss: 11.6397\n","Epoch 28/150\n","90/90 - 0s - loss: 11.3183 - val_loss: 11.4599\n","Epoch 29/150\n","90/90 - 0s - loss: 11.1843 - val_loss: 11.3611\n","Epoch 30/150\n","90/90 - 0s - loss: 11.0962 - val_loss: 11.4093\n","Epoch 31/150\n","90/90 - 0s - loss: 11.1804 - val_loss: 11.1608\n","Epoch 32/150\n","90/90 - 0s - loss: 10.8704 - val_loss: 11.0828\n","Epoch 33/150\n","90/90 - 0s - loss: 10.7796 - val_loss: 10.9122\n","Epoch 34/150\n","90/90 - 0s - loss: 10.6687 - val_loss: 10.8156\n","Epoch 35/150\n","90/90 - 0s - loss: 10.5757 - val_loss: 10.6863\n","Epoch 36/150\n","90/90 - 0s - loss: 10.4822 - val_loss: 10.6017\n","Epoch 37/150\n","90/90 - 0s - loss: 10.4033 - val_loss: 10.5343\n","Epoch 38/150\n","90/90 - 0s - loss: 10.3195 - val_loss: 10.4542\n","Epoch 39/150\n","90/90 - 0s - loss: 10.2449 - val_loss: 10.3800\n","Epoch 40/150\n","90/90 - 0s - loss: 10.1740 - val_loss: 10.3249\n","Epoch 41/150\n","90/90 - 0s - loss: 10.1081 - val_loss: 10.2374\n","Epoch 42/150\n","90/90 - 0s - loss: 10.0454 - val_loss: 10.1843\n","Epoch 43/150\n","90/90 - 0s - loss: 9.9803 - val_loss: 10.1070\n","Epoch 44/150\n","90/90 - 0s - loss: 9.9209 - val_loss: 10.0930\n","Epoch 45/150\n","90/90 - 0s - loss: 9.8775 - val_loss: 10.0102\n","Epoch 46/150\n","90/90 - 0s - loss: 9.8283 - val_loss: 10.0091\n","Epoch 47/150\n","90/90 - 0s - loss: 9.7735 - val_loss: 9.9687\n","Epoch 48/150\n","90/90 - 0s - loss: 9.7229 - val_loss: 9.8926\n","Epoch 49/150\n","90/90 - 0s - loss: 9.6890 - val_loss: 9.8756\n","Epoch 50/150\n","90/90 - 0s - loss: 9.6583 - val_loss: 9.8223\n","Epoch 51/150\n","90/90 - 0s - loss: 9.5943 - val_loss: 9.8016\n","Epoch 52/150\n","90/90 - 0s - loss: 9.5519 - val_loss: 9.7106\n","Epoch 53/150\n","90/90 - 0s - loss: 9.5176 - val_loss: 9.7056\n","Epoch 54/150\n","90/90 - 0s - loss: 9.4755 - val_loss: 9.6813\n","Epoch 55/150\n","90/90 - 0s - loss: 9.4296 - val_loss: 9.6118\n","Epoch 56/150\n","90/90 - 0s - loss: 9.4034 - val_loss: 9.5760\n","Epoch 57/150\n","90/90 - 0s - loss: 9.3707 - val_loss: 9.5589\n","Epoch 58/150\n","90/90 - 0s - loss: 9.3406 - val_loss: 9.5255\n","Epoch 59/150\n","90/90 - 0s - loss: 9.3043 - val_loss: 9.4997\n","Epoch 60/150\n","90/90 - 0s - loss: 9.2747 - val_loss: 9.4787\n","Epoch 61/150\n","90/90 - 0s - loss: 9.2456 - val_loss: 9.4692\n","Epoch 62/150\n","90/90 - 0s - loss: 9.2232 - val_loss: 9.4389\n","Epoch 63/150\n","90/90 - 0s - loss: 9.2002 - val_loss: 9.4125\n","Epoch 64/150\n","90/90 - 0s - loss: 9.1804 - val_loss: 9.4342\n","Epoch 65/150\n","90/90 - 0s - loss: 9.1390 - val_loss: 9.3894\n","Epoch 66/150\n","90/90 - 0s - loss: 9.1080 - val_loss: 9.3403\n","Epoch 67/150\n","90/90 - 0s - loss: 9.0897 - val_loss: 9.3098\n","Epoch 68/150\n","90/90 - 0s - loss: 9.0600 - val_loss: 9.3072\n","Epoch 69/150\n","90/90 - 0s - loss: 9.0414 - val_loss: 9.2639\n","Epoch 70/150\n","90/90 - 0s - loss: 9.0210 - val_loss: 9.2577\n","Epoch 71/150\n","90/90 - 0s - loss: 8.9984 - val_loss: 9.2293\n","Epoch 72/150\n","90/90 - 0s - loss: 8.9826 - val_loss: 9.2094\n","Epoch 73/150\n","90/90 - 0s - loss: 8.9686 - val_loss: 9.2009\n","Epoch 74/150\n","90/90 - 0s - loss: 8.9498 - val_loss: 9.1818\n","Epoch 75/150\n","90/90 - 0s - loss: 8.9230 - val_loss: 9.1707\n","Epoch 76/150\n","90/90 - 0s - loss: 8.9117 - val_loss: 9.1558\n","Epoch 77/150\n","90/90 - 0s - loss: 8.8850 - val_loss: 9.1541\n","Epoch 78/150\n","90/90 - 0s - loss: 8.8905 - val_loss: 9.1346\n","Epoch 79/150\n","90/90 - 0s - loss: 8.8767 - val_loss: 9.1192\n","Epoch 80/150\n","90/90 - 0s - loss: 8.8585 - val_loss: 9.1188\n","Epoch 81/150\n","90/90 - 0s - loss: 8.8544 - val_loss: 9.1156\n","Epoch 82/150\n","90/90 - 0s - loss: 8.8268 - val_loss: 9.1075\n","Epoch 83/150\n","90/90 - 0s - loss: 8.8159 - val_loss: 9.0723\n","Epoch 84/150\n","90/90 - 0s - loss: 8.8017 - val_loss: 9.0710\n","Epoch 85/150\n","90/90 - 0s - loss: 8.7930 - val_loss: 9.1019\n","Epoch 86/150\n","90/90 - 0s - loss: 8.7752 - val_loss: 9.0876\n","Epoch 87/150\n","90/90 - 0s - loss: 8.7631 - val_loss: 9.0482\n","Epoch 88/150\n","90/90 - 0s - loss: 8.7608 - val_loss: 9.0472\n","Epoch 89/150\n","90/90 - 0s - loss: 8.7426 - val_loss: 9.0423\n","Epoch 90/150\n","90/90 - 0s - loss: 8.7356 - val_loss: 9.0243\n","Epoch 91/150\n","90/90 - 0s - loss: 8.7302 - val_loss: 9.0274\n","Epoch 92/150\n","90/90 - 0s - loss: 8.7155 - val_loss: 9.0172\n","Epoch 93/150\n","90/90 - 0s - loss: 8.7112 - val_loss: 8.9813\n","Epoch 94/150\n","90/90 - 0s - loss: 8.6872 - val_loss: 8.9729\n","Epoch 95/150\n","90/90 - 0s - loss: 8.6653 - val_loss: 8.9993\n","Epoch 96/150\n","90/90 - 0s - loss: 8.6624 - val_loss: 8.9774\n","Epoch 97/150\n","90/90 - 0s - loss: 8.6595 - val_loss: 8.9548\n","Epoch 98/150\n","90/90 - 0s - loss: 8.6452 - val_loss: 8.9670\n","Epoch 99/150\n","90/90 - 0s - loss: 8.6268 - val_loss: 8.9293\n","Epoch 100/150\n","90/90 - 0s - loss: 8.6285 - val_loss: 8.9277\n","Epoch 101/150\n","90/90 - 0s - loss: 8.6248 - val_loss: 8.9390\n","Epoch 102/150\n","90/90 - 0s - loss: 8.6178 - val_loss: 8.9096\n","Epoch 103/150\n","90/90 - 0s - loss: 8.5794 - val_loss: 8.9119\n","Epoch 104/150\n","90/90 - 0s - loss: 8.6087 - val_loss: 8.9082\n","Epoch 105/150\n","90/90 - 0s - loss: 8.5887 - val_loss: 8.8767\n","Epoch 106/150\n","90/90 - 0s - loss: 8.5945 - val_loss: 8.8999\n","Epoch 107/150\n","90/90 - 0s - loss: 8.5667 - val_loss: 8.8930\n","Epoch 108/150\n","90/90 - 0s - loss: 8.5870 - val_loss: 8.9172\n","Epoch 109/150\n","90/90 - 0s - loss: 8.5655 - val_loss: 8.8891\n","Epoch 110/150\n","90/90 - 0s - loss: 8.5658 - val_loss: 8.8457\n","Epoch 111/150\n","90/90 - 0s - loss: 8.5417 - val_loss: 8.8686\n","Epoch 112/150\n","90/90 - 0s - loss: 8.5580 - val_loss: 8.8557\n","Epoch 113/150\n","90/90 - 0s - loss: 8.5377 - val_loss: 8.8437\n","Epoch 114/150\n","90/90 - 0s - loss: 8.5367 - val_loss: 8.8640\n","Epoch 115/150\n","90/90 - 0s - loss: 8.5220 - val_loss: 8.8200\n","Epoch 116/150\n","90/90 - 0s - loss: 8.5295 - val_loss: 8.8172\n","Epoch 117/150\n","90/90 - 0s - loss: 8.5075 - val_loss: 8.8139\n","Epoch 118/150\n","90/90 - 0s - loss: 8.5081 - val_loss: 8.8137\n","Epoch 119/150\n","90/90 - 0s - loss: 8.5082 - val_loss: 8.8225\n","Epoch 120/150\n","90/90 - 0s - loss: 8.5003 - val_loss: 8.7923\n","Epoch 121/150\n","90/90 - 0s - loss: 8.4800 - val_loss: 8.8013\n","Epoch 122/150\n","90/90 - 0s - loss: 8.4855 - val_loss: 8.7893\n","Epoch 123/150\n","90/90 - 0s - loss: 8.4687 - val_loss: 8.7813\n","Epoch 124/150\n","90/90 - 0s - loss: 8.4719 - val_loss: 8.7886\n","Epoch 125/150\n","90/90 - 0s - loss: 8.4643 - val_loss: 8.7828\n","Epoch 126/150\n","90/90 - 0s - loss: 8.4554 - val_loss: 8.8027\n","Epoch 127/150\n","90/90 - 0s - loss: 8.4587 - val_loss: 8.7595\n","Epoch 128/150\n","90/90 - 0s - loss: 8.4538 - val_loss: 8.7733\n","Epoch 129/150\n","90/90 - 0s - loss: 8.4454 - val_loss: 8.7603\n","Epoch 130/150\n","90/90 - 0s - loss: 8.4459 - val_loss: 8.7639\n","Epoch 131/150\n","90/90 - 0s - loss: 8.4234 - val_loss: 8.7555\n","Epoch 132/150\n","90/90 - 0s - loss: 8.4311 - val_loss: 8.7539\n","Epoch 133/150\n","90/90 - 0s - loss: 8.4128 - val_loss: 8.7409\n","Epoch 134/150\n","90/90 - 0s - loss: 8.4168 - val_loss: 8.7409\n","Epoch 135/150\n","90/90 - 0s - loss: 8.4157 - val_loss: 8.7606\n","Epoch 136/150\n","90/90 - 0s - loss: 8.4111 - val_loss: 8.7305\n","Epoch 137/150\n","90/90 - 0s - loss: 8.4187 - val_loss: 8.7312\n","Epoch 138/150\n","90/90 - 0s - loss: 8.3960 - val_loss: 8.7238\n","Epoch 139/150\n","90/90 - 0s - loss: 8.3936 - val_loss: 8.7059\n","Epoch 140/150\n","90/90 - 0s - loss: 8.3894 - val_loss: 8.7181\n","Epoch 141/150\n","90/90 - 0s - loss: 8.3886 - val_loss: 8.7045\n","Epoch 142/150\n","90/90 - 0s - loss: 8.3782 - val_loss: 8.7426\n","Epoch 143/150\n","90/90 - 0s - loss: 8.3943 - val_loss: 8.7434\n","Epoch 144/150\n","90/90 - 0s - loss: 8.3806 - val_loss: 8.7102\n","Epoch 145/150\n","90/90 - 0s - loss: 8.3646 - val_loss: 8.6945\n","Epoch 146/150\n","90/90 - 0s - loss: 8.3753 - val_loss: 8.7112\n","Epoch 147/150\n","90/90 - 0s - loss: 8.3552 - val_loss: 8.6881\n","Epoch 148/150\n","90/90 - 0s - loss: 8.3431 - val_loss: 8.7128\n","Epoch 149/150\n","90/90 - 0s - loss: 8.3478 - val_loss: 8.7099\n","Epoch 150/150\n","90/90 - 0s - loss: 8.3786 - val_loss: 8.7433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A2FkfkdJsVLl","executionInfo":{"status":"ok","timestamp":1604887024127,"user_tz":180,"elapsed":50361,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"f7445dba-e7f6-4868-9a94-011de3713164","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 13.805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nFj7JzCJsVaT","executionInfo":{"status":"ok","timestamp":1604887024130,"user_tz":180,"elapsed":50361,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"c4aff936-97e6-4f5f-d7db-d31bc39a4086","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 8.743\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xiTrIjf9sVAH","executionInfo":{"status":"ok","timestamp":1604887024133,"user_tz":180,"elapsed":50363,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"8076da7b-819e-463e-ab1b-8234f5ee65fc","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4E07W2RRrKef"},"source":["#**(spline) LAG=3**"]},{"cell_type":"code","metadata":{"id":"JO3ZkogWrPVK","executionInfo":{"status":"ok","timestamp":1604887114554,"user_tz":180,"elapsed":50400,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"3b337800-80d7-4e8b-add5-e5a78e4487e4","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 3 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 2)\n","reframed = series_to_supervised(scaled, n_hours, 3) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 19.6661 - val_loss: 19.5101\n","Epoch 2/150\n","90/90 - 0s - loss: 18.5233 - val_loss: 18.7034\n","Epoch 3/150\n","90/90 - 0s - loss: 18.0111 - val_loss: 18.1943\n","Epoch 4/150\n","90/90 - 0s - loss: 17.5810 - val_loss: 17.7524\n","Epoch 5/150\n","90/90 - 0s - loss: 17.1861 - val_loss: 17.3494\n","Epoch 6/150\n","90/90 - 0s - loss: 16.8119 - val_loss: 16.9298\n","Epoch 7/150\n","90/90 - 0s - loss: 16.4535 - val_loss: 16.5887\n","Epoch 8/150\n","90/90 - 0s - loss: 16.1554 - val_loss: 16.2892\n","Epoch 9/150\n","90/90 - 0s - loss: 15.8723 - val_loss: 15.9797\n","Epoch 10/150\n","90/90 - 0s - loss: 15.5530 - val_loss: 15.6988\n","Epoch 11/150\n","90/90 - 0s - loss: 15.3065 - val_loss: 15.4232\n","Epoch 12/150\n","90/90 - 0s - loss: 15.0781 - val_loss: 15.2040\n","Epoch 13/150\n","90/90 - 0s - loss: 14.8396 - val_loss: 14.9674\n","Epoch 14/150\n","90/90 - 0s - loss: 14.6422 - val_loss: 14.7624\n","Epoch 15/150\n","90/90 - 0s - loss: 14.4715 - val_loss: 14.5841\n","Epoch 16/150\n","90/90 - 0s - loss: 14.2464 - val_loss: 14.3677\n","Epoch 17/150\n","90/90 - 0s - loss: 14.0808 - val_loss: 14.2130\n","Epoch 18/150\n","90/90 - 0s - loss: 13.9378 - val_loss: 14.0679\n","Epoch 19/150\n","90/90 - 0s - loss: 13.7909 - val_loss: 13.9085\n","Epoch 20/150\n","90/90 - 0s - loss: 13.6269 - val_loss: 13.7654\n","Epoch 21/150\n","90/90 - 0s - loss: 13.5113 - val_loss: 13.6413\n","Epoch 22/150\n","90/90 - 0s - loss: 13.3784 - val_loss: 13.5154\n","Epoch 23/150\n","90/90 - 0s - loss: 13.2560 - val_loss: 13.3900\n","Epoch 24/150\n","90/90 - 0s - loss: 13.1375 - val_loss: 13.2781\n","Epoch 25/150\n","90/90 - 0s - loss: 13.0287 - val_loss: 13.1691\n","Epoch 26/150\n","90/90 - 0s - loss: 12.9239 - val_loss: 13.0665\n","Epoch 27/150\n","90/90 - 0s - loss: 12.8222 - val_loss: 12.9895\n","Epoch 28/150\n","90/90 - 0s - loss: 12.7287 - val_loss: 12.8853\n","Epoch 29/150\n","90/90 - 0s - loss: 12.6462 - val_loss: 12.7919\n","Epoch 30/150\n","90/90 - 0s - loss: 12.5538 - val_loss: 12.7198\n","Epoch 31/150\n","90/90 - 0s - loss: 12.4717 - val_loss: 12.6283\n","Epoch 32/150\n","90/90 - 0s - loss: 12.3951 - val_loss: 12.5702\n","Epoch 33/150\n","90/90 - 0s - loss: 12.3246 - val_loss: 12.4821\n","Epoch 34/150\n","90/90 - 0s - loss: 12.2533 - val_loss: 12.4235\n","Epoch 35/150\n","90/90 - 0s - loss: 12.1817 - val_loss: 12.3667\n","Epoch 36/150\n","90/90 - 0s - loss: 12.1184 - val_loss: 12.3079\n","Epoch 37/150\n","90/90 - 0s - loss: 12.0575 - val_loss: 12.2475\n","Epoch 38/150\n","90/90 - 0s - loss: 11.9961 - val_loss: 12.1880\n","Epoch 39/150\n","90/90 - 0s - loss: 11.9404 - val_loss: 12.1367\n","Epoch 40/150\n","90/90 - 0s - loss: 11.8904 - val_loss: 12.0850\n","Epoch 41/150\n","90/90 - 0s - loss: 11.8430 - val_loss: 12.0027\n","Epoch 42/150\n","90/90 - 0s - loss: 11.7762 - val_loss: 12.0100\n","Epoch 43/150\n","90/90 - 0s - loss: 11.7477 - val_loss: 11.9469\n","Epoch 44/150\n","90/90 - 0s - loss: 11.7080 - val_loss: 11.9186\n","Epoch 45/150\n","90/90 - 0s - loss: 11.6509 - val_loss: 11.8403\n","Epoch 46/150\n","90/90 - 0s - loss: 11.5889 - val_loss: 11.8153\n","Epoch 47/150\n","90/90 - 0s - loss: 11.5806 - val_loss: 11.7997\n","Epoch 48/150\n","90/90 - 0s - loss: 11.5246 - val_loss: 11.7662\n","Epoch 49/150\n","90/90 - 0s - loss: 11.4889 - val_loss: 11.7129\n","Epoch 50/150\n","90/90 - 0s - loss: 11.4486 - val_loss: 11.6857\n","Epoch 51/150\n","90/90 - 0s - loss: 11.4115 - val_loss: 11.6968\n","Epoch 52/150\n","90/90 - 0s - loss: 11.3844 - val_loss: 11.6310\n","Epoch 53/150\n","90/90 - 0s - loss: 11.3495 - val_loss: 11.5873\n","Epoch 54/150\n","90/90 - 0s - loss: 11.3303 - val_loss: 11.5906\n","Epoch 55/150\n","90/90 - 0s - loss: 11.3208 - val_loss: 11.5802\n","Epoch 56/150\n","90/90 - 0s - loss: 11.2692 - val_loss: 11.5287\n","Epoch 57/150\n","90/90 - 0s - loss: 11.2428 - val_loss: 11.4995\n","Epoch 58/150\n","90/90 - 0s - loss: 11.2257 - val_loss: 11.4957\n","Epoch 59/150\n","90/90 - 0s - loss: 11.1999 - val_loss: 11.4959\n","Epoch 60/150\n","90/90 - 0s - loss: 11.1663 - val_loss: 11.4297\n","Epoch 61/150\n","90/90 - 0s - loss: 11.1506 - val_loss: 11.4194\n","Epoch 62/150\n","90/90 - 0s - loss: 11.1271 - val_loss: 11.4274\n","Epoch 63/150\n","90/90 - 0s - loss: 11.1067 - val_loss: 11.4142\n","Epoch 64/150\n","90/90 - 0s - loss: 11.0794 - val_loss: 11.3835\n","Epoch 65/150\n","90/90 - 0s - loss: 11.0720 - val_loss: 11.3735\n","Epoch 66/150\n","90/90 - 0s - loss: 11.0557 - val_loss: 11.3737\n","Epoch 67/150\n","90/90 - 0s - loss: 11.0350 - val_loss: 11.3120\n","Epoch 68/150\n","90/90 - 0s - loss: 11.0190 - val_loss: 11.3218\n","Epoch 69/150\n","90/90 - 0s - loss: 10.9880 - val_loss: 11.3031\n","Epoch 70/150\n","90/90 - 0s - loss: 10.9771 - val_loss: 11.2866\n","Epoch 71/150\n","90/90 - 0s - loss: 10.9656 - val_loss: 11.3021\n","Epoch 72/150\n","90/90 - 0s - loss: 10.9483 - val_loss: 11.2688\n","Epoch 73/150\n","90/90 - 0s - loss: 10.9492 - val_loss: 11.2780\n","Epoch 74/150\n","90/90 - 0s - loss: 10.9187 - val_loss: 11.2328\n","Epoch 75/150\n","90/90 - 0s - loss: 10.9169 - val_loss: 11.2321\n","Epoch 76/150\n","90/90 - 0s - loss: 10.9102 - val_loss: 11.2204\n","Epoch 77/150\n","90/90 - 0s - loss: 10.8787 - val_loss: 11.1804\n","Epoch 78/150\n","90/90 - 0s - loss: 10.8549 - val_loss: 11.1753\n","Epoch 79/150\n","90/90 - 0s - loss: 10.8484 - val_loss: 11.1814\n","Epoch 80/150\n","90/90 - 0s - loss: 10.8313 - val_loss: 11.1377\n","Epoch 81/150\n","90/90 - 0s - loss: 10.8235 - val_loss: 11.1555\n","Epoch 82/150\n","90/90 - 0s - loss: 10.8219 - val_loss: 11.1532\n","Epoch 83/150\n","90/90 - 0s - loss: 10.7783 - val_loss: 11.1218\n","Epoch 84/150\n","90/90 - 0s - loss: 10.7766 - val_loss: 11.1146\n","Epoch 85/150\n","90/90 - 0s - loss: 10.7812 - val_loss: 11.1326\n","Epoch 86/150\n","90/90 - 0s - loss: 10.7635 - val_loss: 11.0914\n","Epoch 87/150\n","90/90 - 0s - loss: 10.7526 - val_loss: 11.1086\n","Epoch 88/150\n","90/90 - 0s - loss: 10.7325 - val_loss: 11.0943\n","Epoch 89/150\n","90/90 - 0s - loss: 10.7238 - val_loss: 11.0885\n","Epoch 90/150\n","90/90 - 0s - loss: 10.7076 - val_loss: 11.0625\n","Epoch 91/150\n","90/90 - 0s - loss: 10.7132 - val_loss: 11.0688\n","Epoch 92/150\n","90/90 - 0s - loss: 10.7015 - val_loss: 11.0456\n","Epoch 93/150\n","90/90 - 0s - loss: 10.6812 - val_loss: 11.0438\n","Epoch 94/150\n","90/90 - 0s - loss: 10.6629 - val_loss: 11.0278\n","Epoch 95/150\n","90/90 - 0s - loss: 10.6502 - val_loss: 11.0267\n","Epoch 96/150\n","90/90 - 0s - loss: 10.6332 - val_loss: 10.9822\n","Epoch 97/150\n","90/90 - 0s - loss: 10.6226 - val_loss: 10.9867\n","Epoch 98/150\n","90/90 - 0s - loss: 10.6162 - val_loss: 11.0048\n","Epoch 99/150\n","90/90 - 0s - loss: 10.6042 - val_loss: 10.9805\n","Epoch 100/150\n","90/90 - 0s - loss: 10.6037 - val_loss: 10.9853\n","Epoch 101/150\n","90/90 - 0s - loss: 10.5760 - val_loss: 10.9592\n","Epoch 102/150\n","90/90 - 0s - loss: 10.5848 - val_loss: 10.9554\n","Epoch 103/150\n","90/90 - 0s - loss: 10.5687 - val_loss: 10.9697\n","Epoch 104/150\n","90/90 - 0s - loss: 10.5665 - val_loss: 10.9217\n","Epoch 105/150\n","90/90 - 0s - loss: 10.5487 - val_loss: 10.9438\n","Epoch 106/150\n","90/90 - 0s - loss: 10.5366 - val_loss: 10.8944\n","Epoch 107/150\n","90/90 - 0s - loss: 10.5415 - val_loss: 10.9173\n","Epoch 108/150\n","90/90 - 0s - loss: 10.5309 - val_loss: 10.8820\n","Epoch 109/150\n","90/90 - 0s - loss: 10.5460 - val_loss: 10.9071\n","Epoch 110/150\n","90/90 - 0s - loss: 10.5224 - val_loss: 10.8975\n","Epoch 111/150\n","90/90 - 0s - loss: 10.5423 - val_loss: 10.8858\n","Epoch 112/150\n","90/90 - 0s - loss: 10.5056 - val_loss: 10.8604\n","Epoch 113/150\n","90/90 - 0s - loss: 10.5088 - val_loss: 10.8930\n","Epoch 114/150\n","90/90 - 0s - loss: 10.4977 - val_loss: 10.8584\n","Epoch 115/150\n","90/90 - 0s - loss: 10.4855 - val_loss: 10.8498\n","Epoch 116/150\n","90/90 - 0s - loss: 10.4758 - val_loss: 10.8580\n","Epoch 117/150\n","90/90 - 0s - loss: 10.4460 - val_loss: 10.8326\n","Epoch 118/150\n","90/90 - 0s - loss: 10.4746 - val_loss: 10.8378\n","Epoch 119/150\n","90/90 - 0s - loss: 10.4376 - val_loss: 10.7885\n","Epoch 120/150\n","90/90 - 0s - loss: 10.4340 - val_loss: 10.8091\n","Epoch 121/150\n","90/90 - 0s - loss: 10.4331 - val_loss: 10.8132\n","Epoch 122/150\n","90/90 - 0s - loss: 10.4158 - val_loss: 10.8318\n","Epoch 123/150\n","90/90 - 0s - loss: 10.4293 - val_loss: 10.8060\n","Epoch 124/150\n","90/90 - 0s - loss: 10.4086 - val_loss: 10.7983\n","Epoch 125/150\n","90/90 - 0s - loss: 10.3947 - val_loss: 10.8225\n","Epoch 126/150\n","90/90 - 0s - loss: 10.4049 - val_loss: 10.8045\n","Epoch 127/150\n","90/90 - 0s - loss: 10.3882 - val_loss: 10.8107\n","Epoch 128/150\n","90/90 - 0s - loss: 10.3916 - val_loss: 10.7707\n","Epoch 129/150\n","90/90 - 0s - loss: 10.3876 - val_loss: 10.7908\n","Epoch 130/150\n","90/90 - 0s - loss: 10.3669 - val_loss: 10.7836\n","Epoch 131/150\n","90/90 - 0s - loss: 10.3720 - val_loss: 10.7671\n","Epoch 132/150\n","90/90 - 0s - loss: 10.3518 - val_loss: 10.7840\n","Epoch 133/150\n","90/90 - 0s - loss: 10.3610 - val_loss: 10.7631\n","Epoch 134/150\n","90/90 - 0s - loss: 10.3617 - val_loss: 10.7197\n","Epoch 135/150\n","90/90 - 0s - loss: 10.3606 - val_loss: 10.7472\n","Epoch 136/150\n","90/90 - 0s - loss: 10.3334 - val_loss: 10.7199\n","Epoch 137/150\n","90/90 - 0s - loss: 10.3394 - val_loss: 10.7554\n","Epoch 138/150\n","90/90 - 0s - loss: 10.3278 - val_loss: 10.7233\n","Epoch 139/150\n","90/90 - 0s - loss: 10.3228 - val_loss: 10.7447\n","Epoch 140/150\n","90/90 - 0s - loss: 10.3195 - val_loss: 10.7033\n","Epoch 141/150\n","90/90 - 0s - loss: 10.3078 - val_loss: 10.7380\n","Epoch 142/150\n","90/90 - 0s - loss: 10.3015 - val_loss: 10.6984\n","Epoch 143/150\n","90/90 - 0s - loss: 10.2916 - val_loss: 10.6943\n","Epoch 144/150\n","90/90 - 0s - loss: 10.2938 - val_loss: 10.6953\n","Epoch 145/150\n","90/90 - 0s - loss: 10.2832 - val_loss: 10.7093\n","Epoch 146/150\n","90/90 - 0s - loss: 10.2678 - val_loss: 10.6712\n","Epoch 147/150\n","90/90 - 0s - loss: 10.2609 - val_loss: 10.6921\n","Epoch 148/150\n","90/90 - 0s - loss: 10.2642 - val_loss: 10.7077\n","Epoch 149/150\n","90/90 - 0s - loss: 10.2523 - val_loss: 10.6719\n","Epoch 150/150\n","90/90 - 0s - loss: 10.2600 - val_loss: 10.6820\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GJ6yUMAPsqvu","executionInfo":{"status":"ok","timestamp":1604887115313,"user_tz":180,"elapsed":51137,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"5825efe3-31cf-4f7a-ed72-2929440b52ed","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 16.353\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JHli25_gsq-Z","executionInfo":{"status":"ok","timestamp":1604887115314,"user_tz":180,"elapsed":51132,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"c68ed2a4-993e-4959-f093-93ec944643af","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 10.682\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zY37NzMssqlw","executionInfo":{"status":"ok","timestamp":1604887115315,"user_tz":180,"elapsed":51128,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"4dd4efb6-1803-4644-86de-87604e2c880d","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.532\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WAzclehErQBM"},"source":["#**(spline) LAG=4**"]},{"cell_type":"code","metadata":{"id":"TL2bRX1PrTFj","executionInfo":{"status":"ok","timestamp":1604887197677,"user_tz":180,"elapsed":49285,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"5dddd0ff-fbe5-4a13-aaef-fff7f020220f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# prepare data for lstm\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score #colocar citação\n","\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","\n","df_sc.drop(columns=['Ano','Mês','Dia','Hora','Hora-minuto'], inplace=True)\n","#df_sc.drop(columns=['Hora-minuto'], inplace=True)\n","df_sc_copy=df_sc.copy()\n","df_sc_copy=df_sc_copy[['O3']]\n","df_sc.drop(columns=['O3'],inplace=True)\n","df_sc = pd.concat([df_sc_copy,df_sc], axis=1, ignore_index=False)\n","\n","values = df_sc.values\n","# ensure all data is float\n","values = values.astype('float32')\n","# normalize features\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","# scaled = scaler.fit_transform(values)\n","\n","scaled=values #não normalizado\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","# specify the number of lag hours\n","n_hours = 2 #aqui definimos o LAg\n","n_features = 10\n","\n","# frame as supervised learning\n","# reframed = series_to_supervised(scaled, n_hours, 4)\n","reframed = series_to_supervised(scaled, n_hours, 4) #é aqui que definimos a janela de previsão\n","\n","# split into train and test sets\n","values = reframed.values\n","n_train_hours = 1855 * 24\n","train = values[:n_train_hours, :]\n","test = values[n_train_hours:, :]\n","\n","# split into input and outputs\n","n_obs = n_hours * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","\n","# design network\n","opt = Adam(learning_rate=0.0005)\n","model = Sequential()\n","model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dense(1))\n","model.compile(loss='mae', optimizer=opt)\n","\n","# fit network\n","history = model.fit(train_X, train_y, epochs=150, batch_size=500, validation_data=(test_X, test_y), verbose=2, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","90/90 - 1s - loss: 19.9510 - val_loss: 19.9674\n","Epoch 2/150\n","90/90 - 0s - loss: 18.9248 - val_loss: 19.2017\n","Epoch 3/150\n","90/90 - 0s - loss: 18.4616 - val_loss: 18.7127\n","Epoch 4/150\n","90/90 - 0s - loss: 18.1449 - val_loss: 18.3815\n","Epoch 5/150\n","90/90 - 0s - loss: 17.8476 - val_loss: 18.0902\n","Epoch 6/150\n","90/90 - 0s - loss: 17.5781 - val_loss: 17.8029\n","Epoch 7/150\n","90/90 - 0s - loss: 17.3306 - val_loss: 17.5413\n","Epoch 8/150\n","90/90 - 0s - loss: 17.1146 - val_loss: 17.3179\n","Epoch 9/150\n","90/90 - 0s - loss: 16.9224 - val_loss: 17.1119\n","Epoch 10/150\n","90/90 - 0s - loss: 16.7281 - val_loss: 16.9203\n","Epoch 11/150\n","90/90 - 0s - loss: 16.5328 - val_loss: 16.7121\n","Epoch 12/150\n","90/90 - 0s - loss: 16.3443 - val_loss: 16.5403\n","Epoch 13/150\n","90/90 - 0s - loss: 16.2035 - val_loss: 16.3351\n","Epoch 14/150\n","90/90 - 0s - loss: 16.0221 - val_loss: 16.1915\n","Epoch 15/150\n","90/90 - 0s - loss: 15.8450 - val_loss: 16.0102\n","Epoch 16/150\n","90/90 - 0s - loss: 15.6870 - val_loss: 15.8515\n","Epoch 17/150\n","90/90 - 0s - loss: 15.5865 - val_loss: 15.7747\n","Epoch 18/150\n","90/90 - 0s - loss: 15.4659 - val_loss: 15.6246\n","Epoch 19/150\n","90/90 - 0s - loss: 15.3237 - val_loss: 15.4962\n","Epoch 20/150\n","90/90 - 0s - loss: 15.2158 - val_loss: 15.3891\n","Epoch 21/150\n","90/90 - 0s - loss: 15.1178 - val_loss: 15.2978\n","Epoch 22/150\n","90/90 - 0s - loss: 15.0239 - val_loss: 15.2011\n","Epoch 23/150\n","90/90 - 0s - loss: 14.9407 - val_loss: 15.1338\n","Epoch 24/150\n","90/90 - 0s - loss: 14.8623 - val_loss: 15.0268\n","Epoch 25/150\n","90/90 - 0s - loss: 14.7800 - val_loss: 14.9530\n","Epoch 26/150\n","90/90 - 0s - loss: 14.7138 - val_loss: 14.8857\n","Epoch 27/150\n","90/90 - 0s - loss: 14.6459 - val_loss: 14.8308\n","Epoch 28/150\n","90/90 - 0s - loss: 14.5670 - val_loss: 14.7693\n","Epoch 29/150\n","90/90 - 0s - loss: 14.5205 - val_loss: 14.7026\n","Epoch 30/150\n","90/90 - 0s - loss: 14.4609 - val_loss: 14.6584\n","Epoch 31/150\n","90/90 - 0s - loss: 14.3504 - val_loss: 14.5618\n","Epoch 32/150\n","90/90 - 0s - loss: 14.2727 - val_loss: 14.5239\n","Epoch 33/150\n","90/90 - 0s - loss: 14.2318 - val_loss: 14.4277\n","Epoch 34/150\n","90/90 - 0s - loss: 14.1824 - val_loss: 14.3871\n","Epoch 35/150\n","90/90 - 0s - loss: 14.1344 - val_loss: 14.3602\n","Epoch 36/150\n","90/90 - 0s - loss: 14.1021 - val_loss: 14.3030\n","Epoch 37/150\n","90/90 - 0s - loss: 14.0273 - val_loss: 14.2345\n","Epoch 38/150\n","90/90 - 0s - loss: 13.9833 - val_loss: 14.2173\n","Epoch 39/150\n","90/90 - 0s - loss: 13.9354 - val_loss: 14.1248\n","Epoch 40/150\n","90/90 - 0s - loss: 13.8775 - val_loss: 14.1287\n","Epoch 41/150\n","90/90 - 0s - loss: 13.8675 - val_loss: 14.0697\n","Epoch 42/150\n","90/90 - 0s - loss: 13.7944 - val_loss: 14.0178\n","Epoch 43/150\n","90/90 - 0s - loss: 13.7955 - val_loss: 13.9896\n","Epoch 44/150\n","90/90 - 0s - loss: 13.7301 - val_loss: 13.9579\n","Epoch 45/150\n","90/90 - 0s - loss: 13.7094 - val_loss: 13.9210\n","Epoch 46/150\n","90/90 - 0s - loss: 13.6705 - val_loss: 13.8889\n","Epoch 47/150\n","90/90 - 0s - loss: 13.6439 - val_loss: 13.8907\n","Epoch 48/150\n","90/90 - 0s - loss: 13.6212 - val_loss: 13.8295\n","Epoch 49/150\n","90/90 - 0s - loss: 13.5925 - val_loss: 13.8233\n","Epoch 50/150\n","90/90 - 0s - loss: 13.5769 - val_loss: 13.8111\n","Epoch 51/150\n","90/90 - 0s - loss: 13.5706 - val_loss: 13.7575\n","Epoch 52/150\n","90/90 - 0s - loss: 13.5192 - val_loss: 13.7616\n","Epoch 53/150\n","90/90 - 0s - loss: 13.5035 - val_loss: 13.7292\n","Epoch 54/150\n","90/90 - 0s - loss: 13.4985 - val_loss: 13.7409\n","Epoch 55/150\n","90/90 - 0s - loss: 13.4784 - val_loss: 13.7489\n","Epoch 56/150\n","90/90 - 0s - loss: 13.4987 - val_loss: 13.6277\n","Epoch 57/150\n","90/90 - 0s - loss: 13.4276 - val_loss: 13.6118\n","Epoch 58/150\n","90/90 - 0s - loss: 13.4209 - val_loss: 13.5884\n","Epoch 59/150\n","90/90 - 0s - loss: 13.4142 - val_loss: 13.5959\n","Epoch 60/150\n","90/90 - 0s - loss: 13.3808 - val_loss: 13.5980\n","Epoch 61/150\n","90/90 - 0s - loss: 13.3561 - val_loss: 13.5755\n","Epoch 62/150\n","90/90 - 0s - loss: 13.3607 - val_loss: 13.5379\n","Epoch 63/150\n","90/90 - 0s - loss: 13.3526 - val_loss: 13.5949\n","Epoch 64/150\n","90/90 - 0s - loss: 13.4046 - val_loss: 13.5987\n","Epoch 65/150\n","90/90 - 0s - loss: 13.3027 - val_loss: 13.4901\n","Epoch 66/150\n","90/90 - 0s - loss: 13.2868 - val_loss: 13.4798\n","Epoch 67/150\n","90/90 - 0s - loss: 13.2977 - val_loss: 13.5797\n","Epoch 68/150\n","90/90 - 0s - loss: 13.3333 - val_loss: 13.5448\n","Epoch 69/150\n","90/90 - 0s - loss: 13.2430 - val_loss: 13.4535\n","Epoch 70/150\n","90/90 - 0s - loss: 13.2491 - val_loss: 13.5258\n","Epoch 71/150\n","90/90 - 0s - loss: 13.2453 - val_loss: 13.4611\n","Epoch 72/150\n","90/90 - 0s - loss: 13.2128 - val_loss: 13.4698\n","Epoch 73/150\n","90/90 - 0s - loss: 13.2277 - val_loss: 13.4295\n","Epoch 74/150\n","90/90 - 0s - loss: 13.2094 - val_loss: 13.5091\n","Epoch 75/150\n","90/90 - 0s - loss: 13.2312 - val_loss: 13.3763\n","Epoch 76/150\n","90/90 - 0s - loss: 13.1523 - val_loss: 13.3847\n","Epoch 77/150\n","90/90 - 0s - loss: 13.1575 - val_loss: 13.3736\n","Epoch 78/150\n","90/90 - 0s - loss: 13.1378 - val_loss: 13.4837\n","Epoch 79/150\n","90/90 - 0s - loss: 13.1858 - val_loss: 13.4346\n","Epoch 80/150\n","90/90 - 0s - loss: 13.1473 - val_loss: 13.3968\n","Epoch 81/150\n","90/90 - 0s - loss: 13.1563 - val_loss: 13.3698\n","Epoch 82/150\n","90/90 - 0s - loss: 13.1146 - val_loss: 13.3856\n","Epoch 83/150\n","90/90 - 0s - loss: 13.1276 - val_loss: 13.3774\n","Epoch 84/150\n","90/90 - 0s - loss: 13.1019 - val_loss: 13.3449\n","Epoch 85/150\n","90/90 - 0s - loss: 13.1246 - val_loss: 13.2789\n","Epoch 86/150\n","90/90 - 0s - loss: 13.0501 - val_loss: 13.4311\n","Epoch 87/150\n","90/90 - 0s - loss: 13.1871 - val_loss: 13.3494\n","Epoch 88/150\n","90/90 - 0s - loss: 13.1251 - val_loss: 13.2554\n","Epoch 89/150\n","90/90 - 0s - loss: 13.0462 - val_loss: 13.3207\n","Epoch 90/150\n","90/90 - 0s - loss: 13.0424 - val_loss: 13.4214\n","Epoch 91/150\n","90/90 - 0s - loss: 13.1511 - val_loss: 13.3246\n","Epoch 92/150\n","90/90 - 0s - loss: 13.0274 - val_loss: 13.2470\n","Epoch 93/150\n","90/90 - 0s - loss: 12.9850 - val_loss: 13.2441\n","Epoch 94/150\n","90/90 - 0s - loss: 12.9892 - val_loss: 13.2897\n","Epoch 95/150\n","90/90 - 0s - loss: 13.0062 - val_loss: 13.2753\n","Epoch 96/150\n","90/90 - 0s - loss: 12.9711 - val_loss: 13.2816\n","Epoch 97/150\n","90/90 - 0s - loss: 12.9420 - val_loss: 13.2549\n","Epoch 98/150\n","90/90 - 0s - loss: 12.9622 - val_loss: 13.2420\n","Epoch 99/150\n","90/90 - 0s - loss: 12.9416 - val_loss: 13.3007\n","Epoch 100/150\n","90/90 - 0s - loss: 12.9627 - val_loss: 13.2520\n","Epoch 101/150\n","90/90 - 0s - loss: 12.9146 - val_loss: 13.3131\n","Epoch 102/150\n","90/90 - 0s - loss: 12.9078 - val_loss: 13.2268\n","Epoch 103/150\n","90/90 - 0s - loss: 12.8968 - val_loss: 13.2378\n","Epoch 104/150\n","90/90 - 0s - loss: 12.9060 - val_loss: 13.1970\n","Epoch 105/150\n","90/90 - 0s - loss: 12.8688 - val_loss: 13.2022\n","Epoch 106/150\n","90/90 - 0s - loss: 12.8626 - val_loss: 13.1494\n","Epoch 107/150\n","90/90 - 0s - loss: 12.8564 - val_loss: 13.1430\n","Epoch 108/150\n","90/90 - 0s - loss: 12.8426 - val_loss: 13.2050\n","Epoch 109/150\n","90/90 - 0s - loss: 12.8514 - val_loss: 13.1499\n","Epoch 110/150\n","90/90 - 0s - loss: 12.8024 - val_loss: 13.1494\n","Epoch 111/150\n","90/90 - 0s - loss: 12.8117 - val_loss: 13.1246\n","Epoch 112/150\n","90/90 - 0s - loss: 12.8036 - val_loss: 13.1636\n","Epoch 113/150\n","90/90 - 0s - loss: 12.8263 - val_loss: 13.1354\n","Epoch 114/150\n","90/90 - 0s - loss: 12.7570 - val_loss: 13.0458\n","Epoch 115/150\n","90/90 - 0s - loss: 12.7678 - val_loss: 13.0629\n","Epoch 116/150\n","90/90 - 0s - loss: 12.7570 - val_loss: 13.1259\n","Epoch 117/150\n","90/90 - 0s - loss: 12.7550 - val_loss: 13.0854\n","Epoch 118/150\n","90/90 - 0s - loss: 12.7095 - val_loss: 13.0324\n","Epoch 119/150\n","90/90 - 0s - loss: 12.7032 - val_loss: 13.0661\n","Epoch 120/150\n","90/90 - 0s - loss: 12.6979 - val_loss: 13.0385\n","Epoch 121/150\n","90/90 - 0s - loss: 12.6811 - val_loss: 13.0022\n","Epoch 122/150\n","90/90 - 0s - loss: 12.6709 - val_loss: 12.9638\n","Epoch 123/150\n","90/90 - 0s - loss: 12.6522 - val_loss: 12.9524\n","Epoch 124/150\n","90/90 - 0s - loss: 12.6439 - val_loss: 12.9395\n","Epoch 125/150\n","90/90 - 0s - loss: 12.6463 - val_loss: 12.9199\n","Epoch 126/150\n","90/90 - 0s - loss: 12.6121 - val_loss: 12.9017\n","Epoch 127/150\n","90/90 - 0s - loss: 12.6225 - val_loss: 12.9139\n","Epoch 128/150\n","90/90 - 0s - loss: 12.5880 - val_loss: 12.9101\n","Epoch 129/150\n","90/90 - 0s - loss: 12.6322 - val_loss: 12.9291\n","Epoch 130/150\n","90/90 - 0s - loss: 12.5503 - val_loss: 12.9453\n","Epoch 131/150\n","90/90 - 0s - loss: 12.6275 - val_loss: 12.9172\n","Epoch 132/150\n","90/90 - 0s - loss: 12.5848 - val_loss: 12.9399\n","Epoch 133/150\n","90/90 - 0s - loss: 12.6313 - val_loss: 12.9198\n","Epoch 134/150\n","90/90 - 0s - loss: 12.5839 - val_loss: 12.9405\n","Epoch 135/150\n","90/90 - 0s - loss: 12.5832 - val_loss: 12.9450\n","Epoch 136/150\n","90/90 - 0s - loss: 12.5774 - val_loss: 12.9561\n","Epoch 137/150\n","90/90 - 0s - loss: 12.5510 - val_loss: 12.8810\n","Epoch 138/150\n","90/90 - 0s - loss: 12.5359 - val_loss: 12.8962\n","Epoch 139/150\n","90/90 - 0s - loss: 12.5427 - val_loss: 12.9070\n","Epoch 140/150\n","90/90 - 0s - loss: 12.5315 - val_loss: 12.8594\n","Epoch 141/150\n","90/90 - 0s - loss: 12.4836 - val_loss: 12.8585\n","Epoch 142/150\n","90/90 - 0s - loss: 12.5092 - val_loss: 12.8433\n","Epoch 143/150\n","90/90 - 0s - loss: 12.4787 - val_loss: 12.8147\n","Epoch 144/150\n","90/90 - 0s - loss: 12.4960 - val_loss: 12.8492\n","Epoch 145/150\n","90/90 - 0s - loss: 12.4730 - val_loss: 12.8353\n","Epoch 146/150\n","90/90 - 0s - loss: 12.5067 - val_loss: 12.9101\n","Epoch 147/150\n","90/90 - 0s - loss: 12.4786 - val_loss: 12.8075\n","Epoch 148/150\n","90/90 - 0s - loss: 12.4340 - val_loss: 12.8474\n","Epoch 149/150\n","90/90 - 0s - loss: 12.4528 - val_loss: 12.7998\n","Epoch 150/150\n","90/90 - 0s - loss: 12.4246 - val_loss: 12.7566\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lLQrIHWQs3RR","executionInfo":{"status":"ok","timestamp":1604887198719,"user_tz":180,"elapsed":50299,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"d337d0d2-16a1-4497-fee4-83f86960a6bd","colab":{"base_uri":"https://localhost:8080/"}},"source":["# make a prediction\n","yhat = model.predict(test_X)\n","test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","\n","# invert scaling for forecast (não foi feito)\n","inv_yhat = concatenate((yhat, test_X[:, -9:]), axis=1)\n","inv_yhat = inv_yhat[:,0]\n","\n","# invert scaling for actual\n","test_y = test_y.reshape((len(test_y), 1))\n","inv_y = concatenate((test_y, test_X[:, -9:]), axis=1)\n","# inv_y = scaler.inverse_transform(inv_y)\n","inv_y = inv_y[:,0]\n","\n","# calculate RMSE\n","rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","print('Test RMSE: %.3f' % rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test RMSE: 19.140\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rFVBrdwMs3gD","executionInfo":{"status":"ok","timestamp":1604887198721,"user_tz":180,"elapsed":50297,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"9a93c01f-3ef0-4263-98f6-64f6e9088ba4","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate MAE\n","mae = mean_absolute_error(inv_y, inv_yhat)\n","print('Test MAE: %.3f' % mae)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test MAE: 12.757\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DV9swVDns3G-","executionInfo":{"status":"ok","timestamp":1604887198722,"user_tz":180,"elapsed":50295,"user":{"displayName":"José Fernando Lopes Leocadio","photoUrl":"","userId":"01550979567368402652"}},"outputId":"10772aea-0444-4371-83e5-154762555eeb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# calculate R2\n","r2=r2_score(inv_y, inv_yhat)\n","print('Test R2: %.3f' % r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test R2: 0.359\n"],"name":"stdout"}]}]}